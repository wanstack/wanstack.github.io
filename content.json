[{"title":"Openstack Mitaka 集群安装部署","date":"2017-04-14T15:37:00.000Z","path":"2017/04/14/Openstack-Mitaka HA 部署手册/","text":"==openstack开发运维群:94776000欢迎加入讨论问题== Openstack Mitaka HA 实施部署测试文档一、环境说明1、主机环境123456controller(VIP) 192.168.10.100controller01 192.168.10.101, 10.0.0.1controller02 192.168.10.102, 10.0.0.2controller03 192.168.10.103, 10.0.0.3compute01 192.168.10.104, 10.0.0.4compute02 192.168.10.105, 10.0.0.5 本次环境仅限于测试环境，主要测试HA功能。具体生产环境请对网络进行划分。 二、配置基础环境1、配置主机解析12345678910111213141516在对应节点上配置主机名：hostnamectl set-hostname controller01hostname contoller01hostnamectl set-hostname controller02hostname contoller02hostnamectl set-hostname controller03hostname contoller03hostnamectl set-hostname compute01hostname compute01hostnamectl set-hostname compute02hostname compute02 12345678910111213在controller01上配置主机解析：[root@controller01 ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.10.100 controller192.168.10.101 controller01192.168.10.102 controller02192.168.10.103 controller03192.168.10.104 compute01192.168.10.105 compute02 2、配置ssh互信1234567在controller01上进行配置：ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &apos;&apos;ssh-copy-id -i .ssh/id_rsa.pub root@controller02ssh-copy-id -i .ssh/id_rsa.pub root@controller03ssh-copy-id -i .ssh/id_rsa.pub root@compute01ssh-copy-id -i .ssh/id_rsa.pub root@compute02 12345拷贝主机名解析配置文件到其他节点scp /etc/hosts controller02:/etc/hostsscp /etc/hosts controller03:/etc/hostsscp /etc/hosts compute01:/etc/hostsscp /etc/hosts compute02:/etc/hosts 3、yum 源配置本次测试机所有节点都可以正常连接网络，故使用阿里云的openstack和base源 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 所有控制和计算节点开启yum缓存[root@controller01 ~]# cat /etc/yum.conf [main]cachedir=/var/cache/yum/$basearch/$releasever# 开启缓存keepcache=1表示开启缓存，keepcache=0表示不开启缓存，默认为0keepcache=1debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1installonly_limit=5bugtracker_url=http://bugs.centos.org/set_project.php?project_id=23&amp;ref=http://bugs.centos.org/bug_report_page.php?category=yumdistroverpkg=centos-release# 基础源yum install wget -yrm -rf /etc/yum.repos.d/*wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# openstack mitaka源yum install centos-release-openstack-mitaka -y# 默认是centos源，建议修改成阿里云的，因为速度快[root@contoller01 yum.repos.d]# vim CentOS-OpenStack-mitaka.repo # CentOS-OpenStack-mitaka.repo## Please see http://wiki.centos.org/SpecialInterestGroup/Cloud for more# information[centos-openstack-mitaka]name=CentOS-7 - OpenStack mitakabaseurl=http://mirrors.aliyun.com//centos/7/cloud/$basearch/openstack-mitaka/gpgcheck=1enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-Cloud# galera源vim mariadb.repo[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.1/centos7-amd64/enable=1gpgcheck=1gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB scp 到其他所有节点 123456789scp CentOS-OpenStack-mitaka.repo controller02:/etc/yum.repos.d/CentOS-OpenStack-mitaka.reposcp CentOS-OpenStack-mitaka.repo controller03:/etc/yum.repos.d/CentOS-OpenStack-mitaka.reposcp CentOS-OpenStack-mitaka.repo compute01:/etc/yum.repos.d/CentOS-OpenStack-mitaka.reposcp CentOS-OpenStack-mitaka.repo compute02:/etc/yum.repos.d/CentOS-OpenStack-mitaka.reposcp mariadb.repo controller02:/etc/yum.repos.d/mariadb.reposcp mariadb.repo controller03:/etc/yum.repos.d/mariadb.reposcp mariadb.repo compute01:/etc/yum.repos.d/mariadb.reposcp mariadb.repo compute02:/etc/yum.repos.d/mariadb.repo 4、ntp配置本机环境已经有ntp服务器，故直接使用。如果没有ntp服务器建议使用controller作为ntp服务器 123yum install ntpdate -yecho &quot;*/5 * * * * /usr/sbin/ntpdate 192.168.2.161 &gt;/dev/null 2&gt;&amp;1&quot; &gt;&gt; /var/spool/cron/root/usr/sbin/ntpdate 192.168.2.161 5、关闭防火墙和selinux12345systemctl disable firewalld.servicesystemctl stop firewalld.servicesed -i -e &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsed -i -e &quot;s#SELINUXTYPE=targeted#\\#SELINUXTYPE=targeted#g&quot; /etc/selinux/configsetenforce 0 6、安装配置pacemaker12345678910111213141516171819202122232425262728293031323334# 所有控制节点安装如下软件yum install -y pcs pacemaker corosync fence-agents-all resource-agents修改corosync配置文件[root@contoller01 ~]# cat /etc/corosync/corosync.conftotem &#123;version: 2secauth: offcluster_name: openstack-clustertransport: udpu&#125;nodelist &#123; node &#123; ring0_addr: controller01 nodeid: 1 &#125; node &#123; ring0_addr: controller02 nodeid: 2 &#125; node &#123; ring0_addr: controller03 nodeid: 3&#125;&#125;quorum &#123;provider: corosync_votequorumtwo_node: 1&#125;logging &#123;to_syslog: yes&#125; 123# 把配置文件拷贝到其他控制节点scp /etc/corosync/corosync.conf controller02:/etc/corosync/corosync.confscp /etc/corosync/corosync.conf controller03:/etc/corosync/corosync.conf 12# 查看成员信息corosync-cmapctl runtime.totem.pg.mrp.srp.members 123456789101112131415161718192021222324# 所有控制节点启动服务systemctl enable pcsdsystemctl start pcsd# 所有控制节点设置hacluster用户的密码echo hacluster | passwd --stdin hacluster# [controller01]设置到集群节点的认证pcs cluster auth controller01 controller02 controller03 -u hacluster -p hacluster --force# [controller01]创建并启动集群pcs cluster setup --force --name openstack-cluster controller01 controller02 controller03pcs cluster start --all# [controller01]设置集群属性pcs property set pe-warn-series-max=1000 pe-input-series-max=1000 pe-error-series-max=1000 cluster-recheck-interval=5min# [controller01] 暂时禁用STONISH，否则资源无法启动pcs property set stonith-enabled=false# [controller01] 忽略投票pcs property set no-quorum-policy=ignore# [controller01]配置VIP资源，VIP可以在集群节点间浮动pcs resource create vip ocf:heartbeat:IPaddr2 params ip=192.168.10.100 cidr_netmask=&quot;24&quot; op monitor interval=&quot;30s&quot; 7、安装haproxy1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# [所有控制节点] 安装软件yum install -y haproxy# [所有控制节点] 修改/etc/rsyslog.d/haproxy.conf文件echo &quot;\\$ModLoad imudp&quot; &gt;&gt; /etc/rsyslog.d/haproxy.conf;echo &quot;\\$UDPServerRun 514&quot; &gt;&gt; /etc/rsyslog.d/haproxy.conf;echo &quot;local3.* /var/log/haproxy.log&quot; &gt;&gt; /etc/rsyslog.d/haproxy.conf;echo &quot;&amp;~&quot; &gt;&gt; /etc/rsyslog.d/haproxy.conf;# [所有控制节点] 修改/etc/sysconfig/rsyslog文件sed -i -e &apos;s#SYSLOGD_OPTIONS=\\&quot;\\&quot;#SYSLOGD_OPTIONS=\\&quot;-c 2 -r -m 0\\&quot;#g&apos; /etc/sysconfig/rsyslog# [所有控制节点] 重启rsyslog服务systemctl restart rsyslog# 创建haproxy基础配置vim /etc/haproxy/haproxy.cfg#---------------------------------------------------------------------# Example configuration for a possible web application. See the# full configuration options online.## http://haproxy.1wt.eu/download/1.4/doc/configuration.txt##---------------------------------------------------------------------#---------------------------------------------------------------------# Global settings#---------------------------------------------------------------------global # to have these messages end up in /var/log/haproxy.log you will # need to: # # 1) configure syslog to accept network log events. This is done # by adding the &apos;-r&apos; option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local3 chroot /var/lib/haproxy daemon group haproxy maxconn 4000 pidfile /var/run/haproxy.pid user haproxy#---------------------------------------------------------------------# common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will# use if not designated in their block#---------------------------------------------------------------------defaults log global maxconn 4000 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout check 10s include conf.d/*.cfg 123# 拷贝到其他控制节点scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg 1234567# [controller01]在pacemaker集群增加haproxy资源pcs resource create haproxy systemd:haproxy --clone# Optional表示只在同时停止和/或启动两个资源时才会产生影响。对第一个指定资源进行的任何更改都不会对第二个指定的资源产生影响，定义在前面的资源先确保运行。pcs constraint order start vip then haproxy-clone kind=Optional# vip的资源决定了haproxy-clone资源的位置约束pcs constraint colocation add haproxy-clone with vipping -c 3 192.168.10.100 8、galera安装配置123456789101112131415161718192021222324#所有控制节点上操作基本操作 ：安装、设置配置文件 yum install -y MariaDB-server xinetd# 在所有控制节点进行配置vim /usr/lib/systemd/system/mariadb.service[Service]新添加两行如下参数：LimitNOFILE=10000LimitNPROC=10000systemctl --system daemon-reload systemctl restart mariadb.service # 初始化数据库，在controller01上执行即可systemctl start mariadbmysql_secure_installation# 查看并发数show variables like &apos;max_connections&apos;; # 关闭服务修改配置文件systemctl stop mariadb# 备份原始配置文件cp /etc/my.cnf.d/server.cnf /etc/my.cnf.d/bak.server.cnf 1234567891011121314151617181920212223# 配置controller01上的配置文件cat /etc/my.cnf.d/server.cnf[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockuser=mysqlbinlog_format=ROWmax_connections = 4096bind-address= 192.168.10.101default_storage_engine=innodbinnodb_autoinc_lock_mode=2innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_on=ONwsrep_provider=/usr/lib64/galera/libgalera_smm.sowsrep_provider_options=&quot;pc.recovery=TRUE;gcache.size=300M&quot;wsrep_cluster_name=&quot;galera_cluster&quot;wsrep_cluster_address=&quot;gcomm://controller01,controller02,controller03&quot;wsrep_node_name= controller01wsrep_node_address= 192.168.10.101wsrep_sst_method=rsync 1234567891011121314151617181920212223# 配置controller02上的配置文件cat /etc/my.cnf.d/server.cnf[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockuser=mysqlbinlog_format=ROWmax_connections = 4096bind-address= 192.168.10.102default_storage_engine=innodbinnodb_autoinc_lock_mode=2innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_on=ONwsrep_provider=/usr/lib64/galera/libgalera_smm.sowsrep_provider_options=&quot;pc.recovery=TRUE;gcache.size=300M&quot;wsrep_cluster_name=&quot;galera_cluster&quot;wsrep_cluster_address=&quot;gcomm://controller01,controller02,controller03&quot;wsrep_node_name= controller02wsrep_node_address= 192.168.10.102wsrep_sst_method=rsync 1234567891011121314151617181920212223# 配置controller03上的配置文件cat /etc/my.cnf.d/server.cnf[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockuser=mysqlbinlog_format=ROWmax_connections = 4096bind-address= 192.168.10.103default_storage_engine=innodbinnodb_autoinc_lock_mode=2innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_on=ONwsrep_provider=/usr/lib64/galera/libgalera_smm.sowsrep_provider_options=&quot;pc.recovery=TRUE;gcache.size=300M&quot;wsrep_cluster_name=&quot;galera_cluster&quot;wsrep_cluster_address=&quot;gcomm://controller01,controller02,controller03&quot;wsrep_node_name= controller03wsrep_node_address= 192.168.10.103wsrep_sst_method=rsync 123456789# 在controller01上执行galera_new_cluster#查看日志tail -f /var/log/messages# 启动其他控制节点systemctl enable mariadbsystemctl start mariadb 1234# 添加checkmysql -uroot -popenstack -e &quot;use mysql;INSERT INTO user(Host, User) VALUES(&apos;192.168.10.100&apos;, &apos;haproxy_check&apos;);FLUSH PRIVILEGES;&quot;mysql -uroot -popenstack -e &quot;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;controller01&apos; IDENTIFIED BY &apos;&quot;openstack&quot;&apos;&quot;;mysql -uroot -popenstack -h 192.168.10.100 -e &quot;SHOW STATUS LIKE &apos;wsrep_cluster_size&apos;;&quot; 12345678910111213141516# 配置haproxy for galera# 所有控制节点创建haproxy配置文件目录cat /etc/haproxy/haproxy.cfglisten galera_cluster bind 192.168.10.100:3306 balance source option mysql-check user haproxy_check server controller01 192.168.10.101:3306 inter 2000 rise 2 fall 5 server controller02 192.168.10.102:3306 backup inter 2000 rise 2 fall 5 server controller03 192.168.10.103:3306 backup inter 2000 rise 2 fall 5# 拷贝配置文件到其他控制节点scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/scp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/ 12345678910111213141516171819# 重启pacemaker,corosync集群脚本vim restart-pcs-cluster.sh#!/bin/shpcs cluster stop --allsleep 10#ps aux|grep &quot;pcs cluster stop --all&quot;|grep -v grep|awk &apos;&#123;print $2 &#125;&apos;|xargs killfor i in 01 02 03; do ssh controller$i pcs cluster kill; donepcs cluster stop --allpcs cluster start --allsleep 5watch -n 0.5 pcs resourceecho &quot;pcs resource&quot;pcs resourcepcs resource|grep Stoppcs resource|grep FAILED# 执行脚本bash restart-pcs-cluster.sh 9、安装配置rabbitmq-server集群123456789101112131415161718192021222324252627282930313233# 所有控制节点yum install -y rabbitmq-server# 拷贝controller01上的cookie到其他控制节点上scp /var/lib/rabbitmq/.erlang.cookie root@controller02:/var/lib/rabbitmq/.erlang.cookiescp /var/lib/rabbitmq/.erlang.cookie root@controller03:/var/lib/rabbitmq/.erlang.cookie# controller01以外的其他节点设置权限chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookiechmod 400 /var/lib/rabbitmq/.erlang.cookie# 启动服务systemctl enable rabbitmq-server.servicesystemctl start rabbitmq-server.service# 在任意控制节点上查看集群配置rabbitmqctl cluster_status# controller01以外的其他节点 加入集群rabbitmqctl stop_apprabbitmqctl join_cluster --ram rabbit@controller01rabbitmqctl start_app# 在任意节点 设置ha-moderabbitmqctl cluster_status;rabbitmqctl set_policy ha-all &apos;^(?!amq\\.).*&apos; &apos;&#123;&quot;ha-mode&quot;: &quot;all&quot;&#125;&apos;# 在任意节点执行创建用户rabbitmqctl add_user openstack openstackrabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 10、安装配置memcache1234567891011121314151617181920212223242526272829yum install -y memcached# controller01上修改配置cat /etc/sysconfig/memcached PORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;64&quot;OPTIONS=&quot;-l 192.168.10.101,::1&quot;# controller02上修改配置cat /etc/sysconfig/memcached PORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;64&quot;OPTIONS=&quot;-l 192.168.10.102,::1&quot;# controller03上修改配置cat /etc/sysconfig/memcached PORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;64&quot;OPTIONS=&quot;-l 192.168.10.103,::1&quot;# 所有节点启动服务systemctl enable memcached.servicesystemctl start memcached.service 三、安装配置openstack软件集群12# 所有控制节点和计算节点安装openstack 基础包yum install -y python-openstackclient openstack-selinux openstack-utils 1、安装openstack Identity123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255# 在任意节点创建keystone数据库mysql -uroot -popenstack -e &quot;CREATE DATABASE keystone;GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;keystone&quot;&apos;;GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;keystone&quot;&apos;;FLUSH PRIVILEGES;&quot;# 所有节点安装keystone软件包yum install -y openstack-keystone httpd mod_wsgi# 任意节点生成临时tokenopenssl rand -hex 108464d030a1f7ac3f7207# 修改keystone配置文件openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token 8464d030a1f7ac3f7207openstack-config --set /etc/keystone/keystone.conf database connection mysql+pymysql://keystone:keystone@controller/keystone#openstack-config --set /etc/keystone/keystone.conf token provider fernetopenstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_durable_queues true# 拷贝配置文件到其他控制节点scp /etc/keystone/keystone.conf controller02:/etc/keystone/keystone.confscp /etc/keystone/keystone.conf controller03:/etc/keystone/keystone.confsed -i -e &apos;s#\\#ServerName www.example.com:80#ServerName &apos;&quot;controller01&quot;&apos;#g&apos; /etc/httpd/conf/httpd.confsed -i -e &apos;s#\\#ServerName www.example.com:80#ServerName &apos;&quot;controller02&quot;&apos;#g&apos; /etc/httpd/conf/httpd.confsed -i -e &apos;s#\\#ServerName www.example.com:80#ServerName &apos;&quot;controller03&quot;&apos;#g&apos; /etc/httpd/conf/httpd.conf# controller01上的配置vim /etc/httpd/conf.d/wsgi-keystone.confListen 192.168.10.101:5000Listen 192.168.10.101:35357&lt;VirtualHost 192.168.10.101:5000&gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;&lt;VirtualHost 192.168.10.101:35357&gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;# controller02上的配置vim /etc/httpd/conf.d/wsgi-keystone.confListen 192.168.10.102:5000Listen 192.168.10.102:35357&lt;VirtualHost 192.168.10.102:5000&gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;&lt;VirtualHost 192.168.10.102:35357&gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;# controller03上的配置vim /etc/httpd/conf.d/wsgi-keystone.confListen 192.168.10.103:5000Listen 192.168.10.103:35357&lt;VirtualHost 192.168.10.103:5000&gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;&lt;VirtualHost 192.168.10.103:35357&gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;# 添加haproxy配置vim /etc/haproxy/haproxy.cfglisten keystone_admin_cluster bind 192.168.10.100:35357 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:35357 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:35357 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:35357 check inter 2000 rise 2 fall 5listen keystone_public_internal_cluster bind 192.168.10.100:5000 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:5000 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:5000 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:5000 check inter 2000 rise 2 fall 5# 把haproxy配置拷贝到其他控制节点scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg# [任一节点]生成数据库su -s /bin/sh -c &quot;keystone-manage db_sync&quot; keystone# [任一节点/controller01]初始化Fernet key，并共享给其他节点#keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone# 在其他控制节点#mkdir -p /etc/keystone/fernet-keys/# 在controller01上#scp /etc/keystone/fernet-keys/* root@controller02:/etc/keystone/fernet-keys/#scp /etc/keystone/fernet-keys/* root@controller03:/etc/keystone/fernet-keys/# 在其他控制节点chown keystone:keystone /etc/keystone/fernet-keys/*# [任一节点]添加pacemaker资源，openstack资源和haproxy资源无关，可以开启A/A模式# interleave=true副本交错启动/停止，改变master/clone间的order约束，每个实例像其他克隆实例一样可快速启动/停止，无需等待其他克隆实例。# interleave这个设置为false的时候,constraint的order顺序的受到其他节点的影响,为true不受其他节点影响pcs resource create openstack-keystone systemd:httpd --clone interleave=truebash restart-pcs-cluster.sh# 在任意节点创建临时tokenexport OS_TOKEN=8464d030a1f7ac3f7207export OS_URL=http://controller:35357/v3export OS_IDENTITY_API_VERSION=3# [任一节点]service entity and API endpointsopenstack service create --name keystone --description &quot;OpenStack Identity&quot; identityopenstack endpoint create --region RegionOne identity public http://controller:5000/v3openstack endpoint create --region RegionOne identity internal http://controller:5000/v3openstack endpoint create --region RegionOne identity admin http://controller:35357/v3# [任一节点]创建项目和用户openstack domain create --description &quot;Default Domain&quot; defaultopenstack project create --domain default --description &quot;Admin Project&quot; adminopenstack user create --domain default --password admin adminopenstack role create adminopenstack role add --project admin --user admin admin### [任一节点]创建service项目openstack project create --domain default --description &quot;Service Project&quot; service# 在任意节点创建demo项目和用户openstack project create --domain default --description &quot;Demo Project&quot; demoopenstack user create --domain default --password demo demoopenstack role create useropenstack role add --project demo --user demo user# 生成keystonerc_admin脚本echo &quot;export OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=adminexport OS_AUTH_URL=http://controller:35357/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2export PS1=&apos;[\\u@\\h \\W(keystone_admin)]\\$ &apos;&quot;&gt;/root/keystonerc_adminchmod +x /root/keystonerc_admin# 生成keystonerc_demo脚本echo &quot;export OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=demoexport OS_USERNAME=demoexport OS_PASSWORD=demoexport OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2export PS1=&apos;[\\u@\\h \\W(keystone_admin)]\\$ &apos;&quot;&gt;/root/keystonerc_demochmod +x /root/keystonerc_demosource keystonerc_admin### checkopenstack token issuesource keystonerc_demo### checkopenstack token issue 2、安装openstack Image集群123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116# [任一节点]创建数据库mysql -uroot -popenstack -e &quot;CREATE DATABASE glance;GRANT ALL PRIVILEGES ON glance.* TO &apos;glance&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;glance&quot;&apos;;GRANT ALL PRIVILEGES ON glance.* TO &apos;glance&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;glance&quot;&apos;;FLUSH PRIVILEGES;&quot;# [任一节点]创建用户等source keystonerc_admin openstack user create --domain default --password glance glanceopenstack role add --project service --user glance adminopenstack service create --name glance --description &quot;OpenStack Image&quot; imageopenstack endpoint create --region RegionOne image public http://controller:9292openstack endpoint create --region RegionOne image internal http://controller:9292openstack endpoint create --region RegionOne image admin http://controller:9292# 所有控制节点安装glance软件包yum install -y openstack-glance# [所有控制节点]配置/etc/glance/glance-api.conf文件openstack-config --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:glance@controller/glanceopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/glance/glance-api.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name serviceopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glanceopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken password glanceopenstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystoneopenstack-config --set /etc/glance/glance-api.conf glance_store stores file,httpopenstack-config --set /etc/glance/glance-api.conf glance_store default_store fileopenstack-config --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/openstack-config --set /etc/glance/glance-api.conf DEFAULT registry_host controlleropenstack-config --set /etc/glance/glance-api.conf DEFAULT bind_host controller01# [所有控制节点]配置/etc/glance/glance-registry.conf文件openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:glance@controller/glanceopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name serviceopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glanceopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password glanceopenstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystoneopenstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/glance/glance-registry.conf DEFAULT registry_host controlleropenstack-config --set /etc/glance/glance-registry.conf DEFAULT bind_host controller01scp /etc/glance/glance-api.conf controller02:/etc/glance/glance-api.confscp /etc/glance/glance-api.conf controller03:/etc/glance/glance-api.conf# 修改bind_host为对应的controller02,controller03scp /etc/glance/glance-registry.conf controller02:/etc/glance/glance-registry.confscp /etc/glance/glance-registry.conf controller03:/etc/glance/glance-registry.conf# 修改bind_host为对应的controller02,controller03vim /etc/haproxy/haproxy.cfg# 增加如下配置listen glance_api_cluster bind 192.168.10.100:9292 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:9292 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:9292 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:9292 check inter 2000 rise 2 fall 5listen glance_registry_cluster bind 192.168.10.100:9191 balance source option tcpka option tcplog server controller01 192.168.10.101:9191 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:9191 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:9191 check inter 2000 rise 2 fall 5scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg# [任一节点]生成数据库su -s /bin/sh -c &quot;glance-manage db_sync&quot; glance# [任一节点]添加pacemaker资源pcs resource create openstack-glance-registry systemd:openstack-glance-registry --clone interleave=truepcs resource create openstack-glance-api systemd:openstack-glance-api --clone interleave=true# 下面2条表示先启动openstack-keystone-clone然后启动openstack-glance-registry-clone然后启动openstack-glance-api-clonepcs constraint order start openstack-keystone-clone then openstack-glance-registry-clonepcs constraint order start openstack-glance-registry-clone then openstack-glance-api-clone# api随着registry启动而启动，如果registry启动不了，则api也启动不了pcs constraint colocation add openstack-glance-api-clone with openstack-glance-registry-clone# 在任意节点重启pacemakerbash restart-pcs-cluster.sh# 上传测试镜像openstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --publicopenstack image list 3、安装openstack Compute集群(控制节点)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147# 所有控制节点安装软件包yum install -y openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler# [任一节点]创建数据库mysql -uroot -popenstack -e &quot;CREATE DATABASE nova;GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;nova&quot;&apos;;GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;nova&quot;&apos;;CREATE DATABASE nova_api;GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;nova&quot;&apos;;GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;nova&quot;&apos;;FLUSH PRIVILEGES;&quot;# [任一节点]创建用户等source keystonerc_adminopenstack user create --domain default --password nova novaopenstack role add --project service --user nova adminopenstack service create --name nova --description &quot;OpenStack Compute&quot; computeopenstack endpoint create --region RegionOne compute public http://controller:8774/v2.1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1/%\\(tenant_id\\)s# [所有控制节点]配置配置nova组件，/etc/nova/nova.conf文件openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata# openstack-config --set /etc/nova/nova.conf DEFAULT memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/nova/nova.conf api_database connection mysql+pymysql://nova:nova@controller/nova_apiopenstack-config --set /etc/nova/nova.conf database connection mysql+pymysql://nova:nova@controller/novaopenstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password openstackopenstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_name serviceopenstack-config --set /etc/nova/nova.conf keystone_authtoken username novaopenstack-config --set /etc/nova/nova.conf keystone_authtoken password novaopenstack-config --set /etc/nova/nova.conf DEFAULT my_ip 192.168.10.101openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron Trueopenstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriveropenstack-config --set /etc/nova/nova.conf vnc vncserver_listen 192.168.10.101openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.10.101openstack-config --set /etc/nova/nova.conf vnc novncproxy_host 192.168.10.101openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmpopenstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_listen 192.168.10.101openstack-config --set /etc/nova/nova.conf DEFAULT metadata_listen 192.168.10.101scp /etc/nova/nova.conf controller02:/etc/nova/nova.confscp /etc/nova/nova.conf controller03:/etc/nova/nova.conf# 其他节点修改my_ip,vncserver_listen,vncserver_proxyclient_address,osapi_compute_listen,metadata_listen,vnc novncproxy_hostopenstack-config --set /etc/nova/nova.conf DEFAULT my_ip 192.168.10.102openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 192.168.10.102openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.10.102openstack-config --set /etc/nova/nova.conf vnc novncproxy_host 192.168.10.102openstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_listen 192.168.10.102openstack-config --set /etc/nova/nova.conf DEFAULT metadata_listen 192.168.10.102################################openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 192.168.10.103openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 192.168.10.103openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.10.103openstack-config --set /etc/nova/nova.conf vnc novncproxy_host 192.168.10.103openstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_listen 192.168.10.103openstack-config --set /etc/nova/nova.conf DEFAULT metadata_listen 192.168.10.103################################### 配置haproxyvim /etc/haproxy/haproxy.cfglisten nova_compute_api_cluster bind 192.168.10.100:8774 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:8774 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:8774 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:8774 check inter 2000 rise 2 fall 5listen nova_metadata_api_cluster bind 192.168.10.100:8775 balance source option tcpka option tcplog server controller01 192.168.10.101:8775 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:8775 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:8775 check inter 2000 rise 2 fall 5listen nova_vncproxy_cluster bind 192.168.10.100:6080 balance source option tcpka option tcplog server controller01 192.168.10.101:6080 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:6080 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:6080 check inter 2000 rise 2 fall 5scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg# [任一节点]生成数据库su -s /bin/sh -c &quot;nova-manage api_db sync&quot; novasu -s /bin/sh -c &quot;nova-manage db sync&quot; nova# [任一节点]添加pacemaker资源pcs resource create openstack-nova-consoleauth systemd:openstack-nova-consoleauth --clone interleave=truepcs resource create openstack-nova-novncproxy systemd:openstack-nova-novncproxy --clone interleave=truepcs resource create openstack-nova-api systemd:openstack-nova-api --clone interleave=truepcs resource create openstack-nova-scheduler systemd:openstack-nova-scheduler --clone interleave=truepcs resource create openstack-nova-conductor systemd:openstack-nova-conductor --clone interleave=true# 下面几个order属性表示先启动 openstack-keystone-clone 然后启动openstack-nova-consoleauth-clone# 然后启动openstack-nova-novncproxy-clone，然后启动openstack-nova-api-clone，然后启动openstack-nova-scheduler-clone# 然后启动openstack-nova-conductor-clone# 下面几个colocation属性表示consoleauth约束着novncproxy资源的位置，也就是说consoleauth停止，则novncproxy停止。# 下面的几个colocation属性依次类推pcs constraint order start openstack-keystone-clone then openstack-nova-consoleauth-clonepcs constraint order start openstack-nova-consoleauth-clone then openstack-nova-novncproxy-clonepcs constraint colocation add openstack-nova-novncproxy-clone with openstack-nova-consoleauth-clonepcs constraint order start openstack-nova-novncproxy-clone then openstack-nova-api-clonepcs constraint colocation add openstack-nova-api-clone with openstack-nova-novncproxy-clonepcs constraint order start openstack-nova-api-clone then openstack-nova-scheduler-clonepcs constraint colocation add openstack-nova-scheduler-clone with openstack-nova-api-clonepcs constraint order start openstack-nova-scheduler-clone then openstack-nova-conductor-clonepcs constraint colocation add openstack-nova-conductor-clone with openstack-nova-scheduler-clonebash restart-pcs-cluster.sh### [任一节点]测试source keystonerc_adminopenstack compute service list 4、安装配置neutron集群(控制节点)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232# [任一节点]创建数据库mysql -uroot -popenstack -e &quot;CREATE DATABASE neutron;GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;neutron&quot;&apos;;GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;neutron&quot;&apos;;FLUSH PRIVILEGES;&quot;# [任一节点]创建用户等source /root/keystonerc_adminopenstack user create --domain default --password neutron neutronopenstack role add --project service --user neutron adminopenstack service create --name neutron --description &quot;OpenStack Networking&quot; networkopenstack endpoint create --region RegionOne network public http://controller:9696openstack endpoint create --region RegionOne network internal http://controller:9696openstack endpoint create --region RegionOne network admin http://controller:9696# 所有控制节点yum install -y openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch ebtables# [所有控制节点]配置neutron server，/etc/neutron/neutron.confopenstack-config --set /etc/neutron/neutron.conf DEFAULT bind_host 192.168.10.101openstack-config --set /etc/neutron/neutron.conf database connection mysql+pymysql://neutron:neutron@controller/neutronopenstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins routeropenstack-config --set /etc/neutron/neutron.conf DEFAULT allow_overlapping_ips Trueopenstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_password openstackopenstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name serviceopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutronopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken password neutronopenstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_status_changes Trueopenstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_data_changes Trueopenstack-config --set /etc/neutron/neutron.conf nova auth_url http://controller:35357openstack-config --set /etc/neutron/neutron.conf nova auth_type passwordopenstack-config --set /etc/neutron/neutron.conf nova project_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf nova user_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf nova region_name RegionOneopenstack-config --set /etc/neutron/neutron.conf nova project_name serviceopenstack-config --set /etc/neutron/neutron.conf nova username novaopenstack-config --set /etc/neutron/neutron.conf nova password novaopenstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp# [所有控制节点]配置ML2 plugin，/etc/neutron/plugins/ml2/ml2_conf.iniopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,vlan,vxlan,greopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types vxlanopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers openvswitch,l2populationopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers port_securityopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks externalopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vxlan vni_ranges 1:1000openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vlan network_vlan_ranges external:1:4090openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_security_group Trueopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset Trueopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup firewall_driver iptables_hybrid# [所有控制节点]配置Open vSwitch agent，/etc/neutron/plugins/ml2/openvswitch_agent.ini，注意，此处填写第二块网卡openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_security_group Trueopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_ipset Trueopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup firewall_driver iptables_hybridopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs integration_bridge br-intopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs tunnel_bridge br-tunopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs local_ip 10.0.0.1openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs bridge_mappings external:br-exopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent tunnel_types vxlanopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent l2_population True# [所有控制节点]配置L3 agent，/etc/neutron/l3_agent.iniopenstack-config --set /etc/neutron/l3_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriveropenstack-config --set /etc/neutron/l3_agent.ini DEFAULT external_network_bridge# [所有控制节点]配置DHCP agent，/etc/neutron/dhcp_agent.iniopenstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriveropenstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasqopenstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT enable_isolated_metadata True# [所有控制节点]配置metadata agent，/etc/neutron/metadata_agent.iniopenstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip 192.168.10.100openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret openstack# [所有控制节点]配置nova和neutron集成，/etc/nova/nova.confopenstack-config --set /etc/nova/nova.conf neutron url http://controller:9696openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf neutron auth_type passwordopenstack-config --set /etc/nova/nova.conf neutron project_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron user_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron region_name RegionOneopenstack-config --set /etc/nova/nova.conf neutron project_name serviceopenstack-config --set /etc/nova/nova.conf neutron username neutronopenstack-config --set /etc/nova/nova.conf neutron password neutronopenstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy Trueopenstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret openstack# [所有控制节点]配置L3 agent HA、/etc/neutron/neutron.confopenstack-config --set /etc/neutron/neutron.conf DEFAULT l3_ha Trueopenstack-config --set /etc/neutron/neutron.conf DEFAULT allow_automatic_l3agent_failover Trueopenstack-config --set /etc/neutron/neutron.conf DEFAULT max_l3_agents_per_router 3openstack-config --set /etc/neutron/neutron.conf DEFAULT min_l3_agents_per_router 2# [所有控制节点]配置DHCP agent HA、/etc/neutron/neutron.confopenstack-config --set /etc/neutron/neutron.conf DEFAULT dhcp_agents_per_network 3# [所有控制节点] 配置Open vSwitch (OVS) 服务，创建网桥和端口systemctl enable openvswitch.servicesystemctl start openvswitch.service# [所有控制节点] 创建ML2配置文件软连接ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.inivim /etc/haproxy/haproxy.cfglisten neutron_api_cluster bind 192.168.10.100:9696 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:9696 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:9696 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:9696 check inter 2000 rise 2 fall 5scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg# 备份原来配置文件cp /etc/sysconfig/network-scripts/ifcfg-ens160 /etc/sysconfig/network-scripts/bak-ifcfg-ens160echo &quot;DEVICE=br-exDEVICETYPE=ovsTYPE=OVSBridgeBOOTPROTO=staticIPADDR=$(cat /etc/sysconfig/network-scripts/ifcfg-ens160 |grep IPADDR|awk -F &apos;=&apos; &apos;&#123;print $2&#125;&apos;)NETMASK=$(cat /etc/sysconfig/network-scripts/ifcfg-ens160 |grep NETMASK|awk -F &apos;=&apos; &apos;&#123;print $2&#125;&apos;)GATEWAY=$(cat /etc/sysconfig/network-scripts/ifcfg-ens160 |grep GATEWAY|awk -F &apos;=&apos; &apos;&#123;print $2&#125;&apos;)DNS1=$(cat /etc/sysconfig/network-scripts/ifcfg-ens160 |grep DNS1|awk -F &apos;=&apos; &apos;&#123;print $2&#125;&apos;)DNS2=218.2.2.2ONBOOT=yes&quot;&gt;/etc/sysconfig/network-scripts/ifcfg-br-execho &quot;TYPE=OVSPortDEVICETYPE=ovsOVS_BRIDGE=br-exNAME=ens160DEVICE=ens160ONBOOT=yes&quot;&gt;/etc/sysconfig/network-scripts/ifcfg-ens160ovs-vsctl add-br br-exovs-vsctl add-port br-ex ens160systemctl restart network.service# 拷贝配置文件到其他控制节点并作相应修改scp /etc/neutron/neutron.conf controller02:/etc/neutron/neutron.confscp /etc/neutron/neutron.conf controller03:/etc/neutron/neutron.confscp /etc/neutron/plugins/ml2/ml2_conf.ini controller02:/etc/neutron/plugins/ml2/ml2_conf.iniscp /etc/neutron/plugins/ml2/ml2_conf.ini controller03:/etc/neutron/plugins/ml2/ml2_conf.iniscp /etc/neutron/plugins/ml2/openvswitch_agent.ini controller02:/etc/neutron/plugins/ml2/openvswitch_agent.iniscp /etc/neutron/plugins/ml2/openvswitch_agent.ini controller03:/etc/neutron/plugins/ml2/openvswitch_agent.iniscp /etc/neutron/l3_agent.ini controller02:/etc/neutron/l3_agent.iniscp /etc/neutron/l3_agent.ini controller03:/etc/neutron/l3_agent.iniscp /etc/neutron/dhcp_agent.ini controller02:/etc/neutron/dhcp_agent.iniscp /etc/neutron/dhcp_agent.ini controller03:/etc/neutron/dhcp_agent.iniscp /etc/neutron/metadata_agent.ini controller02:/etc/neutron/metadata_agent.iniscp /etc/neutron/metadata_agent.ini controller03:/etc/neutron/metadata_agent.ini# [任一节点]生成数据库su -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head&quot; neutron# [任一节点]添加pacemaker资源pcs resource create neutron-server systemd:neutron-server op start timeout=90 --clone interleave=truepcs constraint order start openstack-keystone-clone then neutron-server-clone# 全局唯一克隆：参数globally-unique=true。这些资源各不相同。一个节点上运行的克隆实例与另一个节点上运行的实例不同,同一个节点上运行的任何两个实例也不同。# clone-max: 在集群中最多能运行多少份克隆资源，默认和集群中的节点数相同； clone-node-max：每个节点上最多能运行多少份克隆资源，默认是1；pcs resource create neutron-scale ocf:neutron:NeutronScale --clone globally-unique=true clone-max=3 interleave=truepcs constraint order start neutron-server-clone then neutron-scale-clonepcs resource create neutron-ovs-cleanup ocf:neutron:OVSCleanup --clone interleave=truepcs resource create neutron-netns-cleanup ocf:neutron:NetnsCleanup --clone interleave=truepcs resource create neutron-openvswitch-agent systemd:neutron-openvswitch-agent --clone interleave=truepcs resource create neutron-dhcp-agent systemd:neutron-dhcp-agent --clone interleave=truepcs resource create neutron-l3-agent systemd:neutron-l3-agent --clone interleave=truepcs resource create neutron-metadata-agent systemd:neutron-metadata-agent --clone interleave=truepcs constraint order start neutron-scale-clone then neutron-ovs-cleanup-clonepcs constraint colocation add neutron-ovs-cleanup-clone with neutron-scale-clonepcs constraint order start neutron-ovs-cleanup-clone then neutron-netns-cleanup-clonepcs constraint colocation add neutron-netns-cleanup-clone with neutron-ovs-cleanup-clonepcs constraint order start neutron-netns-cleanup-clone then neutron-openvswitch-agent-clonepcs constraint colocation add neutron-openvswitch-agent-clone with neutron-netns-cleanup-clonepcs constraint order start neutron-openvswitch-agent-clone then neutron-dhcp-agent-clonepcs constraint colocation add neutron-dhcp-agent-clone with neutron-openvswitch-agent-clonepcs constraint order start neutron-dhcp-agent-clone then neutron-l3-agent-clonepcs constraint colocation add neutron-l3-agent-clone with neutron-dhcp-agent-clonepcs constraint order start neutron-l3-agent-clone then neutron-metadata-agent-clonepcs constraint colocation add neutron-metadata-agent-clone with neutron-l3-agent-clonebash restart-pcs-cluster.sh# [任一节点] 测试soource keystonerc_adminneutron ext-listneutron agent-listovs-vsctl showneutron agent-list 5、安装配置dashboard集群1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 所有节点安装yum install -y openstack-dashboard# [所有控制节点] 修改配置文件/etc/openstack-dashboard/local_settingssed -i \\ -e &apos;s#OPENSTACK_HOST =.*#OPENSTACK_HOST = &quot;&apos;&quot;192.168.10.101&quot;&apos;&quot;#g&apos; \\ -e &quot;s#ALLOWED_HOSTS.*#ALLOWED_HOSTS = [&apos;*&apos;,]#g&quot; \\ -e &quot;s#^CACHES#SESSION_ENGINE = &apos;django.contrib.sessions.backends.cache&apos;\\nCACHES#g#&quot; \\ -e &quot;s#locmem.LocMemCache&apos;#memcached.MemcachedCache&apos;,\\n &apos;LOCATION&apos; : [ &apos;controller01:11211&apos;, &apos;controller02:11211&apos;, &apos;controller03:11211&apos;, ]#g&quot; \\ -e &apos;s#^OPENSTACK_KEYSTONE_URL =.*#OPENSTACK_KEYSTONE_URL = &quot;http://%s:5000/v3&quot; % OPENSTACK_HOST#g&apos; \\ -e &quot;s/^#OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT.*/OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True/g&quot; \\ -e &apos;s/^#OPENSTACK_API_VERSIONS.*/OPENSTACK_API_VERSIONS = &#123;\\n &quot;identity&quot;: 3,\\n &quot;image&quot;: 2,\\n &quot;volume&quot;: 2,\\n&#125;\\n#OPENSTACK_API_VERSIONS = &#123;/g&apos; \\ -e &quot;s/^#OPENSTACK_KEYSTONE_DEFAULT_DOMAIN.*/OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &apos;default&apos;/g&quot; \\ -e &apos;s#^OPENSTACK_KEYSTONE_DEFAULT_ROLE.*#OPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;user&quot;#g&apos; \\ -e &quot;s#^LOCAL_PATH.*#LOCAL_PATH = &apos;/var/lib/openstack-dashboard&apos;#g&quot; \\ -e &quot;s#^SECRET_KEY.*#SECRET_KEY = &apos;4050e76a15dfb7755fe3&apos;#g&quot; \\ -e &quot;s#&apos;enable_ha_router&apos;.*#&apos;enable_ha_router&apos;: True,#g&quot; \\ -e &apos;s#TIME_ZONE = .*#TIME_ZONE = &quot;&apos;&quot;Asia/Shanghai&quot;&apos;&quot;#g&apos; \\ /etc/openstack-dashboard/local_settingsscp /etc/openstack-dashboard/local_settings controller02:/etc/openstack-dashboard/local_settingsscp /etc/openstack-dashboard/local_settings controller03:/etc/openstack-dashboard/local_settings# 在controller02上修改sed -i -e &apos;s#OPENSTACK_HOST =.*#OPENSTACK_HOST = &quot;&apos;&quot;192.168.10.102&quot;&apos;&quot;#g&apos; /etc/openstack-dashboard/local_settings# 在controiller03上修改sed -i -e &apos;s#OPENSTACK_HOST =.*#OPENSTACK_HOST = &quot;&apos;&quot;192.168.10.103&quot;&apos;&quot;#g&apos; /etc/openstack-dashboard/local_settings# [所有控制节点]echo &quot;COMPRESS_OFFLINE = True&quot; &gt;&gt; /etc/openstack-dashboard/local_settingspython /usr/share/openstack-dashboard/manage.py compress# [所有控制节点] 设置HTTPD在特定的IP上监听sed -i -e &apos;s/^Listen.*/Listen &apos;&quot;$(ip addr show dev br-ex scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;|head -n 1)&quot;&apos;:80/g&apos; /etc/httpd/conf/httpd.confvim /etc/haproxy/haproxy.cfglisten dashboard_cluster bind 192.168.10.100:80 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:80 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:80 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:80 check inter 2000 rise 2 fall 5scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg 6、安装配置cinder123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119# 所有控制节点yum install -y openstack-cinder# [任一节点]创建数据库mysql -uroot -popenstack -e &quot;CREATE DATABASE cinder;GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;cinder&quot;&apos;;GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;cinder&quot;&apos;;FLUSH PRIVILEGES;&quot;# [任一节点]创建用户等. /root/keystonerc_admin# [任一节点]创建用户等openstack user create --domain default --password cinder cinderopenstack role add --project service --user cinder adminopenstack service create --name cinder --description &quot;OpenStack Block Storage&quot; volumeopenstack service create --name cinderv2 --description &quot;OpenStack Block Storage&quot; volumev2#创建cinder服务的API endpointsopenstack endpoint create --region RegionOne volume public http://controller:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne volume internal http://controller:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne volume admin http://controller:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne volumev2 public http://controller:8776/v2/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne volumev2 internal http://controller:8776/v2/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne volumev2 admin http://controller:8776/v2/%\\(tenant_id\\)s#[所有控制节点] 修改/etc/cinder/cinder.confopenstack-config --set /etc/cinder/cinder.conf database connection mysql+pymysql://cinder:cinder@controller/cinderopenstack-config --set /etc/cinder/cinder.conf database max_retries -1openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/cinder/cinder.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_name serviceopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken username cinderopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken password cinderopenstack-config --set /etc/cinder/cinder.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_password openstackopenstack-config --set /etc/cinder/cinder.conf DEFAULT control_exchange cinderopenstack-config --set /etc/cinder/cinder.conf DEFAULT osapi_volume_listen $(ip addr show dev br-ex scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;|head -n 1)openstack-config --set /etc/cinder/cinder.conf DEFAULT my_ip $(ip addr show dev br-ex scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;|head -n 1)openstack-config --set /etc/cinder/cinder.conf oslo_concurrency lock_path /var/lib/cinder/tmpopenstack-config --set /etc/cinder/cinder.conf DEFAULT glance_api_servers http://controller:9292# [任一节点]生成数据库su -s /bin/sh -c &quot;cinder-manage db sync&quot; cinder# 所有控制节点修改计算节点配置openstack-config --set /etc/nova/nova.conf cinder os_region_name RegionOne# 重启计算节点 nova-api# pcs resource restart openstack-nova-api-clone# 安装配置存储节点 ，存储节点和控制节点复用# 所有节点yum install lvm2 -ysystemctl enable lvm2-lvmetad.servicesystemctl start lvm2-lvmetad.servicepvcreate /dev/sdbvgcreate cinder-volumes /dev/sdbyum install openstack-cinder targetcli python-keystone -y# 所有控制节点修改部分配置文件openstack-config --set /etc/cinder/cinder.conf lvm volume_driver cinder.volume.drivers.lvm.LVMVolumeDriveropenstack-config --set /etc/cinder/cinder.conf lvm volume_group cinder-volumesopenstack-config --set /etc/cinder/cinder.conf lvm iscsi_protocol iscsiopenstack-config --set /etc/cinder/cinder.conf lvm iscsi_helper lioadmopenstack-config --set /etc/cinder/cinder.conf DEFAULT enabled_backends lvm# 增加haproxy.cfg配置文件vim /etc/haproxy/haproxy.cfglisten cinder_api_cluster bind 192.168.10.100:8776 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:8776 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:8776 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:8776 check inter 2000 rise 2 fall 5scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg# [任一节点]添加pacemaker资源pcs resource create openstack-cinder-api systemd:openstack-cinder-api --clone interleave=truepcs resource create openstack-cinder-scheduler systemd:openstack-cinder-scheduler --clone interleave=truepcs resource create openstack-cinder-volume systemd:openstack-cinder-volumepcs constraint order start openstack-keystone-clone then openstack-cinder-api-clonepcs constraint order start openstack-cinder-api-clone then openstack-cinder-scheduler-clonepcs constraint colocation add openstack-cinder-scheduler-clone with openstack-cinder-api-clonepcs constraint order start openstack-cinder-scheduler-clone then openstack-cinder-volumepcs constraint colocation add openstack-cinder-volume with openstack-cinder-scheduler-clone# 重启集群bash restart-pcs-cluster.sh# [任一节点]测试. /root/keystonerc_admincinder service-list 7、安装配置ceilometer和aodh集群7.1 安装配置ceilometer集群实在无力吐槽这个项目，所以不想写了 7.2 安装配置aodh集群实在无力吐槽这个项目，所以不想写了 四、安装配置计算节点4.1 OpenStack Compute service123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 所有计算节点yum install -y openstack-nova-compute# 修改配置文件/etc/nova/nova.confopenstack-config --set /etc/nova/nova.conf DEFAULT my_ip $(ip addr show dev ens160 scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;)openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron Trueopenstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriveropenstack-config --set /etc/nova/nova.conf DEFAULT memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password openstackopenstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_name serviceopenstack-config --set /etc/nova/nova.conf keystone_authtoken username novaopenstack-config --set /etc/nova/nova.conf keystone_authtoken password novaopenstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmpopenstack-config --set /etc/nova/nova.conf vnc enabled Trueopenstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address $(ip addr show dev ens160 scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;)openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://192.168.10.100:6080/vnc_auto.htmlopenstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292openstack-config --set /etc/nova/nova.conf libvirt virt_type $(count=$(egrep -c &apos;(vmx|svm)&apos; /proc/cpuinfo); if [ $count -eq 0 ];then echo &quot;qemu&quot;; else echo &quot;kvm&quot;; fi)# 打开虚拟机迁移的监听端口sed -i -e &quot;s#\\#listen_tls *= *0#listen_tls = 0#g&quot; /etc/libvirt/libvirtd.confsed -i -e &quot;s#\\#listen_tcp *= *1#listen_tcp = 1#g&quot; /etc/libvirt/libvirtd.confsed -i -e &quot;s#\\#auth_tcp *= *\\&quot;sasl\\&quot;#auth_tcp = \\&quot;none\\&quot;#g&quot; /etc/libvirt/libvirtd.confsed -i -e &quot;s#\\#LIBVIRTD_ARGS *= *\\&quot;--listen\\&quot;#LIBVIRTD_ARGS=\\&quot;--listen\\&quot;#g&quot; /etc/sysconfig/libvirtd#启动服务systemctl enable libvirtd.service openstack-nova-compute.servicesystemctl start libvirtd.service openstack-nova-compute.service 4.2 OpenStack Network service1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 安装组件yum install -y openstack-neutron-openvswitch ebtables ipsetyum install -y openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch# 修改/etc/neutron/neutron.confopenstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_password openstackopenstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name serviceopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutronopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken password neutronopenstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp### 配置Open vSwitch agent，/etc/neutron/plugins/ml2/openvswitch_agent.ini，注意，此处填写第二块网卡openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_security_group Trueopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_ipset Trueopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup firewall_driver iptables_hybridopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs local_ip $(ip addr show dev ens192 scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;)openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent tunnel_types vxlanopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent l2_population False### 配置nova和neutron集成，/etc/nova/nova.confopenstack-config --set /etc/nova/nova.conf neutron url http://controller:9696openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf neutron auth_type passwordopenstack-config --set /etc/nova/nova.conf neutron project_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron user_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron region_name RegionOneopenstack-config --set /etc/nova/nova.conf neutron project_name serviceopenstack-config --set /etc/nova/nova.conf neutron username neutronopenstack-config --set /etc/nova/nova.conf neutron password neutronln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.inisystemctl restart openstack-nova-compute.servicesystemctl start openvswitch.servicesystemctl restart neutron-openvswitch-agent.servicesystemctl enable openvswitch.servicesystemctl enable neutron-openvswitch-agent.service","tags":[{"name":"Openstack","slug":"Openstack","permalink":"http://yoursite.com/tags/Openstack/"}]},{"title":"Neutron SDN 手动实现手册","date":"2017-04-13T15:37:00.000Z","path":"2017/04/13/手工实现SDN/","text":"openstack运维开发QQ群:94776000 本文旨在通过自己搭建类似neutron （openvswitch + gre） 实现SDN 的环境，学习了解其工作原理，模拟核心原理，比如：同一租户自定义网络 instance 互通，手动为instance 分配 floating ip 等相关内容。 虚拟网络需要新建3个虚拟网络Net0、Net1和Net2，其在virtual box 中对应配置如下。 1234567891011121314151617181920Net0: Network name: VirtualBox host-only Ethernet Adapter#2 Purpose: administrator / management network IP block: 10.20.0.0/24 DHCP: disable Linux device: eth0Net1: Network name: VirtualBox host-only Ethernet Adapter#3 Purpose: public network DHCP: disable IP block: 172.16.0.0/24 Linux device: eth1Net2： Network name: VirtualBox host-only Ethernet Adapter#4 Purpose: Storage/private network DHCP: disable IP block: 192.168.4.0/24 Linux device: eth2 虚拟机需要新建2个虚拟机VM1和VM2，其对应配置如下。 12345678910111213VM1： Name : network1 vCPU:1 Memory :1G Disk:30G Network:net1,net2,net3VM2： Name: compute1 vCPU:1 Memory :1G Disk:30G Networks:net1,net2,net3 ###Linux interface设置 123456789network1 eth0:10.20.0.201 (management network) eht1:172.16.0.201 (public/external network) eht2:192.168.4.201 (private network，gre tunning)compute1 eth0:10.20.0.202 (management network) eht1:(disabled) eht2:192.168.4.202 (private network，gre tunning) 模拟安装网络节点(Network1) 模拟Network 节点相关实现，比如L3、dhcp-agent实现，为了模拟多节点网络情况，这里Network同时也模拟一个计算节点，模拟M2 openvswitch 实现，上面运行instance1。 网络接口配置 1234567891011121314151617181920212223242526vi /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 TYPE=Ethernet ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=static IPADDR=10.20.0.201 NETMASK=255.255.255.0vi /etc/sysconfig/network-scripts/ifcfg-eth1 DEVICE=eth1 TYPE=Ethernet ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=static IPADDR=172.16.0.201 NETMASK=255.255.255.0vi /etc/sysconfig/network-scripts/ifcfg-eth2 DEVICE=eth2 TYPE=Ethernet ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=static IPADDR=192.168.4.201 NETMASK=255.255.255.0 重启网络服务 1service network restart 安装需要用到的包 1yum install libvirt openvswitch python-virtinst xauth tigervnc qemu-* -y 移除默认的libvirt 网络，方便清晰分析网络情况 123virsh net-destroy defaultvirsh net-autostart --disable defaultvirsh net-undefine default 设置允许ipforwarding 1234vi /etc/sysctl.conf net.ipv4.ip_forward=1 net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 立即生效 1sysctl -p 启动openvswitch 12service openvswitch startchkconfig openvswitch on 创建一个linux bridge 12brctl addbr qbr01ip link set qbr01 up 创建一个instance，并连接到qbr01 Bridge，网络接口部分配置如下 1234567&lt;interface type=&apos;bridge&apos;&gt; &lt;source bridge=&apos;qbr01&apos;/&gt; &lt;target dev=&apos;tap01&apos;/&gt; &lt;model type=&apos;virtio&apos;/&gt; &lt;driver name=&apos;qemu&apos;/&gt; &lt;address type=&apos;pci&apos; domain=&apos;0x0000&apos; bus=&apos;0x00&apos; slot=&apos;0x03&apos; function=&apos;0x0&apos;/&gt;&lt;/interface&gt; 可以参考附件./gre/instance1.xml创建 1234567cp ~/gre/ /var/tmp/cd /var/tmp/gremv cirros-0.3.0-x86_64-disk.img instance1.imgvirsh define instance1.xmlvirsh start instance1virsh vncdisplay instance1vncviewer :0 启动console 以后,登录添加ip 地址 192.168.1.11 12ip addr add 192.168.1.11/24 dev eth0route add default gw 192.168.1.1 创建一个内部bridge br-int， 模拟 OpenStack integrated bridge 12ovs-vsctl add-br br-intovs-vsctl add-port br-int gre0 -- set interface gre0 type=gre options:remote_ip=192.168.4.202 创建一个veth peer，连接Linux Bridge ‘qbr01’ 和 OpenvSwich Bridge ‘br-ini’ 123456ip link add qvo01 type veth peer name qvb01brctl addif qbr01 qvb01ovs-vsctl add-port br-int qvo01ovs-vsctl set port qvo01 tag=100ip link set qvb01 upip link set qvo01 up 查看现在network1上的 br-int 1ovs-vsctl show ##模拟安装计算节点(compute1) ##网络接口配置 1234567891011121314151617181920212223242526vi /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 TYPE=Ethernet ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=static IPADDR=10.20.0.202 NETMASK=255.255.255.0vi /etc/sysconfig/network-scripts/ifcfg-eth1 DEVICE=eth1 TYPE=Ethernet ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=static IPADDR=172.16.0.202 NETMASK=255.255.255.0vi /etc/sysconfig/network-scripts/ifcfg-eth2 DEVICE=eth2 TYPE=Ethernet ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=static IPADDR=192.168.4.202 NETMASK=255.255.255.0 重启网络服务 1service network restart 安装需要用到的包 1yum install libvirt openvswitch python-virtinst xauth tigervnc qemu-* 移除libvirt 默认的网络 123virsh net-destroy defaultvirsh net-autostart --disableu defaultvirsh net-undefine default 设置允许ipforwarding 1234vi /etc/sysctl.conf net.ipv4.ip_forward=1 net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 立即生效 1sysctl -p 启动openvswitch 12service openvswitch startchkconfig openvswitch on 创建一个linux bridge 12brctl addbr qbr02ip link set qbr02 up 创建一个vm，并连接到qbr02 1234567&lt;interface type=&apos;bridge&apos;&gt; &lt;source bridge=&apos;qbr02&apos;/&gt; &lt;target dev=&apos;tap02&apos;/&gt; &lt;model type=&apos;virtio&apos;/&gt; &lt;driver name=&apos;qemu&apos;/&gt; &lt;address type=&apos;pci&apos; domain=&apos;0x0000&apos; bus=&apos;0x00&apos; slot=&apos;0x03&apos; function=&apos;0x0&apos;/&gt;&lt;/interface&gt; 上gre目录到compute1 节点，可以参考附件./gre/instance2.xml创建 1234567cp ~/gre/ /var/tmp/cd /var/tmp/gremv cirros-0.3.0-x86_64-disk.img instance2.imgvirsh define instance2.xmlvirsh start instance2virsh vncdisplay instance2vncviewer :0 启动console 以后,登录添加ip得知 192.168.1.12 12ip addr add 192.168.1.12/24 dev eth0route add default gw 192.168.1.1 创建一个内部bridge br-int， 模拟 OpenStack integrated bridge 12ovs-vsctl add-br br-intovs-vsctl add-port br-int gre0 -- set interface gre0 type=gre options:remote_ip=192.168.4.201 创建一个veth peer，连接Linux Bridge ‘qbr02’ 和 OpenvSwich Bridge ‘br-ini’ 123456ip link add qvo02 type veth peer name qvb02brctl addif qbr02 qvb02ovs-vsctl add-port br-int qvo02ovs-vsctl set port qvo02 tag=100ip link set qvb02 upip link set qvo02 up 查看现在network1 上的 br-int 1ovs-vsctl show 检查是否能连通instance1，在instance2的控制台 1ping 192.168.1.11 ##通过 Network Namespace 实现租户私有网络互访 添加一个namespace，dhcp01用于隔离租户网络。 1ip netns add dhcp01 为私有网络192.168.1.0/24 ，在命名空间dhcp01 中 创建dhcp 服务 12345ovs-vsctl add-port br-int tapdhcp01 -- set interface tapdhcp01 type=internalovs-vsctl set port tapdhcp01 tag=100ip link set tapdhcp01 netns dhcp01ip netns exec dhcp01 ip addr add 192.168.1.2/24 dev tapdhcp01ip netns exec dhcp01 ip link set tapdhcp01 up 检查网络是否连通，在namespace 访问instance1 和 instance2 12ip netns exec dhcp01 ping 192.168.1.12ip netns exec dhcp01 ping 192.168.1.11 通过 Network Namespace 和Iptables 实现L3 router 1ovs-vsctl add-br br-ex 重新配置eth1 和 br-ex 123456789101112131415161718192021222324252627282930313233343536vi /etc/sysconfig/network-scripts/ifcfg-eth1 DEVICE=eth1 ONBOOT=yes BOOTPROTO=none PROMISC=yes MTU=1546 ################################### DEVICE=ens160 TYPE=OVSPort DEVICETYPE=ovs OVS_BRIDGE=br-ex ONBOOT=yes ####################################vi /etc/sysconfig/network-scripts/ifcfg-br-ex DEVICE=br-ex TYPE=Bridge ONBOOT=yes BOOTPROTO=none IPADDR0=172.16.0.201 PREFIX0=24 ####################################### DEVICE=br-ex ONBOOT=yes DEVICETYPE=ovs TYPE=OVSBridge BOOTPROTO=static IPADDR=192.168.2.134 NETMASK=255.255.255.0 GATEWAY=192.168.2.1 DNS1=218.2.2.2 ###################################### 重启启动网络服务 1ovs-vsctl add-port br-ex eth1 &amp;&amp; service network restart 检查网络，配置后是否连通 1ping 172.16.0.201 添加一个namespace，router01 用于路由和floating ip 分配 1ip netns add router01 在br-int添加一个接口，作为私有网络192.168.1.0/24的网关 123456ovs-vsctl add-port br-int qr01 -- set interface qr01 type=internalovs-vsctl set port qr01 tag=100ip link set qr01 netns router01ip netns exec router01 ip addr add 192.168.1.1/24 dev qr01ip netns exec router01 ip link set qr01 upip netns exec router01 ip link set lo up 在br-ex中添加一个接口，用于私网192.168.1.0/24设置下一跳地址 12345ovs-vsctl add-port br-ex qg01 -- set interface qg01 type=internalip link set qg01 netns router01ip netns exec router01 ip addr add 172.16.0.100/24 dev qg01 ip netns exec router01 ip link set qg01 upip netns exec router01 ip link set lo up 模拟分配floating ip 访问instance1 为instance1 192.168.1.11 分配floating ip，172.16.0.101 12345ip netns exec router01 ip addr add 172.16.0.101/32 dev qg01 ip netns exec router01 iptables -t nat -A OUTPUT -d 172.16.0.101/32 -j DNAT --to-destination 192.168.1.11ip netns exec router01 iptables -t nat -A PREROUTING -d 172.16.0.101/32 -j DNAT --to-destination 192.168.1.11ip netns exec router01 iptables -t nat -A POSTROUTING -s 192.168.1.11/32 -j SNAT --to-source 172.16.0.101ip netns exec router01 iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -j SNAT --to-source 172.16.0.100 测试floating ip 1ping 172.16.0.101 如果需要清除nat chain 1iptables -t nat -F 1234567891011ip netns exec router01 iptables -t nat -A OUTPUT -d 192.168.2.102/32 -j DNAT --to-destination 192.168.10.11ip netns exec router01 iptables -t nat -A PREROUTING -d 192.168.2.102/32 -j DNAT --to-destination 192.168.10.11ip netns exec router01 iptables -t nat -A POSTROUTING -s 192.168.10.11/32 -j SNAT --to-source 192.168.2.102ip netns exec router01 ip addr add 192.168.2.103/32 dev qg01ip netns exec router01 iptables -t nat -A OUTPUT -d 192.168.2.103/32 -j DNAT --to-destination 192.168.10.11ip netns exec router01 iptables -t nat -A PREROUTING -d 192.168.2.103/32 -j DNAT --to-destination 192.168.10.11ip netns exec router01 iptables -t nat -A POSTROUTING -s 192.168.10.11/32 -j SNAT --to-source 192.168.2.103ip netns exec router01 route add default gw 192.168.2.1ip netns exec router01 route -n","tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"packstack 安装配置openstack mitaka","date":"2017-03-30T01:20:00.000Z","path":"2017/03/30/packstack install openstack-mitaka/","text":"1、修改主机名 12345678hostnamectl set-hostname controllerhostname controllerhostnamectl set-hostname compute01hostname compute01hostnamectl set-hostname compute02hostname compute02 1234567cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.12.50 controller192.168.12.51 compute01192.168.12.52 compute02 2、安装基础yum源 123456yum install ntpdate -yecho &quot;*/5 * * * * /usr/sbin/ntpdate 192.168.2.161 &gt;/dev/null 2&gt;&amp;1&quot; &gt;&gt; /var/spool/cron/root/usr/sbin/ntpdate 192.168.2.161yum install wget -yrm -rf /etc/yum.repos.d/*wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 3、安装配置openstack源 123456789yum install -y centos-release-openstack-mitakavim CentOS-OpenStack-mitaka.repo [centos-openstack-mitaka]name=CentOS-7 - OpenStack mitakabaseurl=http://mirrors.aliyun.com/centos/7/cloud/$basearch/openstack-mitaka/gpgcheck=1enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-Cloud 4、安装配置openstack 1yum update -y 控制节点执行: CONFIG_PROVISION_DEMO=n 123yum install -y openstack-packstackpackstack --gen-answer-file=openstack.txt 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317[root@controller ~]# cat openstack.txt | egrep -v &quot;^$|^#&quot;[general]CONFIG_SSH_KEY=/root/.ssh/id_rsa.pubCONFIG_DEFAULT_PASSWORD=CONFIG_SERVICE_WORKERS=%&#123;::processorcount&#125;CONFIG_MARIADB_INSTALL=yCONFIG_GLANCE_INSTALL=yCONFIG_CINDER_INSTALL=yCONFIG_MANILA_INSTALL=nCONFIG_NOVA_INSTALL=yCONFIG_NEUTRON_INSTALL=yCONFIG_HORIZON_INSTALL=yCONFIG_SWIFT_INSTALL=yCONFIG_CEILOMETER_INSTALL=yCONFIG_AODH_INSTALL=yCONFIG_GNOCCHI_INSTALL=yCONFIG_SAHARA_INSTALL=nCONFIG_HEAT_INSTALL=nCONFIG_TROVE_INSTALL=nCONFIG_IRONIC_INSTALL=nCONFIG_CLIENT_INSTALL=yCONFIG_NTP_SERVERS=CONFIG_NAGIOS_INSTALL=yEXCLUDE_SERVERS=CONFIG_DEBUG_MODE=nCONFIG_CONTROLLER_HOST=192.168.12.50CONFIG_COMPUTE_HOSTS=192.168.12.51,192.168.12.52CONFIG_NETWORK_HOSTS=192.168.12.50CONFIG_VMWARE_BACKEND=nCONFIG_UNSUPPORTED=nCONFIG_USE_SUBNETS=nCONFIG_VCENTER_HOST=CONFIG_VCENTER_USER=CONFIG_VCENTER_PASSWORD=CONFIG_VCENTER_CLUSTER_NAMES=CONFIG_STORAGE_HOST=192.168.12.50CONFIG_SAHARA_HOST=192.168.12.50CONFIG_USE_EPEL=nCONFIG_REPO=CONFIG_ENABLE_RDO_TESTING=nCONFIG_RH_USER=CONFIG_SATELLITE_URL=CONFIG_RH_SAT6_SERVER=CONFIG_RH_PW=CONFIG_RH_OPTIONAL=yCONFIG_RH_PROXY=CONFIG_RH_SAT6_ORG=CONFIG_RH_SAT6_KEY=CONFIG_RH_PROXY_PORT=CONFIG_RH_PROXY_USER=CONFIG_RH_PROXY_PW=CONFIG_SATELLITE_USER=CONFIG_SATELLITE_PW=CONFIG_SATELLITE_AKEY=CONFIG_SATELLITE_CACERT=CONFIG_SATELLITE_PROFILE=CONFIG_SATELLITE_FLAGS=CONFIG_SATELLITE_PROXY=CONFIG_SATELLITE_PROXY_USER=CONFIG_SATELLITE_PROXY_PW=CONFIG_SSL_CACERT_FILE=/etc/pki/tls/certs/selfcert.crtCONFIG_SSL_CACERT_KEY_FILE=/etc/pki/tls/private/selfkey.keyCONFIG_SSL_CERT_DIR=~/packstackca/CONFIG_SSL_CACERT_SELFSIGN=yCONFIG_SELFSIGN_CACERT_SUBJECT_C=--CONFIG_SELFSIGN_CACERT_SUBJECT_ST=StateCONFIG_SELFSIGN_CACERT_SUBJECT_L=CityCONFIG_SELFSIGN_CACERT_SUBJECT_O=openstackCONFIG_SELFSIGN_CACERT_SUBJECT_OU=packstackCONFIG_SELFSIGN_CACERT_SUBJECT_CN=controllerCONFIG_SELFSIGN_CACERT_SUBJECT_MAIL=admin@controllerCONFIG_AMQP_BACKEND=rabbitmqCONFIG_AMQP_HOST=192.168.12.50CONFIG_AMQP_ENABLE_SSL=nCONFIG_AMQP_ENABLE_AUTH=nCONFIG_AMQP_NSS_CERTDB_PW=PW_PLACEHOLDERCONFIG_AMQP_AUTH_USER=amqp_userCONFIG_AMQP_AUTH_PASSWORD=PW_PLACEHOLDERCONFIG_MARIADB_HOST=192.168.12.50CONFIG_MARIADB_USER=rootCONFIG_MARIADB_PW=openstackCONFIG_KEYSTONE_DB_PW=38cddfa9ce6149c9CONFIG_KEYSTONE_DB_PURGE_ENABLE=TrueCONFIG_KEYSTONE_REGION=RegionOneCONFIG_KEYSTONE_ADMIN_TOKEN=e462f66d03974d8fa410cf76aae40da9CONFIG_KEYSTONE_ADMIN_EMAIL=root@localhostCONFIG_KEYSTONE_ADMIN_USERNAME=adminCONFIG_KEYSTONE_ADMIN_PW=adminCONFIG_KEYSTONE_DEMO_PW=demoCONFIG_KEYSTONE_API_VERSION=v2.0CONFIG_KEYSTONE_TOKEN_FORMAT=UUIDCONFIG_KEYSTONE_SERVICE_NAME=httpdCONFIG_KEYSTONE_IDENTITY_BACKEND=sqlCONFIG_KEYSTONE_LDAP_URL=ldap://192.168.12.50CONFIG_KEYSTONE_LDAP_USER_DN=CONFIG_KEYSTONE_LDAP_USER_PASSWORD=CONFIG_KEYSTONE_LDAP_SUFFIX=CONFIG_KEYSTONE_LDAP_QUERY_SCOPE=oneCONFIG_KEYSTONE_LDAP_PAGE_SIZE=-1CONFIG_KEYSTONE_LDAP_USER_SUBTREE=CONFIG_KEYSTONE_LDAP_USER_FILTER=CONFIG_KEYSTONE_LDAP_USER_OBJECTCLASS=CONFIG_KEYSTONE_LDAP_USER_ID_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_NAME_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_MAIL_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_ENABLED_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_ENABLED_MASK=-1CONFIG_KEYSTONE_LDAP_USER_ENABLED_DEFAULT=TRUECONFIG_KEYSTONE_LDAP_USER_ENABLED_INVERT=nCONFIG_KEYSTONE_LDAP_USER_ATTRIBUTE_IGNORE=CONFIG_KEYSTONE_LDAP_USER_DEFAULT_PROJECT_ID_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_ALLOW_CREATE=nCONFIG_KEYSTONE_LDAP_USER_ALLOW_UPDATE=nCONFIG_KEYSTONE_LDAP_USER_ALLOW_DELETE=nCONFIG_KEYSTONE_LDAP_USER_PASS_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_ENABLED_EMULATION_DN=CONFIG_KEYSTONE_LDAP_USER_ADDITIONAL_ATTRIBUTE_MAPPING=CONFIG_KEYSTONE_LDAP_GROUP_SUBTREE=CONFIG_KEYSTONE_LDAP_GROUP_FILTER=CONFIG_KEYSTONE_LDAP_GROUP_OBJECTCLASS=CONFIG_KEYSTONE_LDAP_GROUP_ID_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_GROUP_NAME_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_GROUP_MEMBER_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_GROUP_DESC_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_GROUP_ATTRIBUTE_IGNORE=CONFIG_KEYSTONE_LDAP_GROUP_ALLOW_CREATE=nCONFIG_KEYSTONE_LDAP_GROUP_ALLOW_UPDATE=nCONFIG_KEYSTONE_LDAP_GROUP_ALLOW_DELETE=nCONFIG_KEYSTONE_LDAP_GROUP_ADDITIONAL_ATTRIBUTE_MAPPING=CONFIG_KEYSTONE_LDAP_USE_TLS=nCONFIG_KEYSTONE_LDAP_TLS_CACERTDIR=CONFIG_KEYSTONE_LDAP_TLS_CACERTFILE=CONFIG_KEYSTONE_LDAP_TLS_REQ_CERT=demandCONFIG_GLANCE_DB_PW=b5b27691c2a14788CONFIG_GLANCE_KS_PW=3ed7ad2ece9146a9CONFIG_GLANCE_BACKEND=fileCONFIG_CINDER_DB_PW=dadf81afd3be45ccCONFIG_CINDER_DB_PURGE_ENABLE=TrueCONFIG_CINDER_KS_PW=68c0f33d48664240CONFIG_CINDER_BACKEND=lvmCONFIG_CINDER_VOLUMES_CREATE=yCONFIG_CINDER_VOLUMES_SIZE=20GCONFIG_CINDER_GLUSTER_MOUNTS=CONFIG_CINDER_NFS_MOUNTS=CONFIG_CINDER_NETAPP_LOGIN=CONFIG_CINDER_NETAPP_PASSWORD=CONFIG_CINDER_NETAPP_HOSTNAME=CONFIG_CINDER_NETAPP_SERVER_PORT=80CONFIG_CINDER_NETAPP_STORAGE_FAMILY=ontap_clusterCONFIG_CINDER_NETAPP_TRANSPORT_TYPE=httpCONFIG_CINDER_NETAPP_STORAGE_PROTOCOL=nfsCONFIG_CINDER_NETAPP_SIZE_MULTIPLIER=1.0CONFIG_CINDER_NETAPP_EXPIRY_THRES_MINUTES=720CONFIG_CINDER_NETAPP_THRES_AVL_SIZE_PERC_START=20CONFIG_CINDER_NETAPP_THRES_AVL_SIZE_PERC_STOP=60CONFIG_CINDER_NETAPP_NFS_SHARES=CONFIG_CINDER_NETAPP_NFS_SHARES_CONFIG=/etc/cinder/shares.confCONFIG_CINDER_NETAPP_VOLUME_LIST=CONFIG_CINDER_NETAPP_VFILER=CONFIG_CINDER_NETAPP_PARTNER_BACKEND_NAME=CONFIG_CINDER_NETAPP_VSERVER=CONFIG_CINDER_NETAPP_CONTROLLER_IPS=CONFIG_CINDER_NETAPP_SA_PASSWORD=CONFIG_CINDER_NETAPP_ESERIES_HOST_TYPE=linux_dm_mpCONFIG_CINDER_NETAPP_WEBSERVICE_PATH=/devmgr/v2CONFIG_CINDER_NETAPP_STORAGE_POOLS=CONFIG_IRONIC_DB_PW=PW_PLACEHOLDERCONFIG_IRONIC_KS_PW=PW_PLACEHOLDERCONFIG_NOVA_DB_PURGE_ENABLE=TrueCONFIG_NOVA_DB_PW=588239b4fe244aebCONFIG_NOVA_KS_PW=3f355b76a92d41f1CONFIG_NOVA_SCHED_CPU_ALLOC_RATIO=16.0CONFIG_NOVA_SCHED_RAM_ALLOC_RATIO=1.5CONFIG_NOVA_COMPUTE_MIGRATE_PROTOCOL=tcpCONFIG_NOVA_COMPUTE_MANAGER=nova.compute.manager.ComputeManagerCONFIG_VNC_SSL_CERT=CONFIG_VNC_SSL_KEY=CONFIG_NOVA_PCI_ALIAS=CONFIG_NOVA_PCI_PASSTHROUGH_WHITELIST=CONFIG_NOVA_LIBVIRT_VIRT_TYPE=%&#123;::default_hypervisor&#125;CONFIG_NOVA_COMPUTE_PRIVIF=CONFIG_NOVA_NETWORK_MANAGER=nova.network.manager.FlatDHCPManagerCONFIG_NOVA_NETWORK_PUBIF=eth0CONFIG_NOVA_NETWORK_PRIVIF=CONFIG_NOVA_NETWORK_FIXEDRANGE=192.168.32.0/22CONFIG_NOVA_NETWORK_FLOATRANGE=10.3.4.0/22CONFIG_NOVA_NETWORK_AUTOASSIGNFLOATINGIP=nCONFIG_NOVA_NETWORK_VLAN_START=100CONFIG_NOVA_NETWORK_NUMBER=1CONFIG_NOVA_NETWORK_SIZE=255CONFIG_NEUTRON_KS_PW=e6c0896ef2fa412cCONFIG_NEUTRON_DB_PW=1202d0c54fdb4f5dCONFIG_NEUTRON_L3_EXT_BRIDGE=br-exCONFIG_NEUTRON_METADATA_PW=fd7e182acbc842e3CONFIG_LBAAS_INSTALL=nCONFIG_NEUTRON_METERING_AGENT_INSTALL=yCONFIG_NEUTRON_FWAAS=nCONFIG_NEUTRON_VPNAAS=nCONFIG_NEUTRON_ML2_TYPE_DRIVERS=vxlanCONFIG_NEUTRON_ML2_TENANT_NETWORK_TYPES=vxlanCONFIG_NEUTRON_ML2_MECHANISM_DRIVERS=openvswitchCONFIG_NEUTRON_ML2_FLAT_NETWORKS=*CONFIG_NEUTRON_ML2_VLAN_RANGES=CONFIG_NEUTRON_ML2_TUNNEL_ID_RANGES=CONFIG_NEUTRON_ML2_VXLAN_GROUP=CONFIG_NEUTRON_ML2_VNI_RANGES=10:100CONFIG_NEUTRON_L2_AGENT=openvswitchCONFIG_NEUTRON_ML2_SUPPORTED_PCI_VENDOR_DEVS=[&apos;15b3:1004&apos;, &apos;8086:10ca&apos;]CONFIG_NEUTRON_ML2_SRIOV_AGENT_REQUIRED=nCONFIG_NEUTRON_ML2_SRIOV_INTERFACE_MAPPINGS=CONFIG_NEUTRON_LB_INTERFACE_MAPPINGS=CONFIG_NEUTRON_OVS_BRIDGE_MAPPINGS=CONFIG_NEUTRON_OVS_BRIDGE_IFACES=CONFIG_NEUTRON_OVS_BRIDGES_COMPUTE=CONFIG_NEUTRON_OVS_TUNNEL_IF=CONFIG_NEUTRON_OVS_TUNNEL_SUBNETS=CONFIG_NEUTRON_OVS_VXLAN_UDP_PORT=4789CONFIG_MANILA_DB_PW=PW_PLACEHOLDERCONFIG_MANILA_KS_PW=PW_PLACEHOLDERCONFIG_MANILA_BACKEND=genericCONFIG_MANILA_NETAPP_DRV_HANDLES_SHARE_SERVERS=falseCONFIG_MANILA_NETAPP_TRANSPORT_TYPE=httpsCONFIG_MANILA_NETAPP_LOGIN=adminCONFIG_MANILA_NETAPP_PASSWORD=CONFIG_MANILA_NETAPP_SERVER_HOSTNAME=CONFIG_MANILA_NETAPP_STORAGE_FAMILY=ontap_clusterCONFIG_MANILA_NETAPP_SERVER_PORT=443CONFIG_MANILA_NETAPP_AGGREGATE_NAME_SEARCH_PATTERN=(.*)CONFIG_MANILA_NETAPP_ROOT_VOLUME_AGGREGATE=CONFIG_MANILA_NETAPP_ROOT_VOLUME_NAME=rootCONFIG_MANILA_NETAPP_VSERVER=CONFIG_MANILA_GENERIC_DRV_HANDLES_SHARE_SERVERS=trueCONFIG_MANILA_GENERIC_VOLUME_NAME_TEMPLATE=manila-share-%sCONFIG_MANILA_GENERIC_SHARE_MOUNT_PATH=/sharesCONFIG_MANILA_SERVICE_IMAGE_LOCATION=https://www.dropbox.com/s/vi5oeh10q1qkckh/ubuntu_1204_nfs_cifs.qcow2CONFIG_MANILA_SERVICE_INSTANCE_USER=ubuntuCONFIG_MANILA_SERVICE_INSTANCE_PASSWORD=ubuntuCONFIG_MANILA_NETWORK_TYPE=neutronCONFIG_MANILA_NETWORK_STANDALONE_GATEWAY=CONFIG_MANILA_NETWORK_STANDALONE_NETMASK=CONFIG_MANILA_NETWORK_STANDALONE_SEG_ID=CONFIG_MANILA_NETWORK_STANDALONE_IP_RANGE=CONFIG_MANILA_NETWORK_STANDALONE_IP_VERSION=4CONFIG_MANILA_GLUSTERFS_SERVERS=CONFIG_MANILA_GLUSTERFS_NATIVE_PATH_TO_PRIVATE_KEY=CONFIG_MANILA_GLUSTERFS_VOLUME_PATTERN=CONFIG_MANILA_GLUSTERFS_TARGET=CONFIG_MANILA_GLUSTERFS_MOUNT_POINT_BASE=CONFIG_MANILA_GLUSTERFS_NFS_SERVER_TYPE=glusterCONFIG_MANILA_GLUSTERFS_PATH_TO_PRIVATE_KEY=CONFIG_MANILA_GLUSTERFS_GANESHA_SERVER_IP=CONFIG_HORIZON_SSL=nCONFIG_HORIZON_SECRET_KEY=82c2cbbf5e744a4785e2d0ddd3ec9b80CONFIG_HORIZON_SSL_CERT=CONFIG_HORIZON_SSL_KEY=CONFIG_HORIZON_SSL_CACERT=CONFIG_SWIFT_KS_PW=d7b6a1ec2c8f4334CONFIG_SWIFT_STORAGES=CONFIG_SWIFT_STORAGE_ZONES=1CONFIG_SWIFT_STORAGE_REPLICAS=1CONFIG_SWIFT_STORAGE_FSTYPE=ext4CONFIG_SWIFT_HASH=67485c805b524830CONFIG_SWIFT_STORAGE_SIZE=2GCONFIG_HEAT_DB_PW=PW_PLACEHOLDERCONFIG_HEAT_AUTH_ENC_KEY=ac8680aafaca4fa9CONFIG_HEAT_KS_PW=PW_PLACEHOLDERCONFIG_HEAT_CLOUDWATCH_INSTALL=nCONFIG_HEAT_CFN_INSTALL=nCONFIG_HEAT_DOMAIN=heatCONFIG_HEAT_DOMAIN_ADMIN=heat_adminCONFIG_HEAT_DOMAIN_PASSWORD=PW_PLACEHOLDERCONFIG_PROVISION_DEMO=nCONFIG_PROVISION_TEMPEST=nCONFIG_PROVISION_DEMO_FLOATRANGE=172.24.4.224/28CONFIG_PROVISION_IMAGE_NAME=cirrosCONFIG_PROVISION_IMAGE_URL=http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgCONFIG_PROVISION_IMAGE_FORMAT=qcow2CONFIG_PROVISION_IMAGE_SSH_USER=cirrosCONFIG_PROVISION_UEC_IMAGE_NAME=cirros-uecCONFIG_PROVISION_UEC_IMAGE_KERNEL_URL=http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-kernelCONFIG_PROVISION_UEC_IMAGE_RAMDISK_URL=http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-initramfsCONFIG_PROVISION_UEC_IMAGE_DISK_URL=http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgCONFIG_TEMPEST_HOST=CONFIG_PROVISION_TEMPEST_USER=CONFIG_PROVISION_TEMPEST_USER_PW=PW_PLACEHOLDERCONFIG_PROVISION_TEMPEST_FLOATRANGE=172.24.4.224/28CONFIG_PROVISION_TEMPEST_REPO_URI=https://github.com/openstack/tempest.gitCONFIG_PROVISION_TEMPEST_REPO_REVISION=masterCONFIG_RUN_TEMPEST=nCONFIG_RUN_TEMPEST_TESTS=smokeCONFIG_PROVISION_OVS_BRIDGE=yCONFIG_GNOCCHI_DB_PW=9e46b3492cd441aeCONFIG_GNOCCHI_KS_PW=553369ab22294506CONFIG_CEILOMETER_SECRET=7d41192be1264a47CONFIG_CEILOMETER_KS_PW=2b56ad68aab341a0CONFIG_CEILOMETER_SERVICE_NAME=httpdCONFIG_CEILOMETER_COORDINATION_BACKEND=redisCONFIG_CEILOMETER_METERING_BACKEND=databaseCONFIG_MONGODB_HOST=192.168.12.50CONFIG_REDIS_MASTER_HOST=192.168.12.50CONFIG_REDIS_PORT=6379CONFIG_REDIS_HA=nCONFIG_REDIS_SLAVE_HOSTS=CONFIG_REDIS_SENTINEL_HOSTS=CONFIG_REDIS_SENTINEL_CONTACT_HOST=CONFIG_REDIS_SENTINEL_PORT=26379CONFIG_REDIS_SENTINEL_QUORUM=2CONFIG_REDIS_MASTER_NAME=mymasterCONFIG_AODH_KS_PW=d6881f5b3b0e4ce2CONFIG_TROVE_DB_PW=PW_PLACEHOLDERCONFIG_TROVE_KS_PW=PW_PLACEHOLDERCONFIG_TROVE_NOVA_USER=troveCONFIG_TROVE_NOVA_TENANT=servicesCONFIG_TROVE_NOVA_PW=PW_PLACEHOLDERCONFIG_SAHARA_DB_PW=PW_PLACEHOLDERCONFIG_SAHARA_KS_PW=PW_PLACEHOLDERCONFIG_NAGIOS_PW=383e86a1cd4348b5 5、安装 1packstack --answer-file=openstack.txt 6、安装后的修改 1234vim /etc/neutron/plugins/ml2/ml2_conf.ini 修改控制节点网络配置:[ml2_type_flat]flat_networks = * 1234567891011121314151617修改控制节点网卡配置:[root@controller network-scripts(keystone_admin)]# cat ifcfg-ens160 DEVICE=ens160TYPE=OVSPortDEVICETYPE=ovsOVS_BRIDGE=br-exONBOOT=yes[root@controller network-scripts(keystone_admin)]# cat ifcfg-br-ex DEVICE=br-exONBOOT=yesDEVICETYPE=ovsTYPE=OVSBridgeBOOTPROTO=staticIPADDR=192.168.12.50NETMASK=255.255.255.0GATEWAY=192.168.12.1DNS1=218.2.2.2 #####################为2个计算节点挂载共享存储，本次是nfs 12345678910111213141516171819202122232425262728293031323334353637383940找一个硬盘分区后，此次是sdc1挂载到/opt/nova/instances上[root@cinder ~]# cat /etc/fstab /dev/mapper/centos-root / xfs defaults 0 0UUID=06e34a87-b0fc-40e5-aae2-fa4d136543fc /boot xfs defaults 0 0/dev/mapper/centos-swap swap swap defaults 0 0/dev/sdc1 /opt/nova/instances xfs defaults 0 0格式化/dev/sdc1:mkfs.xfs /dev/sdb1修改过/etc/fstab后mount -a 即可生效yum -y install nfs-utils rpcbind -yvim /etc/exports/opt/nova/instances 192.168.12.0/24(rw,no_root_squash,no_all_squash,sync)exportfs -rsystemctl enable rpcbind.servicesystemctl start rpcbind.servicesystemctl enable nfs-server.service systemctl start nfs-server.service 2、2个nova节点查看showmount -e 192.168.12.50mount -t nfs 192.168.12.50:/opt/nova/instances /var/lib/nova/instances/mount -a2个nova节点：chown -R nova.nova /var/lib/nova","tags":[{"name":"Openstack","slug":"Openstack","permalink":"http://yoursite.com/tags/Openstack/"}]},{"title":"devstack mitaka 安装配置","date":"2017-03-29T15:37:00.000Z","path":"2017/03/29/devstack/","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551、操作系统ubuntu 14.04 2、安装方式 devstack 3、安装版本 openstack mitaka版本4、安装devstack 步骤1) 目前国内已经提供了完整的OpenStack的github的mirrorhttp://git.trystack.cn2) 另外devstack还会下载image，下载的过程也是非常缓慢。trystack也提供大家常用的image下载。http://images.trystack.cn3) 设置源sudo vim /etc/apt/sources.listdeb http://cn.archive.ubuntu.com/ubuntu/ trusty main restricted universe multiversedeb http://cn.archive.ubuntu.com/ubuntu/ trusty-security main restricted universe multiversedeb http://cn.archive.ubuntu.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://cn.archive.ubuntu.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://cn.archive.ubuntu.com/ubuntu/ trusty-backports main restricted universe multiversesudo apt-get updatesudo ntpdate 192.168.2.161sudo apt-get install git -ysudo git clone http://git.trystack.cn/openstack-dev/devstack.git -b stable/mitaka# 目前Devstack脚本已经不支持直接使用root身份运行，你需要创建stack用户运行cd devstack/tools/sudo ./create-stack-user.shsudo passwd stack# 修改devstack目录权限,让stack用户可以运行sudo chown -R stack:stack devstacksudo chmod 777 /dev/pts/0su stackcd devstackvim local.confstack@ubuntu:/home/wanstack/devstack$ cat local.conf [[local|localrc]]# use TryStack git mirrorGIT_BASE=http://git.trystack.cnNOVNC_REPO=http://git.trystack.cn/kanaka/noVNC.gitSPICE_REPO=http://git.trystack.cn/git/spice/spice-html5.git#OFFLINE=TrueRECLONE=False# Define images to be automatically downloaded during the DevStack built process.DOWNLOAD_DEFAULT_IMAGES=FalseIMAGE_URLS=&quot;http://images.trystack.cn/cirros/cirros-0.3.4-x86_64-disk.img&quot;HOST_IP=192.168.12.10# CredentialsDATABASE_PASSWORD=openstackADMIN_PASSWORD=openstackSERVICE_PASSWORD=openstackSERVICE_TOKEN=openstackRABBIT_PASSWORD=openstackHORIZON_BRANCH=stable/mitakaKEYSTONE_BRANCH=stable/mitakaNOVA_BRANCH=stable/mitakaNEUTRON_BRANCH=stable/mitakaGLANCE_BRANCH=stable/mitakaCINDER_BRANCH=stable/mitakaCEILOMETER_BRANCH=stable/mitakaAODH_BRANCH=stable/mitakaenable_plugin ceilometer https://git.openstack.org/openstack/ceilometer stable/mitakaenable_plugin aodh https://git.openstack.org/openstack/aodh stable/mitaka#keystoneKEYSTONE_TOKEN_FORMAT=UUID##HeatHEAT_BRANCH=stable/mitakaenable_service h-eng h-api h-api-cfn h-api-cw## SwiftSWIFT_BRANCH=stable/mitakaENABLED_SERVICES+=,s-proxy,s-object,s-container,s-accountSWIFT_REPLICAS=1SWIFT_HASH=011688b44136573e209e# Enabling Neutron (network) Servicedisable_service n-netenable_service q-svcenable_service q-agtenable_service q-dhcpenable_service q-l3enable_service q-metaenable_service q-meteringenable_service neutron## Neutron optionsQ_USE_SECGROUP=TrueFLOATING_RANGE=&quot;192.168.12.0/24&quot;FIXED_RANGE=&quot;10.0.0.0/24&quot;Q_FLOATING_ALLOCATION_POOL=start=192.168.12.110,end=192.168.12.120PUBLIC_NETWORK_GATEWAY=&quot;192.168.12.1&quot;Q_L3_ENABLED=TruePUBLIC_INTERFACE=eth0Q_USE_PROVIDERNET_FOR_PUBLIC=TrueOVS_PHYSICAL_BRIDGE=br-exPUBLIC_BRIDGE=br-exOVS_BRIDGE_MAPPINGS=public:br-ex# #VLAN configuration.Q_PLUGIN=ml2ENABLE_TENANT_VLANS=True# LoggingLOGFILE=/opt/stack/logs/stack.sh.logVERBOSE=TrueLOG_COLOR=TrueSCREEN_LOGDIR=/opt/stack/logs报错:File &quot;/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/response.py&quot;, line 246, in _error_catcher2017-03-21 07:54:11.931 | raise ReadTimeoutError(self._pool, None, &apos;Read timed out.&apos;)2017-03-21 07:54:11.931 | ReadTimeoutError: HTTPSConnectionPool(host=&apos;pypi.python.org&apos;, port=443): Read timed out.报错解决:./stack.sh: line 488: generate-subunit: command not foundsudo apt-get install python-pip -ysudo pip install --upgrade pipsudo pip install -U os-testr报错:File &quot;/usr/local/lib/python2.7/dist-packages/openstack/session.py&quot;, line 29, in &lt;module&gt;2017-03-20 04:52:02.612 | DEFAULT_USER_AGENT = &quot;openstacksdk/%s&quot; % openstack.__version__2017-03-20 04:52:02.612 | AttributeError: &apos;module&apos; object has no attribute &apos;__version__&apos;2017-03-20 04:52:02.643 | +lib/keystone:create_keystone_accounts:373 admin_tenant=处理:root@ubuntu:~# python &gt;&gt;&gt; import openstack &gt;&gt;&gt; import pbr.version &gt;&gt;&gt; print(pbr.version.VersionInfo(&apos;openstacksdk&apos;).version_string()) 8 0.8.1 &gt;&gt;&gt; quit() vim /usr/local/lib/python2.7/dist-packages/openstack/session.py修改为： DEFAULT_USER_AGENT = &quot;openstacksdk/0.8.1&quot;./stack.sh 在ubuntu上搭建sambasudo apt-get install samba samba-common -y 我们把devstack安装在了/opt目录下，所以共享这个目录#sudo chmod -R 777 /optsudo useradd openstacksudo smbpasswd -a openstacksudo vim /etc/samba/smb.conf在文件末尾添加：[openstack]comment=openstackpublic=yesbrowseable = yespath=/optread only = no forceuser=rootforcegroup=root #################################################### 重启服务sudo /etc/init.d/smbd restart ```","tags":[{"name":"Openstack","slug":"Openstack","permalink":"http://yoursite.com/tags/Openstack/"}]},{"title":"python代码调试工具","date":"2017-03-29T15:37:00.000Z","path":"2017/03/29/python 调试工具/","text":"以前都是用print或者log来调试程序,在小规模的程序下很方便，但是遇到向openstack这种项目级别的程序就太尴尬了。于是找到了ipdb 1、安装 1pip install ipdb 2、使用方法有两种使用方法,一种是不用改变程序直接用ipdb单步执行Python程序,第二种是在程序里标记断点,进行调试. 1) 第一种方法 1python -m ipdb xxx.py 2) 第二种方法 在需要断点的地方插入 12from ipdb import set_traceset_trace() 3、常用命令 n(下一个) ENTER(重复上次命令) q(退出) p&lt;变量&gt;(打印变量) c(继续) l(查找当前位于哪里) s(进入子程序) r(运行直到子程序结束) ! h(帮助) 如果上面的工具觉得好用的话，下面这款工具真心觉得用的爽。 1、安装1pip install pudb 2、在代码的入口处插入 1from pudb import set_trace; set_trace() 3、调试代码 1sudo pudb bp.py 4、简单使用 1Ctrl-p可以配置pudb: 是否显示行号, 选择主题, 内置Python解释器的类别 n: next，也就是执行一步 s: step into，进入函数内部 c: continue b: break point，断点 !: python command line ?: help","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"关于感情","date":"2017-03-29T15:37:00.000Z","path":"2017/03/29/关于生活/","text":"懵懂无知急功近利说爱情,花前月下纸醉金迷遭人叛,平平淡淡知足常乐即是福。 人生若是初相见是美好。 如果在感情的路上遇到过伤害, 请务必谢谢那些伤害过你的人。不要试图伤害她们, 一念成佛,一念成魔, 没有他(她)们你也只是停留在懵懂状态。 珍惜那些跟你一起粗茶淡饭的人,只有他们才愿意和你长相厮守。没有任何前提, 没有如果, 更没有长期投资。","tags":[{"name":"Life","slug":"Life","permalink":"http://yoursite.com/tags/Life/"}]},{"title":"ceilometer-mitaka api分析","date":"2017-03-29T14:36:22.000Z","path":"2017/03/29/ceilometer-mitaka api分析/","text":"本文的讲解是基于目前ceilometer的mitaka版本 ceilometer –debug meter-list的调用流程分析 大概过程 python-ceilometerclient ceilometer –debug meter-list curl GET http://192.168.12.10:8777/v2/meters Python Application的创建 HTTP请求的解析 RootController V2Controller MetersController 实际去获取数据的两个函数 pecan.request.storage_conn.get_meters的执行 Meter.from_db_model的执行 curl GET http://192.168.12.10:8777/v2/meters 这句很重要，下面我们就分析ceilometer收到这个HTTP请求后是如何解析，如何使用Python Application去查询MongoDB数据库的 1、Python Application的创建过程需要使用Python Application来接收HTTP请求，因此需要先创建Python Application。下面我们介绍使用Pecan+PasetDeploy创建Python Application的过程。 1在配置文件api_pasete.ini中我们可以看到，PasteDeploy会调用ceilometer.api.app:app_factory。 123456789101112131415[pipeline:main]pipeline = cors request_id authtoken api-server[app:api-server]paste.app_factory = ceilometer.api.app:app_factory[filter:authtoken]paste.filter_factory = keystonemiddleware.auth_token:filter_factory[filter:request_id]paste.filter_factory = oslo_middleware:RequestId.factory[filter:cors]paste.filter_factory = oslo_middleware.cors:filter_factoryoslo_config_project = ceilometer 根据[app:api-server] 我们得知app_factory函数的具体位置返回的是setup_app() 12def app_factory(global_config, **local_conf): return setup_app() setup_app函数就是真正创建Python Application的函数，在此函数中最重要的就是调用了pecan.make_app函数，在此函数中最重要的就是指定了解析HTTP Request的RootController，是通过pecan_config.app.root参数指定的；另外一个重要的地方就是hoooks.DBHook，在这里面初始化了数据库的链接，关于这一点后面再做介绍。 1234567891011121314151617181920212223242526272829303132def setup_app(pecan_config=None): # FIXME: Replace DBHook with a hooks.TransactionHook app_hooks = [hooks.ConfigHook(), hooks.DBHook(), hooks.NotifierHook(), hooks.TranslationHook()] pecan_config = pecan_config or &#123; &quot;app&quot;: &#123; &apos;root&apos;: &apos;ceilometer.api.controllers.root.RootController&apos;, &apos;modules&apos;: [&apos;ceilometer.api&apos;], &#125; &#125; pecan.configuration.set_config(dict(pecan_config), overwrite=True) # NOTE(sileht): pecan debug won&apos;t work in multi-process environment pecan_debug = CONF.api.pecan_debug if CONF.api.workers and CONF.api.workers != 1 and pecan_debug: pecan_debug = False LOG.warning(_LW(&apos;pecan_debug cannot be enabled, if workers is &gt; 1, &apos; &apos;the value is overrided with False&apos;)) app = pecan.make_app( pecan_config[&apos;app&apos;][&apos;root&apos;], debug=pecan_debug, hooks=app_hooks, wrap_app=middleware.ParsableErrorMiddleware, guess_content_type_from_ext=False ) return app 在上面我们提到解析HTTP请求的RootController是通过pecan_config.app.root参数指定的，阅读代码可知pecan_config.app.root就pecan_config中指定的，也就是ceilometer.api.controllers.root.RootController。这样Python Application就创建完成了，并且指定好了解析HTTP Request的RootController，也就是说/v2/meters/从RootController开始处理。 2、HTTP请求的解析过程在RootController我们可以看到它先创建了一个类属性:v2，于是所有的以/v2开头的HTTP Request都会由V2Controller来处理，/v2/meters/当然也不例外 12345678910111213class RootController(object): def __init__(self): self.v2 = v2.V2Controller() @pecan.expose(&apos;json&apos;) def index(self): base_url = pecan.request.application_url available = [&#123;&apos;tag&apos;: &apos;v2&apos;, &apos;date&apos;: &apos;2013-02-13T00:00:00Z&apos;, &#125;] collected = [version_descriptor(base_url, v[&apos;tag&apos;], v[&apos;date&apos;]) for v in available] versions = &#123;&apos;versions&apos;: &#123;&apos;values&apos;: collected&#125;&#125; return versions 下面我们再去看V2Controller，我们可以看到它有类属性:event_types,events,capabilities，也就是说/v2/event_types会由EventTypesController来处理，/v2/events/会由EventsController来处理，/v2/capabilities/会由CapabilitiesController来处理，那/v2/meters/呢？往下看，在_lookup函数中我们可以看到/v2/meters/会由MetersController来处理，关于__lookup可以参看官方文档。 123456class V2Controller(object): &quot;&quot;&quot;Version 2 API controller root.&quot;&quot;&quot; event_types = events.EventTypesController() events = events.EventsController() capabilities = capabilities.CapabilitiesController() 12345678910111213141516171819202122232425262728@pecan.expose() def _lookup(self, kind, *remainder): if (kind in [&apos;meters&apos;, &apos;resources&apos;, &apos;samples&apos;] and self.gnocchi_is_enabled): if kind == &apos;meters&apos; and pecan.request.method == &apos;POST&apos;: direct = pecan.request.params.get(&apos;direct&apos;, &apos;&apos;) if strutils.bool_from_string(direct): pecan.abort(400, _(&apos;direct option cannot be true when &apos; &apos;Gnocchi is enabled.&apos;)) return meters.MetersController(), remainder gnocchi_abort() elif kind == &apos;meters&apos;: return meters.MetersController(), remainder elif kind == &apos;resources&apos;: return resources.ResourcesController(), remainder elif kind == &apos;samples&apos;: return samples.SamplesController(), remainder elif kind == &apos;query&apos;: return QueryController( gnocchi_is_enabled=self.gnocchi_is_enabled, aodh_url=self.aodh_url, ), remainder elif kind == &apos;alarms&apos; and (not self.aodh_url): aodh_abort() elif kind == &apos;alarms&apos; and self.aodh_url: aodh_redirect(self.aodh_url) else: pecan.abort(404) 因为我们是meters所以最终会执行1return meters.MetersController(), remainder 终于到了MetersController，这里就是最终处理HTTP请求:/v2/meters/的地方 12345678910111213141516171819202122232425262728class MetersController(rest.RestController): &quot;&quot;&quot;Works on meters.&quot;&quot;&quot; @pecan.expose() def _lookup(self, meter_name, *remainder): return MeterController(meter_name), remainder @wsme_pecan.wsexpose([Meter], [base.Query], int, str) def get_all(self, q=None, limit=None, unique=&apos;&apos;): &quot;&quot;&quot;Return all known meters, based on the data recorded so far. :param q: Filter rules for the meters to be returned. :param unique: flag to indicate unique meters to be returned. &quot;&quot;&quot; rbac.enforce(&apos;get_meters&apos;, pecan.request) q = q or [] # Timestamp field is not supported for Meter queries limit = v2_utils.enforce_limit(limit) kwargs = v2_utils.query_to_kwargs( q, pecan.request.storage_conn.get_meters, [&apos;limit&apos;], allow_timestamps=False) return [Meter.from_db_model(m) for m in pecan.request.storage_conn.get_meters( limit=limit, unique=strutils.bool_from_string(unique), **kwargs)] 在这里最重要的就是最后那一行代码：return [Meter.from_db_model(m) for m in pecan.request.storage_conn.get_meters(limit=limit,**kwargs)]，在这行代码中有两个重要的函数调用：pecan.request.storage_conn.get_meters 以及 Meter.from_db_model。所以还没结束，下面还得分析这两个函数是如何执行的。 pecan.request.storage_conn.get_meters的执行过程要分析pecan.request.storage_conn.get_meters(limit=limit, **kwargs)的执行过程我们分两步：一是storage_conn是如何获得的，二是get_meters是如何执行的。 storage_conn指的是数据库的链接，在DBHook.init中可以看出它是通过调用函数get_connection来获得的，而函数get_connection又调用了函数storage.get_connection_from_config。 123456789101112131415161718192021222324class DBHook(hooks.PecanHook): def __init__(self): self.storage_connection = DBHook.get_connection(&apos;metering&apos;) self.event_storage_connection = DBHook.get_connection(&apos;event&apos;) if (not self.storage_connection and not self.event_storage_connection): raise Exception(&quot;Api failed to start. Failed to connect to &quot; &quot;databases, purpose: %s&quot; % &apos;, &apos;.join([&apos;metering&apos;, &apos;event&apos;])) def before(self, state): state.request.storage_conn = self.storage_connection state.request.event_storage_conn = self.event_storage_connection @staticmethod def get_connection(purpose): try: return storage.get_connection_from_config(cfg.CONF, purpose) except Exception as err: params = &#123;&quot;purpose&quot;: purpose, &quot;err&quot;: err&#125; LOG.exception(_LE(&quot;Failed to connect to db, purpose %(purpose)s &quot; &quot;retry later: %(err)s&quot;) % params) 下面再看storage.get_connection_from_config函数是如何执行的，函数storage.get_connection_from_config调用了函数storage.get_connection，而在函数storage.get_connection中重要的是: mgr = driver.DriverManger(namespace, engine_name)，其中的driver为：from stevedore import driver，namespace为：ceilometer.meterings.storage，engine_name为：mongodb。于是stevedore会到setup.cfg中查找相应的设置。 12345678910111213141516171819202122232425262728293031323334def get_connection_from_config(conf, purpose=&apos;metering&apos;): retries = conf.database.max_retries # Convert retry_interval secs to msecs for retry decorator @retrying.retry(wait_fixed=conf.database.retry_interval * 1000, stop_max_attempt_number=retries if retries &gt;= 0 else None) def _inner(): if conf.database_connection: conf.set_override(&apos;connection&apos;, conf.database_connection, group=&apos;database&apos;) namespace = &apos;ceilometer.%s.storage&apos; % purpose url = (getattr(conf.database, &apos;%s_connection&apos; % purpose) or conf.database.connection) return get_connection(url, namespace) return _inner()def get_connection(url, namespace): &quot;&quot;&quot;Return an open connection to the database.&quot;&quot;&quot; connection_scheme = urlparse.urlparse(url).scheme # SqlAlchemy connections specify may specify a &apos;dialect&apos; or # &apos;dialect+driver&apos;. Handle the case where driver is specified. engine_name = connection_scheme.split(&apos;+&apos;)[0] if engine_name == &apos;db2&apos;: import warnings warnings.simplefilter(&quot;always&quot;) import debtcollector debtcollector.deprecate(&quot;The DB2nosql driver is no longer supported&quot;, version=&quot;Liberty&quot;, removal_version=&quot;N*-cycle&quot;) # NOTE: translation not applied bug #1446983 LOG.debug(&apos;looking for %(name)r driver in %(namespace)r&apos;, &#123;&apos;name&apos;: engine_name, &apos;namespace&apos;: namespace&#125;) mgr = driver.DriverManager(namespace, engine_name) return mgr.driver(url) 根据 12from stevedore import driver namespace为：ceilometer.meterings.storage，engine_name为：mongodb 所以会去调用setup.cfg中的 12ceilometer.event.storage = mongodb = ceilometer.event.storage.impl_mongodb:Connection 在ceilometer.storage.imple_mongodb.Connection类的初始化函数中会初始化数据库的连接，没有相应collection的情况下新建相应collection，设置ttl等。 ../ceilometer/storage/impl_mongodb.py: Connection.init123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112class Connection(pymongo_base.Connection): CAPABILITIES = utils.update_nested(pymongo_base.Connection.CAPABILITIES, AVAILABLE_CAPABILITIES) CONNECTION_POOL = pymongo_utils.ConnectionPool() STANDARD_AGGREGATES = dict([(a.name, a) for a in [ pymongo_utils.SUM_AGGREGATION, pymongo_utils.AVG_AGGREGATION, pymongo_utils.MIN_AGGREGATION, pymongo_utils.MAX_AGGREGATION, pymongo_utils.COUNT_AGGREGATION, ]]) AGGREGATES = dict([(a.name, a) for a in [ pymongo_utils.SUM_AGGREGATION, pymongo_utils.AVG_AGGREGATION, pymongo_utils.MIN_AGGREGATION, pymongo_utils.MAX_AGGREGATION, pymongo_utils.COUNT_AGGREGATION, pymongo_utils.STDDEV_AGGREGATION, pymongo_utils.CARDINALITY_AGGREGATION, ]]) SORT_OPERATION_MAPPING = &#123;&apos;desc&apos;: (pymongo.DESCENDING, &apos;$lt&apos;), &apos;asc&apos;: (pymongo.ASCENDING, &apos;$gt&apos;)&#125; MAP_RESOURCES = bson.code.Code(&quot;&quot;&quot; function () &#123; emit(this.resource_id, &#123;user_id: this.user_id, project_id: this.project_id, source: this.source, first_timestamp: this.timestamp, last_timestamp: this.timestamp, metadata: this.resource_metadata&#125;) &#125;&quot;&quot;&quot;) REDUCE_RESOURCES = bson.code.Code(&quot;&quot;&quot; function (key, values) &#123; var merge = &#123;user_id: values[0].user_id, project_id: values[0].project_id, source: values[0].source, first_timestamp: values[0].first_timestamp, last_timestamp: values[0].last_timestamp, metadata: values[0].metadata&#125; values.forEach(function(value) &#123; if (merge.first_timestamp - value.first_timestamp &gt; 0) &#123; merge.first_timestamp = value.first_timestamp; merge.user_id = value.user_id; merge.project_id = value.project_id; merge.source = value.source; &#125; else if (merge.last_timestamp - value.last_timestamp &lt;= 0) &#123; merge.last_timestamp = value.last_timestamp; merge.metadata = value.metadata; &#125; &#125;); return merge; &#125;&quot;&quot;&quot;) _GENESIS = datetime.datetime(year=datetime.MINYEAR, month=1, day=1) _APOCALYPSE = datetime.datetime(year=datetime.MAXYEAR, month=12, day=31, hour=23, minute=59, second=59) def __init__(self, url): # NOTE(jd) Use our own connection pooling on top of the Pymongo one. # We need that otherwise we overflow the MongoDB instance with new # connection since we instantiate a Pymongo client each time someone # requires a new storage connection. self.conn = self.CONNECTION_POOL.connect(url) self.version = self.conn.server_info()[&apos;versionArray&apos;] # Require MongoDB 2.4 to use $setOnInsert if self.version &lt; pymongo_utils.MINIMUM_COMPATIBLE_MONGODB_VERSION: raise storage.StorageBadVersion( &quot;Need at least MongoDB %s&quot; % pymongo_utils.MINIMUM_COMPATIBLE_MONGODB_VERSION) connection_options = pymongo.uri_parser.parse_uri(url) self.db = getattr(self.conn, connection_options[&apos;database&apos;]) if connection_options.get(&apos;username&apos;): self.db.authenticate(connection_options[&apos;username&apos;], connection_options[&apos;password&apos;]) # NOTE(jd) Upgrading is just about creating index, so let&apos;s do this # on connection to be sure at least the TTL is correctly updated if # needed. self.upgrade() @staticmethod def update_ttl(ttl, ttl_index_name, index_field, coll): &quot;&quot;&quot;Update or create time_to_live indexes. :param ttl: time to live in seconds. :param ttl_index_name: name of the index we want to update or create. :param index_field: field with the index that we need to update. :param coll: collection which indexes need to be updated. &quot;&quot;&quot; indexes = coll.index_information() if ttl &lt;= 0: if ttl_index_name in indexes: coll.drop_index(ttl_index_name) return if ttl_index_name in indexes: return coll.database.command( &apos;collMod&apos;, coll.name, index=&#123;&apos;keyPattern&apos;: &#123;index_field: pymongo.ASCENDING&#125;, &apos;expireAfterSeconds&apos;: ttl&#125;) coll.create_index([(index_field, pymongo.ASCENDING)], expireAfterSeconds=ttl, name=ttl_index_name)... get_meters的执行过程这里有一个类的继承关系：object &lt;– storage.base.Connection &lt;– storage.pymongo_base.Connection &lt;– storage.impl_mongodb.Connection，关于storage.impl_mongodb.Connection需要讲的都在上面提到了。 在storage.base.Connection中给出了一些函数定义，但都没有具体的实现。 在storage.pymongo_base.Connection中给出了get_meters的定义，在这里我们就可以真正的看到查询数据库的语句了: self.db.resource.find(q)，下面我们还要解释一下语句models.Meter。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283class Connection(base.Connection): &quot;&quot;&quot;Base Connection class for MongoDB and DB2 drivers.&quot;&quot;&quot; CAPABILITIES = utils.update_nested(base.Connection.CAPABILITIES, COMMON_AVAILABLE_CAPABILITIES) STORAGE_CAPABILITIES = utils.update_nested( base.Connection.STORAGE_CAPABILITIES, AVAILABLE_STORAGE_CAPABILITIES, ) def get_meters(self, user=None, project=None, resource=None, source=None, metaquery=None, limit=None, unique=False): &quot;&quot;&quot;Return an iterable of models.Meter instances :param user: Optional ID for user that owns the resource. :param project: Optional ID for project that owns the resource. :param resource: Optional resource filter. :param source: Optional source filter. :param metaquery: Optional dict with metadata to match on. :param limit: Maximum number of results to return. :param unique: If set to true, return only unique meter information. &quot;&quot;&quot; if limit == 0: return metaquery = pymongo_utils.improve_keys(metaquery, metaquery=True) or &#123;&#125; q = &#123;&#125; if user == &apos;None&apos;: q[&apos;user_id&apos;] = None elif user is not None: q[&apos;user_id&apos;] = user if project == &apos;None&apos;: q[&apos;project_id&apos;] = None elif project is not None: q[&apos;project_id&apos;] = project if resource == &apos;None&apos;: q[&apos;_id&apos;] = None elif resource is not None: q[&apos;_id&apos;] = resource if source is not None: q[&apos;source&apos;] = source q.update(metaquery) count = 0 if unique: meter_names = set() for r in self.db.resource.find(q): for r_meter in r[&apos;meter&apos;]: if unique: if r_meter[&apos;counter_name&apos;] in meter_names: continue else: meter_names.add(r_meter[&apos;counter_name&apos;]) if limit and count &gt;= limit: return else: count += 1 if unique: yield models.Meter( name=r_meter[&apos;counter_name&apos;], type=r_meter[&apos;counter_type&apos;], # Return empty string if &apos;counter_unit&apos; is not valid # for backward compatibility. unit=r_meter.get(&apos;counter_unit&apos;, &apos;&apos;), resource_id=None, project_id=None, source=None, user_id=None) else: yield models.Meter( name=r_meter[&apos;counter_name&apos;], type=r_meter[&apos;counter_type&apos;], # Return empty string if &apos;counter_unit&apos; is not valid # for backward compatibility. unit=r_meter.get(&apos;counter_unit&apos;, &apos;&apos;), resource_id=r[&apos;_id&apos;], project_id=r[&apos;project_id&apos;], source=r[&apos;source&apos;], user_id=r[&apos;user_id&apos;]) 这里又有一个类的继承关系：object &lt;– storage.base.Model &lt;– storage.models.Meter。 这里其实就是把查询到的值和键做个对应的设置。 1234567891011121314151617181920212223242526class Model(object): &quot;&quot;&quot;Base class for storage API models.&quot;&quot;&quot; def __init__(self, **kwds): self.fields = list(kwds) for k, v in six.iteritems(kwds): setattr(self, k, v) def as_dict(self): d = &#123;&#125; for f in self.fields: v = getattr(self, f) if isinstance(v, Model): v = v.as_dict() elif isinstance(v, list) and v and isinstance(v[0], Model): v = [sub.as_dict() for sub in v] d[f] = v return d def __eq__(self, other): return self.as_dict() == other.as_dict() @classmethod def get_field_names(cls): fields = inspect.getargspec(cls.__init__)[0] return set(fields) - set([&quot;self&quot;]) Meter.from_db_model(m)的执行过程这里有个类的继承关系：wsme.types.Base &lt;– wsme.types.DynamicBase &lt;– api.controllers.v2.base.Base &lt;– api.v2.meters.Meter 后面api.v2.meters.Meter的初始化函数调用了wsme.types.Base.init函数。 123456789101112131415161718192021222324252627282930class Base(six.with_metaclass(BaseMeta)): &quot;&quot;&quot;Base type for complex types&quot;&quot;&quot; def __init__(self, **kw): for key, value in kw.items(): if hasattr(self, key): setattr(self, key, value) class DynamicBase(Base): &quot;&quot;&quot;Base type for complex types for which all attributes are not defined when the class is constructed. This class is meant to be used as a base for types that have properties added after the main class is created, such as by loading plugins. &quot;&quot;&quot; @classmethod def add_attributes(cls, **attrs): &quot;&quot;&quot;Add more attributes The arguments should be valid Python attribute names associated with a type for the new attribute. &quot;&quot;&quot; for n, t in attrs.items(): setattr(cls, n, t) cls.__registry__.reregister(cls) 函数Meter.from_db_model定义在ceilometer.api.controllers.v2.base:Base中，函数Meter.from_db_model是把返回的ceilometer.storage.models.Meter实例， 进一步做一些简单的处理。 123456789101112131415161718192021class Base(wtypes.DynamicBase): @classmethod def from_db_model(cls, m): return cls(**(m.as_dict())) @classmethod def from_db_and_links(cls, m, links): return cls(links=links, **(m.as_dict())) def as_dict(self, db_model): valid_keys = inspect.getargspec(db_model.__init__)[0] if &apos;self&apos; in valid_keys: valid_keys.remove(&apos;self&apos;) return self.as_dict_from_keys(valid_keys) def as_dict_from_keys(self, keys): return dict((k, getattr(self, k)) for k in keys if hasattr(self, k) and getattr(self, k) != wsme.Unset) Meter是在ceilometer.api.v2.meters中定义的，Meter中定义它的的类属性:name, type, unit, resource_id, project_id, user_id, source, meter_id；Meter中还定义了初始化函数init，该初始化函数主要是构造meter_id和调用wsme.types.Base的初始化函数init。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Meter(base.Base): &quot;&quot;&quot;One category of measurements.&quot;&quot;&quot; name = wtypes.text &quot;The unique name for the meter&quot; type = wtypes.Enum(str, *sample.TYPES) &quot;The meter type (see :ref:`measurements`)&quot; unit = wtypes.text &quot;The unit of measure&quot; resource_id = wtypes.text &quot;The ID of the :class:`Resource` for which the measurements are taken&quot; project_id = wtypes.text &quot;The ID of the project or tenant that owns the resource&quot; user_id = wtypes.text &quot;The ID of the user who last triggered an update to the resource&quot; source = wtypes.text &quot;The ID of the source that identifies where the meter comes from&quot; meter_id = wtypes.text &quot;The unique identifier for the meter&quot; def __init__(self, **kwargs): meter_id = &apos;%s+%s&apos; % (kwargs[&apos;resource_id&apos;], kwargs[&apos;name&apos;]) # meter_id is of type Unicode but base64.encodestring() only accepts # strings. See bug #1333177 meter_id = base64.b64encode(meter_id.encode(&apos;utf-8&apos;)) kwargs[&apos;meter_id&apos;] = meter_id super(Meter, self).__init__(**kwargs) @classmethod def sample(cls): return cls(name=&apos;instance&apos;, type=&apos;gauge&apos;, unit=&apos;instance&apos;, resource_id=&apos;bd9431c1-8d69-4ad3-803a-8d4a6b89fd36&apos;, project_id=&apos;35b17138-b364-4e6a-a131-8f3099c5be68&apos;, user_id=&apos;efd87807-12d2-4b38-9c70-5f5c2ac427ff&apos;, source=&apos;openstack&apos;, ) 完毕。本文只是简单梳理了一些的代码流程，没有对于细节问题深究。","tags":[{"name":"Openstack","slug":"Openstack","permalink":"http://yoursite.com/tags/Openstack/"}]}]