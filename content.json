[{"title":"Openstack源码基础之wsgi","date":"2017-05-09T15:37:00.000Z","path":"2017/05/09/openstack源码基础之wsgi/","text":"一、wsgi基础1.1 http基础对于 web 应用程序来说，最基本的概念就是客户端发送请求（request），收到服务器端的响应（response）。下面是简单的 HTTP 请求： 12345GET /Index.html HTTP/1.1\\r\\nConnection: Keep-Alive\\r\\nAccept: */*\\r\\nUser-Agent: Sample Application\\r\\nHost: www.microsoft.com\\r\\n\\r\\n 内容包括了 method、 url、 protocol version 以及头部的信息。而 HTTP 响应（不包括数据）可能是如下的内容： 12345678HTTP/1.1 200 OKServer: Microsoft-IIS/5.0\\r\\nContent-Location: http://www.microsoft.com/default.htm\\r\\nDate: Tue, 25 Jun 2002 19:33:18 GMT\\r\\nContent-Type: text/html\\r\\nAccept-Ranges: bytes\\r\\nLast-Modified: Mon, 24 Jun 2002 20:27:23 GMT\\r\\nContent-Length: 26812\\r\\n 实际生产中，python 程序是放在服务器的 http server（比如 apache， nginx 等）上的。现在的问题是 服务器程序怎么把接受到的请求传递给 python 呢，怎么在网络的数据流和 python 的结构体之间转换呢？这就是 wsgi 做的事情：一套关于程序端和服务器端的规范，或者说统一的接口。 大概的流程是 client —&gt; 服务器端程序 —&gt; wsgi —&gt; python程序 123Python Web 开发中，服务端程序可以分为两个部分，一是服务器程序，二是应用程序。前者负责把客户端请求接收，整理，后者负责具体的逻辑处理。为了方便应用程序的开发，我们把常用的功能封装起来，成为各种Web开发框架，例如 Django, Flask, Tornado。不同的框架有不同的开发方式，但是无论如何，开发出的应用程序都要和服务器程序配合，才能为用户提供服务。这样，服务器程序就需要为不同的框架提供不同的支持。这样混乱的局面无论对于服务器还是框架，都是不好的。对服务器来说，需要支持各种不同框架，对框架来说，只有支持它的服务器才能被开发出的应用使用。这时候，标准化就变得尤为重要。我们可以设立一个标准，只要服务器程序支持这个标准，框架也支持这个标准，那么他们就可以配合使用。一旦标准确定，双方各自实现。这样，服务器可以支持更多支持标准的框架，框架也可以使用更多支持标准的服务器。Python Web开发中，这个标准就是 The Web Server Gateway Interface, 即 WSGI. 这个标准在PEP 333中描述，后来，为了支持 Python 3.x, 并且修正一些问题，新的版本在PEP 3333中描述。 1.2 wsgi是什么123WSGI 是服务器程序与应用程序的一个约定，它规定了双方各自需要实现什么接口，提供什么功能，以便二者能够配合使用。WSGI 不能规定的太复杂，否则对已有的服务器来说，实现起来会困难，不利于WSGI的普及。同时WSGI也不能规定的太多，例如cookie处理就没有在WSGI中规定，这是为了给框架最大的灵活性。要知道WSGI最终的目的是为了方便服务器与应用程序配合使用，而不是成为一个Web框架的标准。另一方面，WSGI需要使用middleware（是中间件么？）易于实现。middleware处于服务器程序与应用程序之间，对服务器程序来说，它相当于应用程序，对应用程序来说，它相当于服务器程序。这样，对用户请求的处理，可以变成多个 middleware 叠加在一起，每个middleware实现不同的功能。请求从服务器来的时候，依次通过middleware，响应从应用程序返回的时候，反向通过层层middleware。我们可以方便地添加，替换middleware，以便对用户请求作出不同的处理。 1.3 wsgi分类1.3.1 应用程序1) 应用程序需要是一个可调用的对象1234在Python中可以是函数可以是一个实例，它的类实现了__call__方法，对一个实例像调用函数一样调用这个实例可以是一个类，这时候，用这个类生成实例的过程就相当于调用这个类 2) 可调用对象接收两个参数3) 可调用对象要返回一个值，这个值是可迭代的。这样的话，application看起来像下面代码中的实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&quot;&quot;&quot;WSGI 规定每个 python 程序（Application）必须是一个可调用的对象，接受两个参数 environ（WSGI 的环境信息） 和 start_response（开始响应请求的函数，是一个回调函数），并且返回 iterable。几点说明：1) environ 和 start_response 由 http server 提供并实现2) environ 变量是包含了环境信息的字典3) Application 内部在返回前调用 start_response4) start_response也是一个 callable，接受两个必须的参数，status（HTTP状态）和 response_headers（响应消息的头）可调用对象要返回一个值，这个值是可迭代的。&quot;&quot;&quot;# 可调用对象是一个方法def application(environ,start_response): response_body = &apos;The request method was %s&apos; % environ[&apos;REQUEST_METHOD&apos;] status = &apos;200 OK&apos; # 应答的头部是一个列表，每对键值都必须是一个 tuple。 response_header = [(&apos;Content-Type&apos;,&apos;text/plain&apos;),(&apos;Content-Length&apos;,str(len(response_body)))] # 调用服务器程序提供的 start_response，填入两个参数 start_response(status,response_header) # 返回必须是 iterable return [response_body]# 可调用对象是一个类&quot;&quot;&quot;这里的可调用对象就是 AppClass 这个类，调用它就能生成可以迭代的结果。使用方法类似于：for result in AppClass(env, start_response): do_somthing(result)&quot;&quot;&quot;class MyAPP(object): def __init__(self,environ,start_response): self.environ = environ self.start_response = start_response def __iter__(self): response_body = &apos;The request method was %s&apos; % self.environ[&apos;REQUEST_METHOD&apos;] status = &apos;200 OK&apos; response_header = [(&apos;Content-Type&apos;,&apos;text/plain&apos;),(&apos;Content-Length&apos;,str(len(response_body)))] self.start_response(status,response_header) yield response_body# 可调用对象是一个实例&quot;&quot;&quot;这里的可调用对象就是 AppClass 的实例，使用方法类似于：app = AppClass()for result in app(environ, start_response): do_somthing(result)&quot;&quot;&quot;class MyAPP(object): def __init__(self,environ,start_response): self.environ = environ self.start_response = start_response def __call__(self): response_body = &apos;The request method was %s&apos; % self.environ[&apos;REQUEST_METHOD&apos;] status = &apos;200 OK&apos; response_header = [(&apos;Content-Type&apos;,&apos;text/plain&apos;),(&apos;Content-Length&apos;,str(len(response_body)))] self.start_response(status,response_header) yield response_body 1.3.2 服务器端程序服务器程序会在每次客户端的请求传来时，调用我们写好的应用程序，并将处理好的结果返回给客户端。 123456789101112131415# 此函数接受application作为参数，application就是上面定义的三种形式中的一种def run(application): environ = &#123;&#125; # 实现start_response函数，供application调用 def start_response(status, response_headers, exc_info=None): pass # application返回的结果是一个可调用的对象，这里是获取application的返回值赋值给result result = application(environ, start_response) # 定义一个写数据的函数 def write(data): pass # 循环输出application返回的结果，并调用写数据函数写入进去 for data in result: write(data) 从上面的代码可以看出服务器程序是与application程序配合完成工作的。 1.4 middleware中间件123另外，有些功能可能介于服务器程序和应用程序之间，例如，服务器拿到了客户端请求的URL, 不同的URL需要交由不同的函数处理，这个功能叫做 URL Routing，这个功能就可以放在二者中间实现，这个中间层就是 middleware。middleware对服务器程序和应用程序是透明的，也就是说，服务器程序以为它就是应用程序，而应用程序以为它就是服务器程序。这就告诉我们，middleware需要把自己伪装成一个服务器，接受应用程序，调用它，同时middleware还需要把自己伪装成一个应用程序，传给服务器程序。其实无论是服务器程序，middleware 还是应用程序，都在服务端，为客户端提供服务，之所以把他们抽象成不同层，就是为了控制复杂度，使得每一次都不太复杂，各司其职。 下面，我们看看middleware大概是什么样子的。 12345678910111213# URL Routing middleware# 这里接受一个url_app_mapping的作用是通过url路由到相应的app，然后调用app(environ,start_response)# 返回相应app的结果def urlrouting(url_app_mapping): def midware_app(environ, start_response): url = environ[&apos;PATH_INFO&apos;] app = url_app_mapping[url] result = app(environ, start_response) return result return midware_app 函数 midware_app就是一个简单的middleware：对服务器而言，它是一个应用程序，是一个可调用对象， 有两个参数，返回一个可调用对象。对应用程序而言，它是一个服务器，为应用程序提供了参数，并且调用了应用程序。另外，这里的urlrouting函数，相当于一个函数生成器，你给它不同的 url-app 映射关系，它会生成相应的具有 url routing功能的 middleware。 二、wsgi详解2.1 wsgi serverwsgi server 基本工作流程 服务器创建socket，监听端口，等待客户端连接。 当有请求来时，服务器解析客户端信息放到环境变量environ中，并调用绑定的handler来处理请求。 handler解析这个http请求，将请求信息例如method，path等放到environ中。 wsgi handler再将一些服务器端信息也放到environ中，最后服务器信息，客户端信息，本次请求信息全部都保存到了环境变量environ中。 wsgi handler 调用注册的wsgi app，并将environ和回调函数传给wsgi app wsgi app 将reponse header/status/body 回传给wsgi handler 最终handler还是通过socket将response信息塞回给客户端。 2.2 wsgi application12WSGI Applicationwsgi application就是一个普通的callable对象，当有请求到来时，wsgi server会调用这个wsgi app。这个对象接收两个参数，通常为environ,start_response。environ就像前面介绍的，可以理解为环境变量，跟一次请求相关的所有信息都保存在了这个环境变量中，包括服务器信息，客户端信息，请求信息。start_response是一个callback函数，wsgi application通过调用start_response，将response headers/status 返回给wsgi server。此外这个wsgi app会return 一个iterator对象 ，这个iterator就是response body。这么空讲感觉很虚，对着下面这个简单的例子看就明白很多了。 下面是一个例子 12345def simple_app(environ, start_response): status = &apos;200 OK&apos; response_headers = [(&apos;Content-type&apos;, &apos;text/plain&apos;)] start_response(status, response_headers) return [u&quot;This is hello wsgi app&quot;.encode(&apos;utf8&apos;)] 我们再用wsgiref 作为wsgi server ，然后调用这个wsgi app，就能直观看到一次request,response的效果，简单修改代码如下 1234567891011from wsgiref.simple_server import make_serverdef simple_app(environ, start_response): status = &apos;200 OK&apos; response_headers = [(&apos;Content-type&apos;, &apos;text/plain&apos;)] start_response(status, response_headers) return [u&quot;This is hello wsgi app&quot;.encode(&apos;utf8&apos;)]httpd = make_server(&apos;&apos;, 8000, simple_app)print &quot;Serving on port 8000...&quot;httpd.serve_forever() 访问http://127.0.0.1:8000 就能看到效果了。 此外，上面讲到了wsgi app只要是一个callable对象就可以了，因此不一定要是函数，一个实现了call方法的实例也可以，示例代码如下 1234567891011121314from wsgiref.simple_server import make_serverclass AppClass: def __call__(self,environ, start_response): status = &apos;200 OK&apos; response_headers = [(&apos;Content-type&apos;, &apos;text/plain&apos;)] start_response(status, response_headers) return [&quot;hello world!&quot;]app = AppClass()httpd = make_server(&apos;&apos;, 8000, app)print &quot;Serving on port 8000...&quot;httpd.serve_forever() 2.3 WSGI MiddleWare12345678910111213141516171819202122232425262728293031323334353637383940from wsgiref.simple_server import make_serverURL_PATTERNS= ( (&apos;hi/&apos;,&apos;say_hi&apos;), (&apos;hello/&apos;,&apos;say_hello&apos;), )class Dispatcher(object): def _match(self,path): path = path.split(&apos;/&apos;)[1] for url,app in URL_PATTERNS: if path in url: return app def __call__(self,environ, start_response): # PATH_INFO URL 路径除了起始部分后的剩余部分，用于找到相应的应用程序对象，如果请求的路径就是根路径，这个值为空字符串 path = environ.get(&apos;PATH_INFO&apos;,&apos;/&apos;) app = self._match(path) if app : app = globals()[app] return app(environ, start_response) else: start_response(&quot;404 NOT FOUND&quot;,[(&apos;Content-type&apos;, &apos;text/plain&apos;)]) return [&quot;Page dose not exists!&quot;]def say_hi(environ, start_response): start_response(&quot;200 OK&quot;,[(&apos;Content-type&apos;, &apos;text/html&apos;)]) return [&quot;kenshin say hi to you!&quot;]def say_hello(environ, start_response): start_response(&quot;200 OK&quot;,[(&apos;Content-type&apos;, &apos;text/html&apos;)]) return [&quot;kenshin say hello to you!&quot;]app = Dispatcher()httpd = make_server(&apos;&apos;, 8000, app)print &quot;Serving on port 8000...&quot;httpd.serve_forever() 上面的例子可以看出来，middleware 包装之后，一个简单wsgi app就有了URL dispatch功能。然后我还可以在这个app外面再加上其它的middleware来包装它，例如加一个权限认证的middleware 1234567891011121314class Auth(object): def __init__(self,app): self.app = app def __call__(self,environ, start_response): #TODO return self.app(environ, start_response)app = Dispatcher()auth_app = Auth(app)httpd = make_server(&apos;&apos;, 8000, auth_app)print &quot;Serving on port 8000...&quot;httpd.serve_forever() 经过这些middleware的包装，已经有点框架的感觉了。其实基于wsgi的框架，例如paste,pylons就是这样通过一层层middleware组合起来的。只是一个成熟的框架，这样的middleware会有很多，例如 123456def configure(app): return ErrorHandlerMiddleware( SessionMiddleware( IdentificationMiddleware( AuthenticationMiddleware( UrlParserMiddleware(app)))))) 只要这些Middleware符合wsgi规范，甚至还可以在各个框架之间组合重用。例如pylons的认证Middleware可以直接被TurboGears拿去使用。 好了，各个部分都写完了，以后有时间再看看pylons的代码，相信又会对wsgi有很多新的理解。","tags":[{"name":"Openstack","slug":"Openstack","permalink":"http://yoursite.com/tags/Openstack/"}]},{"title":"python回调函数","date":"2017-05-09T12:33:00.000Z","path":"2017/05/09/python回调函数理解/","text":"回调函数就是一个通过函数指针调用的函数。如果你把函数的指针（地址）作为参数传递给另一个函数，当这个指针被用来调用其所指向的函数时，我们就说这是回调函数。 1234567891011121314151617181920# coding:utf-8# 定义回调函数def callback(): print &quot;in callback&quot;# 定义调用回调函数的函数def test_call(p_call): print &quot;in called.py test_call()&quot; p_call()# 定义执行回调函数def main(): #called.test() test_call(callback) print &quot;in call.py&quot;main() 1234567891011121314151617181920212223242526272829303132333435363738# coding:utf-8class Callback: def __init__(self, instance, function_name): self.instance = instance self.function_name = function_name def action(self, params): self.instance.__getattribute__(self.function_name)(params)class Test: def __init__(self): self.clb = None def register(self, clb): self.clb = clb def do_test(self): params = [] self.clb.action(params)class Api(object): def __init__(self, test_instance): test_instance.register(Callback(self, self.function.__name__)) def function(self, params): print(&apos;function&apos;)t = Test()a = Api(t)t.do_test()&quot;&quot;&quot;Callback类负责封装回调函数的所用者和具体的回调函数（的名字）Test可以向本例子中一样，被同步调用，也可以像网络程序一样被网络事件异步调用而Api其实就是我们的主逻辑模块了&quot;&quot;&quot;","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Neutron ovs+vxlan手工实现SDN","date":"2017-04-26T15:37:00.000Z","path":"2017/04/26/neutron+vxlan手工实现SDN/","text":"openstack运维开发QQ群:94776000 主机网卡配置123456controller: ens160:192.168.11.101/24((management network/public/external network)) ens192:10.0.0.1/24(private network，vxlan tunning)compute01: ens160:192.168.11.102/24((management network/public/external network)) ens192:10.0.0.2/24(private network，vxlan tunning) controller安装配置模拟Network 节点相关实现，比如L3、dhcp-agent实现，为了模拟多节点网络情况，这里Network同时也模拟一个计算节点，模拟M2 openvswitch 实现，上面运行instance1。 安装需要用到的包1yum install libvirt openvswitch python-virtinst xauth tigervnc qemu-* -y 移除默认的libvirt 网络，方便清晰分析网络情况123virsh net-destroy defaultvirsh net-autostart --disable defaultvirsh net-undefine default 设置允许ipforwarding1234echo &quot;net.ipv4.ip_forward=1&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.ipv4.conf.all.rp_filter=0&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.ipv4.conf.default.rp_filter=0&quot; &gt;&gt; /etc/sysctl.confsysctl -p 启动openvswitch12systemctl start openvswitchsystemctl enable openvswitch 创建一个linux bridge12brctl addbr qbr01ip link set qbr01 up 创建一个instance，并连接到qbr01 Linux Bridge 配置文件如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849vim instance1.xml&lt;domain type=&quot;qemu&quot;&gt; &lt;uuid&gt;23469de0-a3a0-4214-a60e-a45322bcc370&lt;/uuid&gt; &lt;name&gt;instance1&lt;/name&gt; &lt;memory&gt;524288&lt;/memory&gt; &lt;vcpu&gt;1&lt;/vcpu&gt; &lt;sysinfo type=&quot;smbios&quot;&gt; &lt;system&gt; &lt;entry name=&quot;manufacturer&quot;&gt;Red Hat Inc.&lt;/entry&gt; &lt;entry name=&quot;product&quot;&gt;OpenStack Nova&lt;/entry&gt; &lt;entry name=&quot;version&quot;&gt;2014.1.1-3.el6&lt;/entry&gt; &lt;entry name=&quot;serial&quot;&gt;b8d4ec5f-acd6-7111-c69b-600912a079bb&lt;/entry&gt; &lt;entry name=&quot;uuid&quot;&gt;23469de0-a3a0-4214-a60e-a45322bcc370&lt;/entry&gt; &lt;/system&gt; &lt;/sysinfo&gt; &lt;os&gt; &lt;type&gt;hvm&lt;/type&gt; &lt;boot dev=&quot;hd&quot;/&gt; &lt;smbios mode=&quot;sysinfo&quot;/&gt; &lt;/os&gt; &lt;features&gt; &lt;acpi/&gt; &lt;apic/&gt; &lt;/features&gt; &lt;clock offset=&quot;utc&quot;/&gt; &lt;cpu mode=&quot;host-model&quot; match=&quot;exact&quot;/&gt; &lt;devices&gt; &lt;disk type=&quot;file&quot; device=&quot;disk&quot;&gt; &lt;driver name=&quot;qemu&quot; type=&quot;qcow2&quot; cache=&quot;none&quot;/&gt; &lt;source file=&quot;/home/sdn/instance1.img&quot;/&gt; &lt;target bus=&quot;virtio&quot; dev=&quot;vda&quot;/&gt; &lt;/disk&gt; &lt;source bridge=&apos;qbr01&apos;/&gt; &lt;target dev=&apos;tap01&apos;/&gt; &lt;model type=&apos;virtio&apos;/&gt; &lt;driver name=&apos;qemu&apos;/&gt; &lt;address type=&apos;pci&apos; domain=&apos;0x0000&apos; bus=&apos;0x00&apos; slot=&apos;0x03&apos; function=&apos;0x0&apos;/&gt; &lt;/interface&gt; &lt;serial type=&quot;file&quot;&gt; &lt;source path=&quot;/home/sdn/instance1.log&quot;/&gt; &lt;/serial&gt; &lt;serial type=&quot;pty&quot;/&gt; &lt;input type=&quot;tablet&quot; bus=&quot;usb&quot;/&gt; &lt;graphics type=&quot;vnc&quot; autoport=&quot;yes&quot; keymap=&quot;en-us&quot; listen=&quot;0.0.0.0&quot;/&gt; &lt;video&gt; &lt;model type=&quot;cirrus&quot;/&gt; &lt;/video&gt; &lt;/devices&gt;&lt;/domain&gt; 启动虚拟机12345mv cirros-0.3.4-x86_64-disk.img instance1.imgvirsh define instance1.xmlvirsh start instance1virsh vncdisplay instance1vncviewer :0 启动console 以后,登录添加ip 地址 172.16.10.1112sudo ip addr add 172.16.10.11/24 dev eth0sudo route add default gw 172.16.10.1 创建一个内部bridge br-int， 模拟 OpenStack integrated bridge123456789101112131415ovs-vsctl add-br br-int# gre隧道# ovs-vsctl add-port br-int gre0 -- set interface gre0 type=gre options:remote_ip=192.168.4.202# vxlan隧道ovs-vsctl add-port br-int vxlan0 -- set Interface vxlan0 type=vxlan options:remote_ip=10.0.0.2 # 创建一个veth peer，连接Linux Bridge &apos;qbr01&apos; 和 OpenvSwich Bridge &apos;br-ini&apos;# 创建一个网卡对ip link add qvo01 type veth peer name qvb01brctl addif qbr01 qvb01ovs-vsctl add-port br-int qvo01ovs-vsctl set port qvo01 tag=100ip link set qvb01 upip link set qvo01 up 模拟安装计算节点(compute01)1yum install libvirt openvswitch python-virtinst xauth tigervnc qemu-* 移除libvirt 默认的网络 123virsh net-destroy defaultvirsh net-autostart --disable defaultvirsh net-undefine default 设置允许ipforwarding1234echo &quot;net.ipv4.ip_forward=1&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.ipv4.conf.all.rp_filter=0&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.ipv4.conf.default.rp_filter=0&quot; &gt;&gt; /etc/sysctl.confsysctl -p 启动openvswitch12systemctl start openvswitchsystemctl enable openvswitch 创建一个linux bridge12brctl addbr qbr02ip link set qbr02 up 创建一个vm，并连接到qbr02 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950cat instance2.xml &lt;domain type=&quot;qemu&quot;&gt; &lt;uuid&gt;23469de0-a3a0-4214-a60e-a45322bcc370&lt;/uuid&gt; &lt;name&gt;instance2&lt;/name&gt; &lt;memory&gt;524288&lt;/memory&gt; &lt;vcpu&gt;1&lt;/vcpu&gt; &lt;sysinfo type=&quot;smbios&quot;&gt; &lt;system&gt; &lt;entry name=&quot;manufacturer&quot;&gt;Red Hat Inc.&lt;/entry&gt; &lt;entry name=&quot;product&quot;&gt;OpenStack Nova&lt;/entry&gt; &lt;entry name=&quot;version&quot;&gt;2014.1.1-3.el6&lt;/entry&gt; &lt;entry name=&quot;serial&quot;&gt;b8d4ec5f-acd6-7111-c69b-600912a079bb&lt;/entry&gt; &lt;entry name=&quot;uuid&quot;&gt;23469de0-a3a0-4214-a60e-a45322bcc370&lt;/entry&gt; &lt;/system&gt; &lt;/sysinfo&gt; &lt;os&gt; &lt;type&gt;hvm&lt;/type&gt; &lt;boot dev=&quot;hd&quot;/&gt; &lt;smbios mode=&quot;sysinfo&quot;/&gt; &lt;/os&gt; &lt;features&gt; &lt;acpi/&gt; &lt;apic/&gt; &lt;/features&gt; &lt;clock offset=&quot;utc&quot;/&gt; &lt;cpu mode=&quot;host-model&quot; match=&quot;exact&quot;/&gt; &lt;devices&gt; &lt;disk type=&quot;file&quot; device=&quot;disk&quot;&gt; &lt;driver name=&quot;qemu&quot; type=&quot;qcow2&quot; cache=&quot;none&quot;/&gt; &lt;source file=&quot;/home/sdn/instance2.img&quot;/&gt; &lt;target bus=&quot;virtio&quot; dev=&quot;vda&quot;/&gt; &lt;/disk&gt; &lt;interface type=&apos;bridge&apos;&gt; &lt;source bridge=&apos;qbr02&apos;/&gt; &lt;target dev=&apos;tap02&apos;/&gt; &lt;model type=&apos;virtio&apos;/&gt; &lt;driver name=&apos;qemu&apos;/&gt; &lt;address type=&apos;pci&apos; domain=&apos;0x0000&apos; bus=&apos;0x00&apos; slot=&apos;0x03&apos; function=&apos;0x0&apos;/&gt; &lt;/interface&gt; &lt;serial type=&quot;file&quot;&gt; &lt;source path=&quot;/home/sdn/instance2.log&quot;/&gt; &lt;/serial&gt; &lt;serial type=&quot;pty&quot;/&gt; &lt;input type=&quot;tablet&quot; bus=&quot;usb&quot;/&gt; &lt;graphics type=&quot;vnc&quot; autoport=&quot;yes&quot; keymap=&quot;en-us&quot; listen=&quot;0.0.0.0&quot;/&gt; &lt;video&gt; &lt;model type=&quot;cirrus&quot;/&gt; &lt;/video&gt; &lt;/devices&gt;&lt;/domain&gt; 1234virsh define instance2.xmlvirsh start instance2virsh vncdisplay instance2vncviewer :0 启动console 以后,登录添加ip 地址 172.16.10.1212sudo ip addr add 172.16.10.12/24 dev eth0sudo route add default gw 172.16.10.1 创建一个内部bridge br-int， 模拟 OpenStack integrated bridge123ovs-vsctl add-br br-int# ovs-vsctl add-port br-int gre0 -- set interface gre0 type=gre options:remote_ip=192.168.4.201ovs-vsctl add-port br-int vxlan0 -- set Interface vxlan0 type=vxlan options:remote_ip=10.0.0.1 创建一个veth peer，连接Linux Bridge ‘qbr02’ 和 OpenvSwich Bridge ‘br-ini’123456ip link add qvo02 type veth peer name qvb02brctl addif qbr02 qvb02ovs-vsctl add-port br-int qvo02ovs-vsctl set port qvo02 tag=100ip link set qvb02 upip link set qvo02 up 检查是否能连通instance1，在instance2的控制台12# 结果是能ping通的ping 172.16.10.11 ##通过 Network Namespace 实现租户私有网络互访 (在控制节点)添加一个namespace，dhcp01用于隔离租户网络。 1ip netns add dhcp01 为私有网络172.16.10.0/24 ，在命名空间dhcp01 中 创建dhcp 服务1234567ovs-vsctl add-port br-int tapdhcp01 -- set interface tapdhcp01 type=internalovs-vsctl set port tapdhcp01 tag=100# 把br-int 上的tapdhcp01上的port连接到dhcp01 命名空间中ip link set tapdhcp01 netns dhcp01# 为dhcp01命名空间上的tapdhcp01端口分配一个 172.16.10.2/24的ip地址 ip netns exec dhcp01 ip addr add 172.16.10.2/24 dev tapdhcp01ip netns exec dhcp01 ip link set tapdhcp01 up 检查网络是否连通，在namespace 访问instance1 和 instance212ip netns exec dhcp01 ping 172.16.10.12ip netns exec dhcp01 ping 172.16.10.11 ##通过 Network Namespace 和Iptables 实现L3 router1ovs-vsctl add-br br-ex 重新配置 ens160 和 br-ex12345678910111213141516171819vim /etc/sysconfig/network-scripts/ifcfg-ens160DEVICE=ens160TYPE=OVSPortDEVICETYPE=ovsOVS_BRIDGE=br-exONBOOT=yesvi /etc/sysconfig/network-scripts/ifcfg-br-exDEVICE=br-exONBOOT=yesDEVICETYPE=ovsTYPE=OVSBridgeBOOTPROTO=staticIPADDR=192.168.11.101NETMASK=255.255.255.0GATEWAY=192.168.11.1DNS1=218.2.2.2 重启启动网络服务1ovs-vsctl add-port br-ex ens160 &amp;&amp; systemctl restart network 检查网络，配置后是否连通1ping 192.168.11.1 添加一个namespace，router01 用于路由和floating ip 分配1ip netns add router01 在br-int添加一个接口，作为私有网络172.16.10.0/24的网关123456ovs-vsctl add-port br-int qr01 -- set interface qr01 type=internalovs-vsctl set port qr01 tag=100ip link set qr01 netns router01ip netns exec router01 ip addr add 172.16.10.1/24 dev qr01ip netns exec router01 ip link set qr01 upip netns exec router01 ip link set lo up 在br-ex中添加一个接口，用于私网172.16.10.0/24设置下一跳地址 12345ovs-vsctl add-port br-ex qg01 -- set interface qg01 type=internalip link set qg01 netns router01ip netns exec router01 ip addr add 192.168.11.200/24 dev qg01 ip netns exec router01 ip link set qg01 upip netns exec router01 ip link set lo up 模拟分配floating ip 访问instance1为instance1 172.16.10.11 分配floating ip，192.168.11.20112345ip netns exec router01 ip addr add 192.168.11.201/32 dev qg01 ip netns exec router01 iptables -t nat -A OUTPUT -d 192.168.11.201/32 -j DNAT --to-destination 172.16.10.11ip netns exec router01 iptables -t nat -A PREROUTING -d 192.168.11.201/32 -j DNAT --to-destination 172.16.10.11ip netns exec router01 iptables -t nat -A POSTROUTING -s 172.16.10.11/32 -j SNAT --to-source 192.168.11.201ip netns exec router01 iptables -t nat -A POSTROUTING -s 172.16.10.0/24 -j SNAT --to-source 192.168.11.200 #################测试floating ip1ping 192.168.11.201 如果需要清除nat chain1iptables -t nat -F 123456789101112&lt;!-- ip netns exec router01 iptables -t nat -A OUTPUT -d 192.168.2.102/32 -j DNAT --to-destination 192.168.10.11ip netns exec router01 iptables -t nat -A PREROUTING -d 192.168.2.102/32 -j DNAT --to-destination 192.168.10.11ip netns exec router01 iptables -t nat -A POSTROUTING -s 192.168.10.11/32 -j SNAT --to-source 192.168.2.102ip netns exec router01 ip addr add 192.168.2.103/32 dev qg01ip netns exec router01 iptables -t nat -A OUTPUT -d 192.168.2.103/32 -j DNAT --to-destination 192.168.10.11ip netns exec router01 iptables -t nat -A PREROUTING -d 192.168.2.103/32 -j DNAT --to-destination 192.168.10.11ip netns exec router01 iptables -t nat -A POSTROUTING -s 192.168.10.11/32 -j SNAT --to-source 192.168.2.103ip netns exec router01 route add default gw 192.168.2.1ip netns exec router01 route -n --&gt;","tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"python装饰器","date":"2017-04-26T15:37:00.000Z","path":"2017/04/26/python装饰器/","text":"理解装饰器： 函数 例如fun 函数的执行fun() 1、终极例子：123456789101112131415161718192021222324252627282930313233343536# coding:utf-8def Before(request, kargs): print &apos;before&apos;def After(request, kargs): print &apos;after&apos;def Filter(before_func, after_func): def outer(main_func): def wrapper(request, kargs): before_result = before_func(request, kargs) if (before_result != None): return before_result main_result = main_func(request, kargs) if (main_result != None): return main_result after_result = after_func(request, kargs) if (after_result != None): return after_result return wrapper return outer@Filter(Before, After)def Index(request, kargs): print &apos;index&apos;Index(&apos;request&apos;,&apos;a&apos;) 今天的任务就是理解上面这个例子，这个例子摘自网上一个比较复杂的例子。 理解上面的例子之前先看一个简单的例子： 1234567891011def w1(func): def inner(): # 验证1 # 验证2 # 验证3 return func() return inner @w1def f1(): print &apos;f1&apos; 当写完这段代码后（函数未被执行、未被执行、未被执行），python解释器就会从上到下解释代码，步骤如下： 12def w1(func): ==&gt;将w1函数加载到内存@w1 从表面上看解释器仅仅会解释这两句代码，因为函数在没有被调用之前其内部代码不会被执行。从表面上看解释器着实会执行这两句，但是 @w1 这一句代码里却有大文章，@函数名 是python的一种语法糖。如上例@w1内部会执行一下操作： 执行w1函数，并将 @w1 下面的 函数 作为w1函数的参数，即：@w1 等价于 w1(f1)所以，内部就会去执行： 1234def inner: #验证 return f1() # func是参数，此时 func 等于 f1return inner # 返回的 inner，inner代表的是函数，非执行函数 其实就是将原来的 f1 函数塞进另外一个函数中将执行完的 w1 函数返回值 赋值给@w1下面的函数的函数名 w1函数的返回值是： 123def inner: #验证return 原来f1() # 此处的 f1 表示原来的f1函数 为什么返回值是一个函数，从上面 1234567def w1(func): def inner(): # 验证1 # 验证2 # 验证3 return func() return inner 可以看到w1函数返回的是inner，是一个函数，而非函数的执行。所以这里的innter就是一个函数。一个被定义在w1的内部函数。 然后，将此返回值再重新赋值给 f1，即： 123新f1 = def inner: #验证 return 原来f1() 所以，想要执行 f1 函数时，就会执行 新f1 函数，在 新f1 函数内部先执行验证，再执行原来的f1函数，然后将 原来f1 函数的返回值 返回给了业务调用者。 如此一来， 即执行了验证的功能，又执行了原来f1函数的内容，并将原f1函数返回值 返回给业务调用着 2、装饰的函数如果有参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#一个参数def w1(func): def inner(arg): # 验证1 # 验证2 # 验证3 return func(arg) return inner @w1def f1(arg): print &apos;f1&apos;#两个参数def w1(func): def inner(arg1,arg2): # 验证1 # 验证2 # 验证3 return func(arg1,arg2) return inner @w1def f1(arg1,arg2): print &apos;f1&apos;#三个参数def w1(func): def inner(arg1,arg2,arg3): # 验证1 # 验证2 # 验证3 return func(arg1,arg2,arg3) return inner @w1def f1(arg1,arg2,arg3): print &apos;f1&apos;# N个参数def w1(func): def inner(*args,**kwargs): # 验证1 # 验证2 # 验证3 return func(*args,**kwargs) return inner @w1def f1(arg1,arg2,arg3): print &apos;f1&apos; 3、一个函数可以被多个装饰器装饰123456789101112131415161718192021def w1(func): def inner(*args,**kwargs): # 验证1 # 验证2 # 验证3 return func(*args,**kwargs) return inner def w2(func): def inner(*args,**kwargs): # 验证1 # 验证2 # 验证3 return func(*args,**kwargs) return inner @w1@w2def f1(arg1,arg2,arg3): print &apos;f1&apos; 最后来解释一下这个复杂的例子1234567891011121314151617181920212223242526272829303132#!/usr/bin/env python#coding:utf-8 def Before(request,kargs): print &apos;before&apos; def After(request,kargs): print &apos;after&apos; def Filter(before_func,after_func): def outer(main_func): def wrapper(request,kargs): before_result = before_func(request,kargs) if(before_result != None): return before_result; main_result = main_func(request,kargs) if(main_result != None): return main_result; after_result = after_func(request,kargs) if(after_result != None): return after_result; return wrapper return outer @Filter(Before, After)def Index(request,kargs): print &apos;index&apos; 这是一个装饰器带参数的例子。等价于1Filter(Before, After)(Index(request,kargs))这样写应该是不合法的。 函数执行的步骤如下： def Before(request,kargs) def After(request,kargs) def Filter(before_func,after_func) @Filter(Before, After) 123@Filter(Before, After)python的语法糖@Filter(Before, After)内部会执行一下操作：执行Filter(Before, After)函数，并且把@Filter(Before, After)下面的函数作为Filter(Before, After)函数的参数，即：@Filter(Before, After)等价于Filter(Before, After)(Index(request,kargs))。 1234567891011121314151617181920212223242526272829303132333435363738394041到了这里就比较关键了，如果上面的结果不好理解可以f = Fileter(Before, After)下面就变成了f(Index(reqeust,kargs))是不是和上面的差不多了。分成了2部分执行。f = Fileter(Before, After) 是函数的执行。f = return outer则f = def outer(main_func): def wrapper(request,kargs): before_result = before_func(request,kargs) if(before_result != None): return before_result; main_result = main_func(request,kargs) if(main_result != None): return main_result; after_result = after_func(request,kargs) if(after_result != None): return after_result; return wrapper了f(Index(reqeust,kargs)) 其实和上面的一样了。了f(Index(reqeust,kargs)) = def wrapper(request,kargs): before_result = before_func(request,kargs) if(before_result != None): return before_result; Index = 原来的Index(request,kargs) if(Index != None): return Index; after_result = after_func(request,kargs) if(after_result != None): return after_result; 4、思考我们费力的说上面这个复杂的例子。那个这里例子到底有啥用呢。这个例子也就是用2个函数修饰一个函数。即： 第一个修饰的函数 被修饰的函数 第二个修饰的函数","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"访问谷歌","date":"2017-04-19T04:00:00.000Z","path":"2017/04/19/访问谷歌/","text":"Gmail访问谷歌助手: http://www.gmailaccess.com/Lantern软件","tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"openstack模块之stevedore","date":"2017-04-18T15:37:00.000Z","path":"2017/04/18/openstack模块之stevedore/","text":"[Toc]stevedore是用来实现动态加载代码的开源模块。它是在OpenStack中用来加载插件的公共模块。可以独立于OpenStack而安装使用：https://pypi.python.org/pypi/stevedore/ stevedore使用setuptools的entry points来定义并加载插件。entry point引用的是定义在模块中的==对象==，比如==类、函数、实例==等，只要在import模块时能够被创建的对象都可以。 一：插件的名字和命名空间 一般来讲，entry point的名字是公开的，用户可见的，经常出现在配置文件中。而命名空间，也就是entry point组名却是一种实现细节，一般是面向开发者而非最终用户的。可以用Python的包名作为entry point命名空间，以保证唯一性，但这不是必须的。 entry points的主要特征就是，它可以是独立注册的，也就是说插件的开发和安装可以完全独立于使用它的应用，只要开发者和使用者在命名空间和API上达成一致即可。 命名空间被用来搜索entry points。entry points的名字在给定的发布包中必须是唯一的，但在一个命名空间中可以不唯一。也就是说，同一个发布包内不允许出现同名的entry point，但是如果是两个独立的发布包，却可以使用完全相同的entrypoint组名和entry point名来注册插件 下面举例： 123entry points的每一个命名空间里,可以包含多个entry point项.stevedore要求每一项都符合如下格式:name = module:importable #我是entry point的一个项左边是插件的名称,右边是它的具体实现,中间用等号分隔开.插件的具体实现用&quot;模块:可导入的对象&quot;的形式来指定,以Ceilometer为例: 123456789ceilometer.compute.virt = libvirt = ceilometer.compute.virt.libvirt.inspector:LibvirtInspector hyperv = ceilometer.compute.virt.hyperv.inspector:HyperVInspector vsphere = ceilometer.compute.virt.vmware.inspector:VsphereInspector ceilometer.hardware.inspectors = snmp = ceilometer.hardware.inspectors.snmp:SNMPInspector 示例中显示了两个不同的entry points的命名空间,“ceilometer.compute.virt&quot;和&quot;ceilometer.hardware.inspectors&quot;,分别注册有3个和1个插件.每个插件都符合&quot;名字=模块:可导入对象”的格式,在“ceilometer.compute.virt&quot;命名空间里的libvirt插件,它的具体可载入的实现是ceilometer.compute.virt.libvirt.inspector模块中的LibvirtInspector类. 二：插件的使用方式2.1 Drivers一个名字对应一个entry point。使用时根据插件的命名空间和名字，定位到单独的插件。 2.2 Hooks一个名字对应多个entry point。允许同一个命名空间中的插件具有相同的名字，根据给定的命名空间和名字，加载该名字对应的多个插件。 类似于下面这种，根据一个命名空间和同一个名字libvirt可以加到下面三个不通的插件1234ceilometer.compute.virt = libvirt = ceilometer.compute.virt.libvirt.inspector:LibvirtInspector libvirt = ceilometer.compute.virt.hyperv.inspector:HyperVInspector libvirt = ceilometer.compute.virt.vmware.inspector:VsphereInspector 2.3 Extensions多个名字，多个entry point。给定命名空间，加载该命名空间中所有的插件，当然也允许同一个命名空间中的插件具有相同的名字。 三 : 插件的定义、注册、载入使用stevedore来帮助程序动态载入插件的过程主要分为三个部分:插件的实现,插件的注册,以及插件的载入.下面我们以Ceilometer里动态载入compute agent上的inspector驱动为例来分别进行介绍. 3.1 插件的定义1234567891011在经过了大量的试验和总结教训之后，发现定义API最简单的方式是遵循下面的步骤： a：使用abc模块，创建一个抽象基类来定义插件API的行为；虽然开发者无需继承一个基类，但是这种方式自有它的好处； b：通过继承基类并实现必要的方法来创建插件 c：为每个API定义一个命名空间。可以将应用或者库的名字，以及API的名字结合起来，这种方式通俗易懂，如 “cliff.formatters”或“ceilometer.pollsters.compute”。 本节例子中创建的插件，用来对数据进行格式化输出，每个格式化方法接受一个字典作为输入，然后按照一定的规则产生要输出的字符串。格式化类可以有一个最大输出宽度的参数 12345678910111213# 首先定义一个基类class FormatterBase(object): def __init__(self, max_width=60): self.max_width = max_width def format(self, data): &quot;&quot;&quot;Format the data and return unicode text. :param data: A dictionary with string keys and simple types as values. :type data: dict(str:?) :returns: Iterable producing the formatted text. &quot;&quot;&quot; 123456789101112# 开始定义具体的插件类，这些类需要实现format方法。下面是一个简单的插件，它产生的输出都在一行上。import pluginbaseclass Simple(pluginbase.FormatterBase): def format(self, data): for name, value in sorted(data.items()): line = &apos;&#123;name&#125; = &#123;value&#125;\\n&apos;.format( name=name, value=value, ) yield line 3.2 插件注册12345setup.pyexample/ __init__.py pluginbase.py simple.py 123456789101112131415from setuptools import find_packages,setupsetup( name=&apos;stevedoretest1&apos;, version=&apos;1.0&apos;, packages=find_packages(), entry_points=&#123; &apos;stevedoretest.formatter&apos;: [ &apos;simple = example.simple:Simple&apos;, &apos;plain = example.simple:Simple&apos;, ], &#125;,) 12345678910每个entry point都以” name = module:importable ”的形式进行注册，name就是插件的名字，module就是python模块，importable就是模块中可引用的对象。这里注册了两个插件，simple和plain，这两个插件所引用的Python对象是一样的，都是example.simple:Simple类，因此plain只是simple的别名而已。定义好setup.py之后，运行python setup.py install即可安装该发布包。安装成功后，在该发布的egg目录中存在文件entry_points.txt，其内容如下：[stevedoretest.formatter]plain = example.simple:Simplesimple = example.simple:Simple运行时，pkg_resources在所有已安装包的entry_points.txt中寻找插件，因此不要手动编辑该文件。 1使用entry points创建插件的好处之一就是，可以为一个应用独立的开发不同的插件。因此可以在另外一个发布包中定义第二个插件： 注册插件21234setup.pyexample2/ __init__.py fields.py 123456789101112131415161718#example2/fields.pyimport textwrapfrom example import pluginbaseclass FieldList(pluginbase.FormatterBase): def format(self, data): for name, value in sorted(data.items()): full_text = &apos;: &#123;name&#125; : &#123;value&#125;&apos;.format( name=name, value=value, ) wrapped_text = textwrap.fill( full_text, initial_indent=&apos;&apos;, subsequent_indent=&apos; &apos;, width=self.max_width, ) yield wrapped_text + &apos;\\n&apos; 12345678910111213141516171819# setup2.pyfrom setuptools import setup, find_packagessetup( name=&apos;stevedoretest2&apos;, version=&apos;1.0&apos;, packages=find_packages(), entry_points=&#123; &apos;stevedoretest.formatter&apos;: [ &apos;fields = example2.fields:FieldList&apos; ], &#125;,)这里注册了插件fields，它引用的是example2.fields:FieldList类。定义好setup.py之后，运行python setup.py install即可安装该发布包。在该发布的entry_points.txt文件内容如下：[stevedoretest.formatter]fields = example2.fields:FieldList 3.3插件的载入3.3.1 Drivers加载 最常见的使用插件的方式是作为单独的驱动来使用，这种场景中，可以有多种插件，但只需要加载和调用其中的一个，这种情况下，可以使用stevedore的DriverManager 类。下面就是一个使用该类的例子： 1234567891011121314151617181920212223242526272829303132333435from __future__ import print_functionfrom stevedore import driverimport argparseif __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser() parser.add_argument( &apos;format&apos;, nargs=&apos;?&apos;, default=&apos;simple&apos;, help=&apos;the output format&apos;, ) parser.add_argument( &apos;--width&apos;, default=60, type=int, help=&apos;maximum output width for text&apos;, ) parsed_args = parser.parse_args()data = &#123; &apos;a&apos;: &apos;A&apos;, &apos;b&apos;: &apos;B&apos;, &apos;long&apos;: &apos;word &apos; * 80, &#125;mgr = driver.DriverManager( namespace=&apos;stevedoretest.formatter&apos;, name=parsed_args.format, invoke_on_load=True, invoke_args=(parsed_args.width,), )for chunk in mgr.driver.format(data): print(chunk, end=&apos;&apos;) 其中的parser主要用来解析命令行参数的，该脚本接受三个参数，一个是format，也就是要使用的插件名字，这里默认是simple；另一个参数是–width，是插件方法可能会用到的参数，这里默认是60，该脚本还可以通过–help参数打印帮助信息： 123456789E:\\workspace\\learn_stevedore&gt;python drivers.py --helpusage: drivers.py [-h] [--width WIDTH] [format]positional arguments: format the output formatoptional arguments: -h, --help show this help message and exit --width WIDTH maximum output width for text 1在该脚本中，driver.DriverManager以插件的命名空间以及插件名来寻找插件，也就是entry points组名和entry points本身的名字。也就是希望通过组名和entry point本身的名字来唯一定位一个插件，但是因为相同的entry points组中可以有同名的entry point，所以，对于DriverManager来说，如果通过entry points组名和entry points本身的名字找到了多个注册的插件，则会报错。比如本例中，如果在”stevedoretest.formatter”中，有多个发布模块注册了名为”simiple”的entry point，则执行该脚本时就会报错： 1234567 因invoke_on_load为True，所以在加载该插件的时候就会调用它，这里插件引用的是一个类，所以加载插件的时候，就会实例化该类。invoke_args会以位置参数的形式，传递给该类的初始化方法，也就是用来设置输出的宽度。当创建了一个manager之后，就已经创建好了某个具体插件类的实例。该实例的引用被关联到了manager的driver属性上，因此可以通过driver调用该实例的方法了：for chunk in mgr.driver.format(data): print(chunk, end=&apos;&apos;) 下面试试看运行一下 1234python drivers.pypython drivers.py plain python drivers.py fieldspython drivers.py fields --width 30 3.3.2 Extensions加载 另一种使用插件的方式是一次性的加载多个扩展，可以有多个manager类支持这种使用模式，包括ExtensionManager，NamedExtensionManager和 EnabledExtensionManager。比如下面的代码： 123456789101112131415161718192021222324252627282930313233343536import argparsefrom stevedore import extensionif __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser() parser.add_argument( &apos;--width&apos;, default=60, type=int, help=&apos;maximum output width for text&apos;, ) parsed_args = parser.parse_args() data = &#123; &apos;a&apos;: &apos;A&apos;, &apos;b&apos;: &apos;B&apos;, &apos;long&apos;: &apos;word &apos; * 80, &#125; mgr = extension.ExtensionManager( namespace=&apos;stevedoretest.formatter&apos;, invoke_on_load=True, invoke_args=(parsed_args.width,), ) def format_data(ext, data): return (ext.name, ext.obj.format(data)) results = mgr.map(format_data, data) for name, result in results: print &apos;Formatter: %s&apos;%name for chunk in result: print chunk 123456789101112131415161718ExtensionManager和DriverManager略有不同，它不需要提前知道要加载哪个插件，它会加载所有找到的插件。要想调用插件，需要使用map方法，需要传给map一个函数，这里就是format_data函数，针对每个扩展都会调用该函数。format_data函数有两个参数，分别是Extension实例和map的第二个参数data：def format_data(ext, data): return (ext.name, ext.obj.format(data))results = mgr.map(format_data, data)format_data的Extension参数，是stevedore中封装插件的一个类，该类的成员有：表示插件名字的name；表示由pkg_resources返回的EntryPoint实例的entry_point，表示插件本身的plugin，也就是entry_point.load()的返回值；如果invoke_on_load为True，则还有一个成员obj表示调用plugin(*args, **kwds)后返回的结果。map函数返回一个序列，其中的每个元素就是回调函数的返回值，也就是format_data的返回值。函数format_data返回一个元组，该元组包含扩展的名字，以及调用插件的format方法后的返回值。插件加载的顺序是未定义的，该顺序取决于找到包的顺序，以及包中元数据文件的读取顺序，如果关心插件加载的顺序的话，可以使用NamedExtensionManager.类。下面是调用该脚本的例子：之所以要在map中使用回调函数，而不是直接调用插件，是因为这样做可以保持应用代码和插件之间的隔离性，这种隔离性有利于应用代码和插件API的设计。如果map直接调用插件，则每个插件必须是可调用的，这样命名空间实际上就只能用于插件的一个方法上了。使用回调函数，则插件的API就无需在应用中匹配特定的用例。 3.3.3 Hook加载最后一种使用插件的方式，相当于Drivers加载和Extensions加载的结合。它允许在给定的entry points组名下有同名的entry point，这样，在给定entry points组名和entry point名的情况下，hook式加载会加载所有找到的插件。 比如这里的例子，在插件2中，注册一个同样名为”simple”的插件，修改其setup.py内如如下： 12345678910111213141516171819202122232425262728# coding: utf-8&quot;&quot;&quot;@version: v1.0@author: wanstack@license: Apache Licence @contact: 45yujing@gmail.com@site: http://www.wanstack.com@software: PyCharm@file: setup3.py@time: 2017/4/18 11:17&quot;&quot;&quot;from setuptools import setup, find_packagessetup( name=&apos;stevedoretest2&apos;, version=&apos;1.0&apos;, packages=find_packages(), entry_points=&#123; &apos;stevedoretest.formatter&apos;: [ &apos;fields = example2.fields:FieldList&apos;, &apos;simple= example2.fields:FieldList&apos; ], &#125;,) 运行1python setup3.py install 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import argparsefrom stevedore import hookif __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser() parser.add_argument( &apos;format&apos;, nargs=&apos;?&apos;, default=&apos;simple&apos;, help=&apos;the output format&apos;, ) parser.add_argument( &apos;--width&apos;, default=60, type=int, help=&apos;maximum output width for text&apos;, ) parsed_args = parser.parse_args() data = &#123; &apos;a&apos;: &apos;A&apos;, &apos;b&apos;: &apos;B&apos;, &apos;long&apos;: &apos;word &apos; * 80, &#125; mgr = hook.HookManager( namespace=&apos;stevedoretest.formatter&apos;, name=parsed_args.format, invoke_on_load=True, invoke_args=(parsed_args.width,), ) def format_data(ext, data): return (ext.name, ext.obj.format(data)) results = mgr.map(format_data, data) for name, result in results: print &apos;Formatter: %s&apos; % name for chunk in result: print chunk 这里使用hook.HookManager加载插件，参数与构建DriverManager 时是一样的，都是需要给定插件的namespace和name。又因为hook.HookManager继承自NamedExtensionManager，而NamedExtensionManager又继承自ExtensionManager。所以这里使用插件的方式与上例一样。下面是调用该脚本的例子","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Openstack Mitaka HA安装部署手册","date":"2017-04-14T05:38:20.398Z","path":"2017/04/14/Openstack-Mitaka HA 部署手册/","text":"==openstack运维开发群:94776000 欢迎牛逼的你== Openstack Mitaka HA 实施部署测试文档一、环境说明1、主机环境123456controller(VIP) 192.168.10.100controller01 192.168.10.101, 10.0.0.1controller02 192.168.10.102, 10.0.0.2controller03 192.168.10.103, 10.0.0.3compute01 192.168.10.104, 10.0.0.4compute02 192.168.10.105, 10.0.0.5 本次环境仅限于测试环境，主要测试HA功能。具体生产环境请对网络进行划分。 二、配置基础环境1、配置主机解析12345678910111213141516在对应节点上配置主机名：hostnamectl set-hostname controller01hostname contoller01hostnamectl set-hostname controller02hostname contoller02hostnamectl set-hostname controller03hostname contoller03hostnamectl set-hostname compute01hostname compute01hostnamectl set-hostname compute02hostname compute02 12345678910111213在controller01上配置主机解析：[root@controller01 ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.10.100 controller192.168.10.101 controller01192.168.10.102 controller02192.168.10.103 controller03192.168.10.104 compute01192.168.10.105 compute02 2、配置ssh互信1234567在controller01上进行配置：ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &apos;&apos;ssh-copy-id -i .ssh/id_rsa.pub root@controller02ssh-copy-id -i .ssh/id_rsa.pub root@controller03ssh-copy-id -i .ssh/id_rsa.pub root@compute01ssh-copy-id -i .ssh/id_rsa.pub root@compute02 12345拷贝主机名解析配置文件到其他节点scp /etc/hosts controller02:/etc/hostsscp /etc/hosts controller03:/etc/hostsscp /etc/hosts compute01:/etc/hostsscp /etc/hosts compute02:/etc/hosts 3、yum 源配置本次测试机所有节点都可以正常连接网络，故使用阿里云的openstack和base源 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 所有控制和计算节点开启yum缓存[root@controller01 ~]# cat /etc/yum.conf [main]cachedir=/var/cache/yum/$basearch/$releasever# 开启缓存keepcache=1表示开启缓存，keepcache=0表示不开启缓存，默认为0keepcache=1debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1installonly_limit=5bugtracker_url=http://bugs.centos.org/set_project.php?project_id=23&amp;ref=http://bugs.centos.org/bug_report_page.php?category=yumdistroverpkg=centos-release# 基础源yum install wget -yrm -rf /etc/yum.repos.d/*wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# openstack mitaka源yum install centos-release-openstack-mitaka -y# 默认是centos源，建议修改成阿里云的，因为速度快[root@contoller01 yum.repos.d]# vim CentOS-OpenStack-mitaka.repo # CentOS-OpenStack-mitaka.repo## Please see http://wiki.centos.org/SpecialInterestGroup/Cloud for more# information[centos-openstack-mitaka]name=CentOS-7 - OpenStack mitakabaseurl=http://mirrors.aliyun.com//centos/7/cloud/$basearch/openstack-mitaka/gpgcheck=1enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-Cloud# galera源vim mariadb.repo[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.1/centos7-amd64/enable=1gpgcheck=1gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB scp 到其他所有节点 123456789scp CentOS-OpenStack-mitaka.repo controller02:/etc/yum.repos.d/CentOS-OpenStack-mitaka.reposcp CentOS-OpenStack-mitaka.repo controller03:/etc/yum.repos.d/CentOS-OpenStack-mitaka.reposcp CentOS-OpenStack-mitaka.repo compute01:/etc/yum.repos.d/CentOS-OpenStack-mitaka.reposcp CentOS-OpenStack-mitaka.repo compute02:/etc/yum.repos.d/CentOS-OpenStack-mitaka.reposcp mariadb.repo controller02:/etc/yum.repos.d/mariadb.reposcp mariadb.repo controller03:/etc/yum.repos.d/mariadb.reposcp mariadb.repo compute01:/etc/yum.repos.d/mariadb.reposcp mariadb.repo compute02:/etc/yum.repos.d/mariadb.repo 4、ntp配置本机环境已经有ntp服务器，故直接使用。如果没有ntp服务器建议使用controller作为ntp服务器 123yum install ntpdate -yecho &quot;*/5 * * * * /usr/sbin/ntpdate 192.168.2.161 &gt;/dev/null 2&gt;&amp;1&quot; &gt;&gt; /var/spool/cron/root/usr/sbin/ntpdate 192.168.2.161 5、关闭防火墙和selinux12345systemctl disable firewalld.servicesystemctl stop firewalld.servicesed -i -e &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsed -i -e &quot;s#SELINUXTYPE=targeted#\\#SELINUXTYPE=targeted#g&quot; /etc/selinux/configsetenforce 0 6、安装配置pacemaker12345678910111213141516171819202122232425262728293031323334# 所有控制节点安装如下软件yum install -y pcs pacemaker corosync fence-agents-all resource-agents修改corosync配置文件[root@contoller01 ~]# cat /etc/corosync/corosync.conftotem &#123;version: 2secauth: offcluster_name: openstack-clustertransport: udpu&#125;nodelist &#123; node &#123; ring0_addr: controller01 nodeid: 1 &#125; node &#123; ring0_addr: controller02 nodeid: 2 &#125; node &#123; ring0_addr: controller03 nodeid: 3&#125;&#125;quorum &#123;provider: corosync_votequorumtwo_node: 1&#125;logging &#123;to_syslog: yes&#125; 123# 把配置文件拷贝到其他控制节点scp /etc/corosync/corosync.conf controller02:/etc/corosync/corosync.confscp /etc/corosync/corosync.conf controller03:/etc/corosync/corosync.conf 12# 查看成员信息corosync-cmapctl runtime.totem.pg.mrp.srp.members 123456789101112131415161718192021222324# 所有控制节点启动服务systemctl enable pcsdsystemctl start pcsd# 所有控制节点设置hacluster用户的密码echo hacluster | passwd --stdin hacluster# [controller01]设置到集群节点的认证pcs cluster auth controller01 controller02 controller03 -u hacluster -p hacluster --force# [controller01]创建并启动集群pcs cluster setup --force --name openstack-cluster controller01 controller02 controller03pcs cluster start --all# [controller01]设置集群属性pcs property set pe-warn-series-max=1000 pe-input-series-max=1000 pe-error-series-max=1000 cluster-recheck-interval=5min# [controller01] 暂时禁用STONISH，否则资源无法启动pcs property set stonith-enabled=false# [controller01] 忽略投票pcs property set no-quorum-policy=ignore# [controller01]配置VIP资源，VIP可以在集群节点间浮动pcs resource create vip ocf:heartbeat:IPaddr2 params ip=192.168.10.100 cidr_netmask=&quot;24&quot; op monitor interval=&quot;30s&quot; 7、安装haproxy1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# [所有控制节点] 安装软件yum install -y haproxy# [所有控制节点] 修改/etc/rsyslog.d/haproxy.conf文件echo &quot;\\$ModLoad imudp&quot; &gt;&gt; /etc/rsyslog.d/haproxy.conf;echo &quot;\\$UDPServerRun 514&quot; &gt;&gt; /etc/rsyslog.d/haproxy.conf;echo &quot;local3.* /var/log/haproxy.log&quot; &gt;&gt; /etc/rsyslog.d/haproxy.conf;echo &quot;&amp;~&quot; &gt;&gt; /etc/rsyslog.d/haproxy.conf;# [所有控制节点] 修改/etc/sysconfig/rsyslog文件sed -i -e &apos;s#SYSLOGD_OPTIONS=\\&quot;\\&quot;#SYSLOGD_OPTIONS=\\&quot;-c 2 -r -m 0\\&quot;#g&apos; /etc/sysconfig/rsyslog# [所有控制节点] 重启rsyslog服务systemctl restart rsyslog# 创建haproxy基础配置vim /etc/haproxy/haproxy.cfg#---------------------------------------------------------------------# Example configuration for a possible web application. See the# full configuration options online.## http://haproxy.1wt.eu/download/1.4/doc/configuration.txt##---------------------------------------------------------------------#---------------------------------------------------------------------# Global settings#---------------------------------------------------------------------global # to have these messages end up in /var/log/haproxy.log you will # need to: # # 1) configure syslog to accept network log events. This is done # by adding the &apos;-r&apos; option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local3 chroot /var/lib/haproxy daemon group haproxy maxconn 4000 pidfile /var/run/haproxy.pid user haproxy#---------------------------------------------------------------------# common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will# use if not designated in their block#---------------------------------------------------------------------defaults log global maxconn 4000 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout check 10s include conf.d/*.cfg 123# 拷贝到其他控制节点scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg 1234567# [controller01]在pacemaker集群增加haproxy资源pcs resource create haproxy systemd:haproxy --clone# Optional表示只在同时停止和/或启动两个资源时才会产生影响。对第一个指定资源进行的任何更改都不会对第二个指定的资源产生影响，定义在前面的资源先确保运行。pcs constraint order start vip then haproxy-clone kind=Optional# vip的资源决定了haproxy-clone资源的位置约束pcs constraint colocation add haproxy-clone with vipping -c 3 192.168.10.100 8、galera安装配置123456789101112131415161718192021222324#所有控制节点上操作基本操作 ：安装、设置配置文件 yum install -y MariaDB-server xinetd# 在所有控制节点进行配置vim /usr/lib/systemd/system/mariadb.service[Service]新添加两行如下参数：LimitNOFILE=10000LimitNPROC=10000systemctl --system daemon-reload systemctl restart mariadb.service # 初始化数据库，在controller01上执行即可systemctl start mariadbmysql_secure_installation# 查看并发数show variables like &apos;max_connections&apos;; # 关闭服务修改配置文件systemctl stop mariadb# 备份原始配置文件cp /etc/my.cnf.d/server.cnf /etc/my.cnf.d/bak.server.cnf 1234567891011121314151617181920212223# 配置controller01上的配置文件cat /etc/my.cnf.d/server.cnf[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockuser=mysqlbinlog_format=ROWmax_connections = 4096bind-address= 192.168.10.101default_storage_engine=innodbinnodb_autoinc_lock_mode=2innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_on=ONwsrep_provider=/usr/lib64/galera/libgalera_smm.sowsrep_provider_options=&quot;pc.recovery=TRUE;gcache.size=300M&quot;wsrep_cluster_name=&quot;galera_cluster&quot;wsrep_cluster_address=&quot;gcomm://controller01,controller02,controller03&quot;wsrep_node_name= controller01wsrep_node_address= 192.168.10.101wsrep_sst_method=rsync 1234567891011121314151617181920212223# 配置controller02上的配置文件cat /etc/my.cnf.d/server.cnf[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockuser=mysqlbinlog_format=ROWmax_connections = 4096bind-address= 192.168.10.102default_storage_engine=innodbinnodb_autoinc_lock_mode=2innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_on=ONwsrep_provider=/usr/lib64/galera/libgalera_smm.sowsrep_provider_options=&quot;pc.recovery=TRUE;gcache.size=300M&quot;wsrep_cluster_name=&quot;galera_cluster&quot;wsrep_cluster_address=&quot;gcomm://controller01,controller02,controller03&quot;wsrep_node_name= controller02wsrep_node_address= 192.168.10.102wsrep_sst_method=rsync 1234567891011121314151617181920212223# 配置controller03上的配置文件cat /etc/my.cnf.d/server.cnf[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockuser=mysqlbinlog_format=ROWmax_connections = 4096bind-address= 192.168.10.103default_storage_engine=innodbinnodb_autoinc_lock_mode=2innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_on=ONwsrep_provider=/usr/lib64/galera/libgalera_smm.sowsrep_provider_options=&quot;pc.recovery=TRUE;gcache.size=300M&quot;wsrep_cluster_name=&quot;galera_cluster&quot;wsrep_cluster_address=&quot;gcomm://controller01,controller02,controller03&quot;wsrep_node_name= controller03wsrep_node_address= 192.168.10.103wsrep_sst_method=rsync 123456789# 在controller01上执行galera_new_cluster#查看日志tail -f /var/log/messages# 启动其他控制节点systemctl enable mariadbsystemctl start mariadb 1234# 添加checkmysql -uroot -popenstack -e &quot;use mysql;INSERT INTO user(Host, User) VALUES(&apos;192.168.10.100&apos;, &apos;haproxy_check&apos;);FLUSH PRIVILEGES;&quot;mysql -uroot -popenstack -e &quot;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;controller01&apos; IDENTIFIED BY &apos;&quot;openstack&quot;&apos;&quot;;mysql -uroot -popenstack -h 192.168.10.100 -e &quot;SHOW STATUS LIKE &apos;wsrep_cluster_size&apos;;&quot; 12345678910111213141516# 配置haproxy for galera# 所有控制节点创建haproxy配置文件目录cat /etc/haproxy/haproxy.cfglisten galera_cluster bind 192.168.10.100:3306 balance source #option mysql-check user haproxy_check server controller01 192.168.10.101:3306 inter 2000 rise 2 fall 5 server controller02 192.168.10.102:3306 inter 2000 rise 2 fall 5 server controller03 192.168.10.103:3306 inter 2000 rise 2 fall 5# 拷贝配置文件到其他控制节点scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/scp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/ 12345678910111213141516171819# 重启pacemaker,corosync集群脚本vim restart-pcs-cluster.sh#!/bin/shpcs cluster stop --allsleep 10#ps aux|grep &quot;pcs cluster stop --all&quot;|grep -v grep|awk &apos;&#123;print $2 &#125;&apos;|xargs killfor i in 01 02 03; do ssh controller$i pcs cluster kill; donepcs cluster stop --allpcs cluster start --allsleep 5watch -n 0.5 pcs resourceecho &quot;pcs resource&quot;pcs resourcepcs resource|grep Stoppcs resource|grep FAILED# 执行脚本bash restart-pcs-cluster.sh 9、安装配置rabbitmq-server集群123456789101112131415161718192021222324252627282930313233# 所有控制节点yum install -y rabbitmq-server# 拷贝controller01上的cookie到其他控制节点上scp /var/lib/rabbitmq/.erlang.cookie root@controller02:/var/lib/rabbitmq/.erlang.cookiescp /var/lib/rabbitmq/.erlang.cookie root@controller03:/var/lib/rabbitmq/.erlang.cookie# controller01以外的其他节点设置权限chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookiechmod 400 /var/lib/rabbitmq/.erlang.cookie# 启动服务systemctl enable rabbitmq-server.servicesystemctl start rabbitmq-server.service# 在任意控制节点上查看集群配置rabbitmqctl cluster_status# controller01以外的其他节点 加入集群rabbitmqctl stop_apprabbitmqctl join_cluster --ram rabbit@controller01rabbitmqctl start_app# 在任意节点 设置ha-moderabbitmqctl cluster_status;rabbitmqctl set_policy ha-all &apos;^(?!amq\\.).*&apos; &apos;&#123;&quot;ha-mode&quot;: &quot;all&quot;&#125;&apos;# 在任意节点执行创建用户rabbitmqctl add_user openstack openstackrabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 10、安装配置memcache1234567891011121314151617181920212223242526272829yum install -y memcached# controller01上修改配置cat /etc/sysconfig/memcached PORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;64&quot;OPTIONS=&quot;-l 192.168.10.101,::1&quot;# controller02上修改配置cat /etc/sysconfig/memcached PORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;64&quot;OPTIONS=&quot;-l 192.168.10.102,::1&quot;# controller03上修改配置cat /etc/sysconfig/memcached PORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;64&quot;OPTIONS=&quot;-l 192.168.10.103,::1&quot;# 所有节点启动服务systemctl enable memcached.servicesystemctl start memcached.service 三、安装配置openstack软件集群12# 所有控制节点和计算节点安装openstack 基础包yum install -y python-openstackclient openstack-selinux openstack-utils 1、安装openstack Identity123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255# 在任意节点创建keystone数据库mysql -uroot -popenstack -e &quot;CREATE DATABASE keystone;GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;keystone&quot;&apos;;GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;keystone&quot;&apos;;FLUSH PRIVILEGES;&quot;# 所有节点安装keystone软件包yum install -y openstack-keystone httpd mod_wsgi# 任意节点生成临时tokenopenssl rand -hex 108464d030a1f7ac3f7207# 修改keystone配置文件openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token 8464d030a1f7ac3f7207openstack-config --set /etc/keystone/keystone.conf database connection mysql+pymysql://keystone:keystone@controller/keystone#openstack-config --set /etc/keystone/keystone.conf token provider fernetopenstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/keystone/keystone.conf oslo_messaging_rabbit rabbit_durable_queues true# 拷贝配置文件到其他控制节点scp /etc/keystone/keystone.conf controller02:/etc/keystone/keystone.confscp /etc/keystone/keystone.conf controller03:/etc/keystone/keystone.confsed -i -e &apos;s#\\#ServerName www.example.com:80#ServerName &apos;&quot;controller01&quot;&apos;#g&apos; /etc/httpd/conf/httpd.confsed -i -e &apos;s#\\#ServerName www.example.com:80#ServerName &apos;&quot;controller02&quot;&apos;#g&apos; /etc/httpd/conf/httpd.confsed -i -e &apos;s#\\#ServerName www.example.com:80#ServerName &apos;&quot;controller03&quot;&apos;#g&apos; /etc/httpd/conf/httpd.conf# controller01上的配置vim /etc/httpd/conf.d/wsgi-keystone.confListen 192.168.10.101:5000Listen 192.168.10.101:35357&lt;VirtualHost 192.168.10.101:5000&gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;&lt;VirtualHost 192.168.10.101:35357&gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;# controller02上的配置vim /etc/httpd/conf.d/wsgi-keystone.confListen 192.168.10.102:5000Listen 192.168.10.102:35357&lt;VirtualHost 192.168.10.102:5000&gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;&lt;VirtualHost 192.168.10.102:35357&gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;# controller03上的配置vim /etc/httpd/conf.d/wsgi-keystone.confListen 192.168.10.103:5000Listen 192.168.10.103:35357&lt;VirtualHost 192.168.10.103:5000&gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;&lt;VirtualHost 192.168.10.103:35357&gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;# 添加haproxy配置vim /etc/haproxy/haproxy.cfglisten keystone_admin_cluster bind 192.168.10.100:35357 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:35357 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:35357 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:35357 check inter 2000 rise 2 fall 5listen keystone_public_internal_cluster bind 192.168.10.100:5000 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:5000 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:5000 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:5000 check inter 2000 rise 2 fall 5# 把haproxy配置拷贝到其他控制节点scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg# [任一节点]生成数据库su -s /bin/sh -c &quot;keystone-manage db_sync&quot; keystone# [任一节点/controller01]初始化Fernet key，并共享给其他节点#keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone# 在其他控制节点#mkdir -p /etc/keystone/fernet-keys/# 在controller01上#scp /etc/keystone/fernet-keys/* root@controller02:/etc/keystone/fernet-keys/#scp /etc/keystone/fernet-keys/* root@controller03:/etc/keystone/fernet-keys/# 在其他控制节点chown keystone:keystone /etc/keystone/fernet-keys/*# [任一节点]添加pacemaker资源，openstack资源和haproxy资源无关，可以开启A/A模式# interleave=true副本交错启动/停止，改变master/clone间的order约束，每个实例像其他克隆实例一样可快速启动/停止，无需等待其他克隆实例。# interleave这个设置为false的时候,constraint的order顺序的受到其他节点的影响,为true不受其他节点影响pcs resource create openstack-keystone systemd:httpd --clone interleave=truebash restart-pcs-cluster.sh# 在任意节点创建临时tokenexport OS_TOKEN=8464d030a1f7ac3f7207export OS_URL=http://controller:35357/v3export OS_IDENTITY_API_VERSION=3# [任一节点]service entity and API endpointsopenstack service create --name keystone --description &quot;OpenStack Identity&quot; identityopenstack endpoint create --region RegionOne identity public http://controller:5000/v3openstack endpoint create --region RegionOne identity internal http://controller:5000/v3openstack endpoint create --region RegionOne identity admin http://controller:35357/v3# [任一节点]创建项目和用户openstack domain create --description &quot;Default Domain&quot; defaultopenstack project create --domain default --description &quot;Admin Project&quot; adminopenstack user create --domain default --password admin adminopenstack role create adminopenstack role add --project admin --user admin admin### [任一节点]创建service项目openstack project create --domain default --description &quot;Service Project&quot; service# 在任意节点创建demo项目和用户openstack project create --domain default --description &quot;Demo Project&quot; demoopenstack user create --domain default --password demo demoopenstack role create useropenstack role add --project demo --user demo user# 生成keystonerc_admin脚本echo &quot;export OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=adminexport OS_AUTH_URL=http://controller:35357/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2export PS1=&apos;[\\u@\\h \\W(keystone_admin)]\\$ &apos;&quot;&gt;/root/keystonerc_adminchmod +x /root/keystonerc_admin# 生成keystonerc_demo脚本echo &quot;export OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=demoexport OS_USERNAME=demoexport OS_PASSWORD=demoexport OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2export PS1=&apos;[\\u@\\h \\W(keystone_admin)]\\$ &apos;&quot;&gt;/root/keystonerc_demochmod +x /root/keystonerc_demosource keystonerc_admin### checkopenstack token issuesource keystonerc_demo### checkopenstack token issue 2、安装openstack Image集群123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116# [任一节点]创建数据库mysql -uroot -popenstack -e &quot;CREATE DATABASE glance;GRANT ALL PRIVILEGES ON glance.* TO &apos;glance&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;glance&quot;&apos;;GRANT ALL PRIVILEGES ON glance.* TO &apos;glance&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;glance&quot;&apos;;FLUSH PRIVILEGES;&quot;# [任一节点]创建用户等source keystonerc_admin openstack user create --domain default --password glance glanceopenstack role add --project service --user glance adminopenstack service create --name glance --description &quot;OpenStack Image&quot; imageopenstack endpoint create --region RegionOne image public http://controller:9292openstack endpoint create --region RegionOne image internal http://controller:9292openstack endpoint create --region RegionOne image admin http://controller:9292# 所有控制节点安装glance软件包yum install -y openstack-glance# [所有控制节点]配置/etc/glance/glance-api.conf文件openstack-config --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:glance@controller/glanceopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/glance/glance-api.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name serviceopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glanceopenstack-config --set /etc/glance/glance-api.conf keystone_authtoken password glanceopenstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystoneopenstack-config --set /etc/glance/glance-api.conf glance_store stores file,httpopenstack-config --set /etc/glance/glance-api.conf glance_store default_store fileopenstack-config --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/openstack-config --set /etc/glance/glance-api.conf DEFAULT registry_host controlleropenstack-config --set /etc/glance/glance-api.conf DEFAULT bind_host controller01# [所有控制节点]配置/etc/glance/glance-registry.conf文件openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:glance@controller/glanceopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name serviceopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glanceopenstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password glanceopenstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystoneopenstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/glance/glance-registry.conf DEFAULT registry_host controlleropenstack-config --set /etc/glance/glance-registry.conf DEFAULT bind_host controller01scp /etc/glance/glance-api.conf controller02:/etc/glance/glance-api.confscp /etc/glance/glance-api.conf controller03:/etc/glance/glance-api.conf# 修改bind_host为对应的controller02,controller03scp /etc/glance/glance-registry.conf controller02:/etc/glance/glance-registry.confscp /etc/glance/glance-registry.conf controller03:/etc/glance/glance-registry.conf# 修改bind_host为对应的controller02,controller03vim /etc/haproxy/haproxy.cfg# 增加如下配置listen glance_api_cluster bind 192.168.10.100:9292 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:9292 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:9292 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:9292 check inter 2000 rise 2 fall 5listen glance_registry_cluster bind 192.168.10.100:9191 balance source option tcpka option tcplog server controller01 192.168.10.101:9191 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:9191 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:9191 check inter 2000 rise 2 fall 5scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg# [任一节点]生成数据库su -s /bin/sh -c &quot;glance-manage db_sync&quot; glance# [任一节点]添加pacemaker资源pcs resource create openstack-glance-registry systemd:openstack-glance-registry --clone interleave=truepcs resource create openstack-glance-api systemd:openstack-glance-api --clone interleave=true# 下面2条表示先启动openstack-keystone-clone然后启动openstack-glance-registry-clone然后启动openstack-glance-api-clonepcs constraint order start openstack-keystone-clone then openstack-glance-registry-clonepcs constraint order start openstack-glance-registry-clone then openstack-glance-api-clone# api随着registry启动而启动，如果registry启动不了，则api也启动不了pcs constraint colocation add openstack-glance-api-clone with openstack-glance-registry-clone# 在任意节点重启pacemakerbash restart-pcs-cluster.sh# 上传测试镜像openstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --publicopenstack image list 3、安装openstack Compute集群(控制节点)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147# 所有控制节点安装软件包yum install -y openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler# [任一节点]创建数据库mysql -uroot -popenstack -e &quot;CREATE DATABASE nova;GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;nova&quot;&apos;;GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;nova&quot;&apos;;CREATE DATABASE nova_api;GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;nova&quot;&apos;;GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;nova&quot;&apos;;FLUSH PRIVILEGES;&quot;# [任一节点]创建用户等source keystonerc_adminopenstack user create --domain default --password nova novaopenstack role add --project service --user nova adminopenstack service create --name nova --description &quot;OpenStack Compute&quot; computeopenstack endpoint create --region RegionOne compute public http://controller:8774/v2.1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1/%\\(tenant_id\\)s# [所有控制节点]配置配置nova组件，/etc/nova/nova.conf文件openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata# openstack-config --set /etc/nova/nova.conf DEFAULT memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/nova/nova.conf api_database connection mysql+pymysql://nova:nova@controller/nova_apiopenstack-config --set /etc/nova/nova.conf database connection mysql+pymysql://nova:nova@controller/novaopenstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password openstackopenstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_name serviceopenstack-config --set /etc/nova/nova.conf keystone_authtoken username novaopenstack-config --set /etc/nova/nova.conf keystone_authtoken password novaopenstack-config --set /etc/nova/nova.conf DEFAULT my_ip 192.168.10.101openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron Trueopenstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriveropenstack-config --set /etc/nova/nova.conf vnc vncserver_listen 192.168.10.101openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.10.101openstack-config --set /etc/nova/nova.conf vnc novncproxy_host 192.168.10.101openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmpopenstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_listen 192.168.10.101openstack-config --set /etc/nova/nova.conf DEFAULT metadata_listen 192.168.10.101scp /etc/nova/nova.conf controller02:/etc/nova/nova.confscp /etc/nova/nova.conf controller03:/etc/nova/nova.conf# 其他节点修改my_ip,vncserver_listen,vncserver_proxyclient_address,osapi_compute_listen,metadata_listen,vnc novncproxy_hostopenstack-config --set /etc/nova/nova.conf DEFAULT my_ip 192.168.10.102openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 192.168.10.102openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.10.102openstack-config --set /etc/nova/nova.conf vnc novncproxy_host 192.168.10.102openstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_listen 192.168.10.102openstack-config --set /etc/nova/nova.conf DEFAULT metadata_listen 192.168.10.102################################openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 192.168.10.103openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 192.168.10.103openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.10.103openstack-config --set /etc/nova/nova.conf vnc novncproxy_host 192.168.10.103openstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_listen 192.168.10.103openstack-config --set /etc/nova/nova.conf DEFAULT metadata_listen 192.168.10.103################################### 配置haproxyvim /etc/haproxy/haproxy.cfglisten nova_compute_api_cluster bind 192.168.10.100:8774 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:8774 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:8774 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:8774 check inter 2000 rise 2 fall 5listen nova_metadata_api_cluster bind 192.168.10.100:8775 balance source option tcpka option tcplog server controller01 192.168.10.101:8775 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:8775 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:8775 check inter 2000 rise 2 fall 5listen nova_vncproxy_cluster bind 192.168.10.100:6080 balance source option tcpka option tcplog server controller01 192.168.10.101:6080 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:6080 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:6080 check inter 2000 rise 2 fall 5scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg# [任一节点]生成数据库su -s /bin/sh -c &quot;nova-manage api_db sync&quot; novasu -s /bin/sh -c &quot;nova-manage db sync&quot; nova# [任一节点]添加pacemaker资源pcs resource create openstack-nova-consoleauth systemd:openstack-nova-consoleauth --clone interleave=truepcs resource create openstack-nova-novncproxy systemd:openstack-nova-novncproxy --clone interleave=truepcs resource create openstack-nova-api systemd:openstack-nova-api --clone interleave=truepcs resource create openstack-nova-scheduler systemd:openstack-nova-scheduler --clone interleave=truepcs resource create openstack-nova-conductor systemd:openstack-nova-conductor --clone interleave=true# 下面几个order属性表示先启动 openstack-keystone-clone 然后启动openstack-nova-consoleauth-clone# 然后启动openstack-nova-novncproxy-clone，然后启动openstack-nova-api-clone，然后启动openstack-nova-scheduler-clone# 然后启动openstack-nova-conductor-clone# 下面几个colocation属性表示consoleauth约束着novncproxy资源的位置，也就是说consoleauth停止，则novncproxy停止。# 下面的几个colocation属性依次类推pcs constraint order start openstack-keystone-clone then openstack-nova-consoleauth-clonepcs constraint order start openstack-nova-consoleauth-clone then openstack-nova-novncproxy-clonepcs constraint colocation add openstack-nova-novncproxy-clone with openstack-nova-consoleauth-clonepcs constraint order start openstack-nova-novncproxy-clone then openstack-nova-api-clonepcs constraint colocation add openstack-nova-api-clone with openstack-nova-novncproxy-clonepcs constraint order start openstack-nova-api-clone then openstack-nova-scheduler-clonepcs constraint colocation add openstack-nova-scheduler-clone with openstack-nova-api-clonepcs constraint order start openstack-nova-scheduler-clone then openstack-nova-conductor-clonepcs constraint colocation add openstack-nova-conductor-clone with openstack-nova-scheduler-clonebash restart-pcs-cluster.sh### [任一节点]测试source keystonerc_adminopenstack compute service list 4、安装配置neutron集群(控制节点)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232# [任一节点]创建数据库mysql -uroot -popenstack -e &quot;CREATE DATABASE neutron;GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;neutron&quot;&apos;;GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;neutron&quot;&apos;;FLUSH PRIVILEGES;&quot;# [任一节点]创建用户等source /root/keystonerc_adminopenstack user create --domain default --password neutron neutronopenstack role add --project service --user neutron adminopenstack service create --name neutron --description &quot;OpenStack Networking&quot; networkopenstack endpoint create --region RegionOne network public http://controller:9696openstack endpoint create --region RegionOne network internal http://controller:9696openstack endpoint create --region RegionOne network admin http://controller:9696# 所有控制节点yum install -y openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch ebtables# [所有控制节点]配置neutron server，/etc/neutron/neutron.confopenstack-config --set /etc/neutron/neutron.conf DEFAULT bind_host 192.168.10.101openstack-config --set /etc/neutron/neutron.conf database connection mysql+pymysql://neutron:neutron@controller/neutronopenstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins routeropenstack-config --set /etc/neutron/neutron.conf DEFAULT allow_overlapping_ips Trueopenstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_password openstackopenstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name serviceopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutronopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken password neutronopenstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_status_changes Trueopenstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_data_changes Trueopenstack-config --set /etc/neutron/neutron.conf nova auth_url http://controller:35357openstack-config --set /etc/neutron/neutron.conf nova auth_type passwordopenstack-config --set /etc/neutron/neutron.conf nova project_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf nova user_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf nova region_name RegionOneopenstack-config --set /etc/neutron/neutron.conf nova project_name serviceopenstack-config --set /etc/neutron/neutron.conf nova username novaopenstack-config --set /etc/neutron/neutron.conf nova password novaopenstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp# [所有控制节点]配置ML2 plugin，/etc/neutron/plugins/ml2/ml2_conf.iniopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,vlan,vxlan,greopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types vxlanopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers openvswitch,l2populationopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers port_securityopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks externalopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vxlan vni_ranges 1:1000openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vlan network_vlan_ranges external:1:4090openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_security_group Trueopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset Trueopenstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup firewall_driver iptables_hybrid# [所有控制节点]配置Open vSwitch agent，/etc/neutron/plugins/ml2/openvswitch_agent.ini，注意，此处填写第二块网卡openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_security_group Trueopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_ipset Trueopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup firewall_driver iptables_hybridopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs integration_bridge br-intopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs tunnel_bridge br-tunopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs local_ip 10.0.0.1openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs bridge_mappings external:br-exopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent tunnel_types vxlanopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent l2_population True# [所有控制节点]配置L3 agent，/etc/neutron/l3_agent.iniopenstack-config --set /etc/neutron/l3_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriveropenstack-config --set /etc/neutron/l3_agent.ini DEFAULT external_network_bridge# [所有控制节点]配置DHCP agent，/etc/neutron/dhcp_agent.iniopenstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.OVSInterfaceDriveropenstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasqopenstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT enable_isolated_metadata True# [所有控制节点]配置metadata agent，/etc/neutron/metadata_agent.iniopenstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip 192.168.10.100openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret openstack# [所有控制节点]配置nova和neutron集成，/etc/nova/nova.confopenstack-config --set /etc/nova/nova.conf neutron url http://controller:9696openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf neutron auth_type passwordopenstack-config --set /etc/nova/nova.conf neutron project_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron user_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron region_name RegionOneopenstack-config --set /etc/nova/nova.conf neutron project_name serviceopenstack-config --set /etc/nova/nova.conf neutron username neutronopenstack-config --set /etc/nova/nova.conf neutron password neutronopenstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy Trueopenstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret openstack# [所有控制节点]配置L3 agent HA、/etc/neutron/neutron.confopenstack-config --set /etc/neutron/neutron.conf DEFAULT l3_ha Trueopenstack-config --set /etc/neutron/neutron.conf DEFAULT allow_automatic_l3agent_failover Trueopenstack-config --set /etc/neutron/neutron.conf DEFAULT max_l3_agents_per_router 3openstack-config --set /etc/neutron/neutron.conf DEFAULT min_l3_agents_per_router 2# [所有控制节点]配置DHCP agent HA、/etc/neutron/neutron.confopenstack-config --set /etc/neutron/neutron.conf DEFAULT dhcp_agents_per_network 3# [所有控制节点] 配置Open vSwitch (OVS) 服务，创建网桥和端口systemctl enable openvswitch.servicesystemctl start openvswitch.service# [所有控制节点] 创建ML2配置文件软连接ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.inivim /etc/haproxy/haproxy.cfglisten neutron_api_cluster bind 192.168.10.100:9696 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:9696 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:9696 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:9696 check inter 2000 rise 2 fall 5scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg# 备份原来配置文件cp /etc/sysconfig/network-scripts/ifcfg-ens160 /etc/sysconfig/network-scripts/bak-ifcfg-ens160echo &quot;DEVICE=br-exDEVICETYPE=ovsTYPE=OVSBridgeBOOTPROTO=staticIPADDR=$(cat /etc/sysconfig/network-scripts/ifcfg-ens160 |grep IPADDR|awk -F &apos;=&apos; &apos;&#123;print $2&#125;&apos;)NETMASK=$(cat /etc/sysconfig/network-scripts/ifcfg-ens160 |grep NETMASK|awk -F &apos;=&apos; &apos;&#123;print $2&#125;&apos;)GATEWAY=$(cat /etc/sysconfig/network-scripts/ifcfg-ens160 |grep GATEWAY|awk -F &apos;=&apos; &apos;&#123;print $2&#125;&apos;)DNS1=$(cat /etc/sysconfig/network-scripts/ifcfg-ens160 |grep DNS1|awk -F &apos;=&apos; &apos;&#123;print $2&#125;&apos;)DNS2=218.2.2.2ONBOOT=yes&quot;&gt;/etc/sysconfig/network-scripts/ifcfg-br-execho &quot;TYPE=OVSPortDEVICETYPE=ovsOVS_BRIDGE=br-exNAME=ens160DEVICE=ens160ONBOOT=yes&quot;&gt;/etc/sysconfig/network-scripts/ifcfg-ens160ovs-vsctl add-br br-exovs-vsctl add-port br-ex ens160systemctl restart network.service# 拷贝配置文件到其他控制节点并作相应修改scp /etc/neutron/neutron.conf controller02:/etc/neutron/neutron.confscp /etc/neutron/neutron.conf controller03:/etc/neutron/neutron.confscp /etc/neutron/plugins/ml2/ml2_conf.ini controller02:/etc/neutron/plugins/ml2/ml2_conf.iniscp /etc/neutron/plugins/ml2/ml2_conf.ini controller03:/etc/neutron/plugins/ml2/ml2_conf.iniscp /etc/neutron/plugins/ml2/openvswitch_agent.ini controller02:/etc/neutron/plugins/ml2/openvswitch_agent.iniscp /etc/neutron/plugins/ml2/openvswitch_agent.ini controller03:/etc/neutron/plugins/ml2/openvswitch_agent.iniscp /etc/neutron/l3_agent.ini controller02:/etc/neutron/l3_agent.iniscp /etc/neutron/l3_agent.ini controller03:/etc/neutron/l3_agent.iniscp /etc/neutron/dhcp_agent.ini controller02:/etc/neutron/dhcp_agent.iniscp /etc/neutron/dhcp_agent.ini controller03:/etc/neutron/dhcp_agent.iniscp /etc/neutron/metadata_agent.ini controller02:/etc/neutron/metadata_agent.iniscp /etc/neutron/metadata_agent.ini controller03:/etc/neutron/metadata_agent.ini# [任一节点]生成数据库su -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head&quot; neutron# [任一节点]添加pacemaker资源pcs resource create neutron-server systemd:neutron-server op start timeout=90 --clone interleave=truepcs constraint order start openstack-keystone-clone then neutron-server-clone# 全局唯一克隆：参数globally-unique=true。这些资源各不相同。一个节点上运行的克隆实例与另一个节点上运行的实例不同,同一个节点上运行的任何两个实例也不同。# clone-max: 在集群中最多能运行多少份克隆资源，默认和集群中的节点数相同； clone-node-max：每个节点上最多能运行多少份克隆资源，默认是1；pcs resource create neutron-scale ocf:neutron:NeutronScale --clone globally-unique=true clone-max=3 interleave=truepcs constraint order start neutron-server-clone then neutron-scale-clonepcs resource create neutron-ovs-cleanup ocf:neutron:OVSCleanup --clone interleave=truepcs resource create neutron-netns-cleanup ocf:neutron:NetnsCleanup --clone interleave=truepcs resource create neutron-openvswitch-agent systemd:neutron-openvswitch-agent --clone interleave=truepcs resource create neutron-dhcp-agent systemd:neutron-dhcp-agent --clone interleave=truepcs resource create neutron-l3-agent systemd:neutron-l3-agent --clone interleave=truepcs resource create neutron-metadata-agent systemd:neutron-metadata-agent --clone interleave=truepcs constraint order start neutron-scale-clone then neutron-ovs-cleanup-clonepcs constraint colocation add neutron-ovs-cleanup-clone with neutron-scale-clonepcs constraint order start neutron-ovs-cleanup-clone then neutron-netns-cleanup-clonepcs constraint colocation add neutron-netns-cleanup-clone with neutron-ovs-cleanup-clonepcs constraint order start neutron-netns-cleanup-clone then neutron-openvswitch-agent-clonepcs constraint colocation add neutron-openvswitch-agent-clone with neutron-netns-cleanup-clonepcs constraint order start neutron-openvswitch-agent-clone then neutron-dhcp-agent-clonepcs constraint colocation add neutron-dhcp-agent-clone with neutron-openvswitch-agent-clonepcs constraint order start neutron-dhcp-agent-clone then neutron-l3-agent-clonepcs constraint colocation add neutron-l3-agent-clone with neutron-dhcp-agent-clonepcs constraint order start neutron-l3-agent-clone then neutron-metadata-agent-clonepcs constraint colocation add neutron-metadata-agent-clone with neutron-l3-agent-clonebash restart-pcs-cluster.sh# [任一节点] 测试soource keystonerc_adminneutron ext-listneutron agent-listovs-vsctl showneutron agent-list 5、安装配置dashboard集群1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 所有节点安装yum install -y openstack-dashboard# [所有控制节点] 修改配置文件/etc/openstack-dashboard/local_settingssed -i \\ -e &apos;s#OPENSTACK_HOST =.*#OPENSTACK_HOST = &quot;&apos;&quot;192.168.10.101&quot;&apos;&quot;#g&apos; \\ -e &quot;s#ALLOWED_HOSTS.*#ALLOWED_HOSTS = [&apos;*&apos;,]#g&quot; \\ -e &quot;s#^CACHES#SESSION_ENGINE = &apos;django.contrib.sessions.backends.cache&apos;\\nCACHES#g#&quot; \\ -e &quot;s#locmem.LocMemCache&apos;#memcached.MemcachedCache&apos;,\\n &apos;LOCATION&apos; : [ &apos;controller01:11211&apos;, &apos;controller02:11211&apos;, &apos;controller03:11211&apos;, ]#g&quot; \\ -e &apos;s#^OPENSTACK_KEYSTONE_URL =.*#OPENSTACK_KEYSTONE_URL = &quot;http://%s:5000/v3&quot; % OPENSTACK_HOST#g&apos; \\ -e &quot;s/^#OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT.*/OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True/g&quot; \\ -e &apos;s/^#OPENSTACK_API_VERSIONS.*/OPENSTACK_API_VERSIONS = &#123;\\n &quot;identity&quot;: 3,\\n &quot;image&quot;: 2,\\n &quot;volume&quot;: 2,\\n&#125;\\n#OPENSTACK_API_VERSIONS = &#123;/g&apos; \\ -e &quot;s/^#OPENSTACK_KEYSTONE_DEFAULT_DOMAIN.*/OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &apos;default&apos;/g&quot; \\ -e &apos;s#^OPENSTACK_KEYSTONE_DEFAULT_ROLE.*#OPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;user&quot;#g&apos; \\ -e &quot;s#^LOCAL_PATH.*#LOCAL_PATH = &apos;/var/lib/openstack-dashboard&apos;#g&quot; \\ -e &quot;s#^SECRET_KEY.*#SECRET_KEY = &apos;4050e76a15dfb7755fe3&apos;#g&quot; \\ -e &quot;s#&apos;enable_ha_router&apos;.*#&apos;enable_ha_router&apos;: True,#g&quot; \\ -e &apos;s#TIME_ZONE = .*#TIME_ZONE = &quot;&apos;&quot;Asia/Shanghai&quot;&apos;&quot;#g&apos; \\ /etc/openstack-dashboard/local_settingsscp /etc/openstack-dashboard/local_settings controller02:/etc/openstack-dashboard/local_settingsscp /etc/openstack-dashboard/local_settings controller03:/etc/openstack-dashboard/local_settings# 在controller02上修改sed -i -e &apos;s#OPENSTACK_HOST =.*#OPENSTACK_HOST = &quot;&apos;&quot;192.168.10.102&quot;&apos;&quot;#g&apos; /etc/openstack-dashboard/local_settings# 在controiller03上修改sed -i -e &apos;s#OPENSTACK_HOST =.*#OPENSTACK_HOST = &quot;&apos;&quot;192.168.10.103&quot;&apos;&quot;#g&apos; /etc/openstack-dashboard/local_settings# [所有控制节点]echo &quot;COMPRESS_OFFLINE = True&quot; &gt;&gt; /etc/openstack-dashboard/local_settingspython /usr/share/openstack-dashboard/manage.py compress# [所有控制节点] 设置HTTPD在特定的IP上监听sed -i -e &apos;s/^Listen.*/Listen &apos;&quot;$(ip addr show dev br-ex scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;|head -n 1)&quot;&apos;:80/g&apos; /etc/httpd/conf/httpd.confvim /etc/haproxy/haproxy.cfglisten dashboard_cluster bind 192.168.10.100:80 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:80 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:80 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:80 check inter 2000 rise 2 fall 5scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg 6、安装配置cinder123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119# 所有控制节点yum install -y openstack-cinder# [任一节点]创建数据库mysql -uroot -popenstack -e &quot;CREATE DATABASE cinder;GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&quot;cinder&quot;&apos;;GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;%&apos; IDENTIFIED BY &apos;&quot;cinder&quot;&apos;;FLUSH PRIVILEGES;&quot;# [任一节点]创建用户等. /root/keystonerc_admin# [任一节点]创建用户等openstack user create --domain default --password cinder cinderopenstack role add --project service --user cinder adminopenstack service create --name cinder --description &quot;OpenStack Block Storage&quot; volumeopenstack service create --name cinderv2 --description &quot;OpenStack Block Storage&quot; volumev2#创建cinder服务的API endpointsopenstack endpoint create --region RegionOne volume public http://controller:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne volume internal http://controller:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne volume admin http://controller:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne volumev2 public http://controller:8776/v2/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne volumev2 internal http://controller:8776/v2/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne volumev2 admin http://controller:8776/v2/%\\(tenant_id\\)s#[所有控制节点] 修改/etc/cinder/cinder.confopenstack-config --set /etc/cinder/cinder.conf database connection mysql+pymysql://cinder:cinder@controller/cinderopenstack-config --set /etc/cinder/cinder.conf database max_retries -1openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/cinder/cinder.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_name serviceopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken username cinderopenstack-config --set /etc/cinder/cinder.conf keystone_authtoken password cinderopenstack-config --set /etc/cinder/cinder.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_password openstackopenstack-config --set /etc/cinder/cinder.conf DEFAULT control_exchange cinderopenstack-config --set /etc/cinder/cinder.conf DEFAULT osapi_volume_listen $(ip addr show dev br-ex scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;|head -n 1)openstack-config --set /etc/cinder/cinder.conf DEFAULT my_ip $(ip addr show dev br-ex scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;|head -n 1)openstack-config --set /etc/cinder/cinder.conf oslo_concurrency lock_path /var/lib/cinder/tmpopenstack-config --set /etc/cinder/cinder.conf DEFAULT glance_api_servers http://controller:9292# [任一节点]生成数据库su -s /bin/sh -c &quot;cinder-manage db sync&quot; cinder# 所有控制节点修改计算节点配置openstack-config --set /etc/nova/nova.conf cinder os_region_name RegionOne# 重启计算节点 nova-api# pcs resource restart openstack-nova-api-clone# 安装配置存储节点 ，存储节点和控制节点复用# 所有节点yum install lvm2 -ysystemctl enable lvm2-lvmetad.servicesystemctl start lvm2-lvmetad.servicepvcreate /dev/sdbvgcreate cinder-volumes /dev/sdbyum install openstack-cinder targetcli python-keystone -y# 所有控制节点修改部分配置文件openstack-config --set /etc/cinder/cinder.conf lvm volume_driver cinder.volume.drivers.lvm.LVMVolumeDriveropenstack-config --set /etc/cinder/cinder.conf lvm volume_group cinder-volumesopenstack-config --set /etc/cinder/cinder.conf lvm iscsi_protocol iscsiopenstack-config --set /etc/cinder/cinder.conf lvm iscsi_helper lioadmopenstack-config --set /etc/cinder/cinder.conf DEFAULT enabled_backends lvm# 增加haproxy.cfg配置文件vim /etc/haproxy/haproxy.cfglisten cinder_api_cluster bind 192.168.10.100:8776 balance source option tcpka option httpchk option tcplog server controller01 192.168.10.101:8776 check inter 2000 rise 2 fall 5 server controller02 192.168.10.102:8776 check inter 2000 rise 2 fall 5 server controller03 192.168.10.103:8776 check inter 2000 rise 2 fall 5scp /etc/haproxy/haproxy.cfg controller02:/etc/haproxy/haproxy.cfgscp /etc/haproxy/haproxy.cfg controller03:/etc/haproxy/haproxy.cfg# [任一节点]添加pacemaker资源pcs resource create openstack-cinder-api systemd:openstack-cinder-api --clone interleave=truepcs resource create openstack-cinder-scheduler systemd:openstack-cinder-scheduler --clone interleave=truepcs resource create openstack-cinder-volume systemd:openstack-cinder-volumepcs constraint order start openstack-keystone-clone then openstack-cinder-api-clonepcs constraint order start openstack-cinder-api-clone then openstack-cinder-scheduler-clonepcs constraint colocation add openstack-cinder-scheduler-clone with openstack-cinder-api-clonepcs constraint order start openstack-cinder-scheduler-clone then openstack-cinder-volumepcs constraint colocation add openstack-cinder-volume with openstack-cinder-scheduler-clone# 重启集群bash restart-pcs-cluster.sh# [任一节点]测试. /root/keystonerc_admincinder service-list 7、安装配置ceilometer和aodh集群7.1 安装配置ceilometer集群实在无力吐槽这个项目，所以不想写了 7.2 安装配置aodh集群实在无力吐槽这个项目，所以不想写了 四、安装配置计算节点4.1 OpenStack Compute service123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 所有计算节点yum install -y openstack-nova-compute# 修改配置文件/etc/nova/nova.confopenstack-config --set /etc/nova/nova.conf DEFAULT my_ip $(ip addr show dev ens160 scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;)openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron Trueopenstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriveropenstack-config --set /etc/nova/nova.conf DEFAULT memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password openstackopenstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/nova/nova.conf keystone_authtoken project_name serviceopenstack-config --set /etc/nova/nova.conf keystone_authtoken username novaopenstack-config --set /etc/nova/nova.conf keystone_authtoken password novaopenstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmpopenstack-config --set /etc/nova/nova.conf vnc enabled Trueopenstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address $(ip addr show dev ens160 scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;)openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://192.168.10.100:6080/vnc_auto.htmlopenstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292openstack-config --set /etc/nova/nova.conf libvirt virt_type $(count=$(egrep -c &apos;(vmx|svm)&apos; /proc/cpuinfo); if [ $count -eq 0 ];then echo &quot;qemu&quot;; else echo &quot;kvm&quot;; fi)# 打开虚拟机迁移的监听端口sed -i -e &quot;s#\\#listen_tls *= *0#listen_tls = 0#g&quot; /etc/libvirt/libvirtd.confsed -i -e &quot;s#\\#listen_tcp *= *1#listen_tcp = 1#g&quot; /etc/libvirt/libvirtd.confsed -i -e &quot;s#\\#auth_tcp *= *\\&quot;sasl\\&quot;#auth_tcp = \\&quot;none\\&quot;#g&quot; /etc/libvirt/libvirtd.confsed -i -e &quot;s#\\#LIBVIRTD_ARGS *= *\\&quot;--listen\\&quot;#LIBVIRTD_ARGS=\\&quot;--listen\\&quot;#g&quot; /etc/sysconfig/libvirtd#启动服务systemctl enable libvirtd.service openstack-nova-compute.servicesystemctl start libvirtd.service openstack-nova-compute.service 4.2 OpenStack Network service1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 安装组件yum install -y openstack-neutron-openvswitch ebtables ipsetyum install -y openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch# 修改/etc/neutron/neutron.confopenstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbitopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_hosts controller01:5672,controller02:5672,controller03:5672openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_ha_queues trueopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_retry_interval 1openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_retry_backoff 2openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_max_retries 0openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_durable_queues trueopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_userid openstackopenstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_password openstackopenstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystoneopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://controller:5000openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://controller:35357openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers controller01:11211,controller02:11211,controller03:11211openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type passwordopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name defaultopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name serviceopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutronopenstack-config --set /etc/neutron/neutron.conf keystone_authtoken password neutronopenstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp### 配置Open vSwitch agent，/etc/neutron/plugins/ml2/openvswitch_agent.ini，注意，此处填写第二块网卡openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_security_group Trueopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup enable_ipset Trueopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup firewall_driver iptables_hybridopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs local_ip $(ip addr show dev ens192 scope global | grep &quot;inet &quot; | sed -e &apos;s#.*inet ##g&apos; -e &apos;s#/.*##g&apos;)openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent tunnel_types vxlanopenstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent l2_population False### 配置nova和neutron集成，/etc/nova/nova.confopenstack-config --set /etc/nova/nova.conf neutron url http://controller:9696openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357openstack-config --set /etc/nova/nova.conf neutron auth_type passwordopenstack-config --set /etc/nova/nova.conf neutron project_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron user_domain_name defaultopenstack-config --set /etc/nova/nova.conf neutron region_name RegionOneopenstack-config --set /etc/nova/nova.conf neutron project_name serviceopenstack-config --set /etc/nova/nova.conf neutron username neutronopenstack-config --set /etc/nova/nova.conf neutron password neutronln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.inisystemctl restart openstack-nova-compute.servicesystemctl start openvswitch.servicesystemctl restart neutron-openvswitch-agent.servicesystemctl enable openvswitch.servicesystemctl enable neutron-openvswitch-agent.service 修补控制节点： 123456GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;controller01&apos; IDENTIFIED BY &quot;openstack&quot;;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;controller02&apos; IDENTIFIED BY &quot;openstack&quot;;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;controller03&apos; IDENTIFIED BY &quot;openstack&quot;;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;192.168.10.101&apos; IDENTIFIED BY &quot;openstack&quot;;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;192.168.10.102&apos; IDENTIFIED BY &quot;openstack&quot;;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;192.168.10.103&apos; IDENTIFIED BY &quot;openstack&quot;; rabbitmq集群相关： 12/sbin/service rabbitmq-server stop/sbin/service rabbitmq-server start 12345# 设置资源超时时间pcs resource op defaults timeout=90s# 清除错误pcs resource cleanup openstack-keystone-clone mariadb集群排错1234报错描述如下：节点启动不了，查看 tailf /var/log/messages日志发现如下报错：[ERROR] WSREP: gcs/src/gcs_group.cpp:group_post_state_exchange():321解决错误: rm -f /var/lib/mysql/grastate.dat然后重启服务即可","tags":[{"name":"Openstack","slug":"Openstack","permalink":"http://yoursite.com/tags/Openstack/"}]},{"title":"packstack 安装配置openstack mitaka","date":"2017-03-30T01:20:00.000Z","path":"2017/03/30/packstack install openstack-mitaka/","text":"1、修改主机名 12345678hostnamectl set-hostname controllerhostname controllerhostnamectl set-hostname compute01hostname compute01hostnamectl set-hostname compute02hostname compute02 1234567cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.12.50 controller192.168.12.51 compute01192.168.12.52 compute02 2、安装基础yum源 123456yum install ntpdate -yecho &quot;*/5 * * * * /usr/sbin/ntpdate 192.168.2.161 &gt;/dev/null 2&gt;&amp;1&quot; &gt;&gt; /var/spool/cron/root/usr/sbin/ntpdate 192.168.2.161yum install wget -yrm -rf /etc/yum.repos.d/*wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 3、安装配置openstack源 123456789yum install -y centos-release-openstack-mitakavim CentOS-OpenStack-mitaka.repo [centos-openstack-mitaka]name=CentOS-7 - OpenStack mitakabaseurl=http://mirrors.aliyun.com/centos/7/cloud/$basearch/openstack-mitaka/gpgcheck=1enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-Cloud 4、安装配置openstack 1yum update -y 控制节点执行: CONFIG_PROVISION_DEMO=n 123yum install -y openstack-packstackpackstack --gen-answer-file=openstack.txt 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317[root@controller ~]# cat openstack.txt | egrep -v &quot;^$|^#&quot;[general]CONFIG_SSH_KEY=/root/.ssh/id_rsa.pubCONFIG_DEFAULT_PASSWORD=CONFIG_SERVICE_WORKERS=%&#123;::processorcount&#125;CONFIG_MARIADB_INSTALL=yCONFIG_GLANCE_INSTALL=yCONFIG_CINDER_INSTALL=yCONFIG_MANILA_INSTALL=nCONFIG_NOVA_INSTALL=yCONFIG_NEUTRON_INSTALL=yCONFIG_HORIZON_INSTALL=yCONFIG_SWIFT_INSTALL=yCONFIG_CEILOMETER_INSTALL=yCONFIG_AODH_INSTALL=yCONFIG_GNOCCHI_INSTALL=yCONFIG_SAHARA_INSTALL=nCONFIG_HEAT_INSTALL=nCONFIG_TROVE_INSTALL=nCONFIG_IRONIC_INSTALL=nCONFIG_CLIENT_INSTALL=yCONFIG_NTP_SERVERS=CONFIG_NAGIOS_INSTALL=yEXCLUDE_SERVERS=CONFIG_DEBUG_MODE=nCONFIG_CONTROLLER_HOST=192.168.12.50CONFIG_COMPUTE_HOSTS=192.168.12.51,192.168.12.52CONFIG_NETWORK_HOSTS=192.168.12.50CONFIG_VMWARE_BACKEND=nCONFIG_UNSUPPORTED=nCONFIG_USE_SUBNETS=nCONFIG_VCENTER_HOST=CONFIG_VCENTER_USER=CONFIG_VCENTER_PASSWORD=CONFIG_VCENTER_CLUSTER_NAMES=CONFIG_STORAGE_HOST=192.168.12.50CONFIG_SAHARA_HOST=192.168.12.50CONFIG_USE_EPEL=nCONFIG_REPO=CONFIG_ENABLE_RDO_TESTING=nCONFIG_RH_USER=CONFIG_SATELLITE_URL=CONFIG_RH_SAT6_SERVER=CONFIG_RH_PW=CONFIG_RH_OPTIONAL=yCONFIG_RH_PROXY=CONFIG_RH_SAT6_ORG=CONFIG_RH_SAT6_KEY=CONFIG_RH_PROXY_PORT=CONFIG_RH_PROXY_USER=CONFIG_RH_PROXY_PW=CONFIG_SATELLITE_USER=CONFIG_SATELLITE_PW=CONFIG_SATELLITE_AKEY=CONFIG_SATELLITE_CACERT=CONFIG_SATELLITE_PROFILE=CONFIG_SATELLITE_FLAGS=CONFIG_SATELLITE_PROXY=CONFIG_SATELLITE_PROXY_USER=CONFIG_SATELLITE_PROXY_PW=CONFIG_SSL_CACERT_FILE=/etc/pki/tls/certs/selfcert.crtCONFIG_SSL_CACERT_KEY_FILE=/etc/pki/tls/private/selfkey.keyCONFIG_SSL_CERT_DIR=~/packstackca/CONFIG_SSL_CACERT_SELFSIGN=yCONFIG_SELFSIGN_CACERT_SUBJECT_C=--CONFIG_SELFSIGN_CACERT_SUBJECT_ST=StateCONFIG_SELFSIGN_CACERT_SUBJECT_L=CityCONFIG_SELFSIGN_CACERT_SUBJECT_O=openstackCONFIG_SELFSIGN_CACERT_SUBJECT_OU=packstackCONFIG_SELFSIGN_CACERT_SUBJECT_CN=controllerCONFIG_SELFSIGN_CACERT_SUBJECT_MAIL=admin@controllerCONFIG_AMQP_BACKEND=rabbitmqCONFIG_AMQP_HOST=192.168.12.50CONFIG_AMQP_ENABLE_SSL=nCONFIG_AMQP_ENABLE_AUTH=nCONFIG_AMQP_NSS_CERTDB_PW=PW_PLACEHOLDERCONFIG_AMQP_AUTH_USER=amqp_userCONFIG_AMQP_AUTH_PASSWORD=PW_PLACEHOLDERCONFIG_MARIADB_HOST=192.168.12.50CONFIG_MARIADB_USER=rootCONFIG_MARIADB_PW=openstackCONFIG_KEYSTONE_DB_PW=38cddfa9ce6149c9CONFIG_KEYSTONE_DB_PURGE_ENABLE=TrueCONFIG_KEYSTONE_REGION=RegionOneCONFIG_KEYSTONE_ADMIN_TOKEN=e462f66d03974d8fa410cf76aae40da9CONFIG_KEYSTONE_ADMIN_EMAIL=root@localhostCONFIG_KEYSTONE_ADMIN_USERNAME=adminCONFIG_KEYSTONE_ADMIN_PW=adminCONFIG_KEYSTONE_DEMO_PW=demoCONFIG_KEYSTONE_API_VERSION=v2.0CONFIG_KEYSTONE_TOKEN_FORMAT=UUIDCONFIG_KEYSTONE_SERVICE_NAME=httpdCONFIG_KEYSTONE_IDENTITY_BACKEND=sqlCONFIG_KEYSTONE_LDAP_URL=ldap://192.168.12.50CONFIG_KEYSTONE_LDAP_USER_DN=CONFIG_KEYSTONE_LDAP_USER_PASSWORD=CONFIG_KEYSTONE_LDAP_SUFFIX=CONFIG_KEYSTONE_LDAP_QUERY_SCOPE=oneCONFIG_KEYSTONE_LDAP_PAGE_SIZE=-1CONFIG_KEYSTONE_LDAP_USER_SUBTREE=CONFIG_KEYSTONE_LDAP_USER_FILTER=CONFIG_KEYSTONE_LDAP_USER_OBJECTCLASS=CONFIG_KEYSTONE_LDAP_USER_ID_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_NAME_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_MAIL_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_ENABLED_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_ENABLED_MASK=-1CONFIG_KEYSTONE_LDAP_USER_ENABLED_DEFAULT=TRUECONFIG_KEYSTONE_LDAP_USER_ENABLED_INVERT=nCONFIG_KEYSTONE_LDAP_USER_ATTRIBUTE_IGNORE=CONFIG_KEYSTONE_LDAP_USER_DEFAULT_PROJECT_ID_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_ALLOW_CREATE=nCONFIG_KEYSTONE_LDAP_USER_ALLOW_UPDATE=nCONFIG_KEYSTONE_LDAP_USER_ALLOW_DELETE=nCONFIG_KEYSTONE_LDAP_USER_PASS_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_USER_ENABLED_EMULATION_DN=CONFIG_KEYSTONE_LDAP_USER_ADDITIONAL_ATTRIBUTE_MAPPING=CONFIG_KEYSTONE_LDAP_GROUP_SUBTREE=CONFIG_KEYSTONE_LDAP_GROUP_FILTER=CONFIG_KEYSTONE_LDAP_GROUP_OBJECTCLASS=CONFIG_KEYSTONE_LDAP_GROUP_ID_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_GROUP_NAME_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_GROUP_MEMBER_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_GROUP_DESC_ATTRIBUTE=CONFIG_KEYSTONE_LDAP_GROUP_ATTRIBUTE_IGNORE=CONFIG_KEYSTONE_LDAP_GROUP_ALLOW_CREATE=nCONFIG_KEYSTONE_LDAP_GROUP_ALLOW_UPDATE=nCONFIG_KEYSTONE_LDAP_GROUP_ALLOW_DELETE=nCONFIG_KEYSTONE_LDAP_GROUP_ADDITIONAL_ATTRIBUTE_MAPPING=CONFIG_KEYSTONE_LDAP_USE_TLS=nCONFIG_KEYSTONE_LDAP_TLS_CACERTDIR=CONFIG_KEYSTONE_LDAP_TLS_CACERTFILE=CONFIG_KEYSTONE_LDAP_TLS_REQ_CERT=demandCONFIG_GLANCE_DB_PW=b5b27691c2a14788CONFIG_GLANCE_KS_PW=3ed7ad2ece9146a9CONFIG_GLANCE_BACKEND=fileCONFIG_CINDER_DB_PW=dadf81afd3be45ccCONFIG_CINDER_DB_PURGE_ENABLE=TrueCONFIG_CINDER_KS_PW=68c0f33d48664240CONFIG_CINDER_BACKEND=lvmCONFIG_CINDER_VOLUMES_CREATE=yCONFIG_CINDER_VOLUMES_SIZE=20GCONFIG_CINDER_GLUSTER_MOUNTS=CONFIG_CINDER_NFS_MOUNTS=CONFIG_CINDER_NETAPP_LOGIN=CONFIG_CINDER_NETAPP_PASSWORD=CONFIG_CINDER_NETAPP_HOSTNAME=CONFIG_CINDER_NETAPP_SERVER_PORT=80CONFIG_CINDER_NETAPP_STORAGE_FAMILY=ontap_clusterCONFIG_CINDER_NETAPP_TRANSPORT_TYPE=httpCONFIG_CINDER_NETAPP_STORAGE_PROTOCOL=nfsCONFIG_CINDER_NETAPP_SIZE_MULTIPLIER=1.0CONFIG_CINDER_NETAPP_EXPIRY_THRES_MINUTES=720CONFIG_CINDER_NETAPP_THRES_AVL_SIZE_PERC_START=20CONFIG_CINDER_NETAPP_THRES_AVL_SIZE_PERC_STOP=60CONFIG_CINDER_NETAPP_NFS_SHARES=CONFIG_CINDER_NETAPP_NFS_SHARES_CONFIG=/etc/cinder/shares.confCONFIG_CINDER_NETAPP_VOLUME_LIST=CONFIG_CINDER_NETAPP_VFILER=CONFIG_CINDER_NETAPP_PARTNER_BACKEND_NAME=CONFIG_CINDER_NETAPP_VSERVER=CONFIG_CINDER_NETAPP_CONTROLLER_IPS=CONFIG_CINDER_NETAPP_SA_PASSWORD=CONFIG_CINDER_NETAPP_ESERIES_HOST_TYPE=linux_dm_mpCONFIG_CINDER_NETAPP_WEBSERVICE_PATH=/devmgr/v2CONFIG_CINDER_NETAPP_STORAGE_POOLS=CONFIG_IRONIC_DB_PW=PW_PLACEHOLDERCONFIG_IRONIC_KS_PW=PW_PLACEHOLDERCONFIG_NOVA_DB_PURGE_ENABLE=TrueCONFIG_NOVA_DB_PW=588239b4fe244aebCONFIG_NOVA_KS_PW=3f355b76a92d41f1CONFIG_NOVA_SCHED_CPU_ALLOC_RATIO=16.0CONFIG_NOVA_SCHED_RAM_ALLOC_RATIO=1.5CONFIG_NOVA_COMPUTE_MIGRATE_PROTOCOL=tcpCONFIG_NOVA_COMPUTE_MANAGER=nova.compute.manager.ComputeManagerCONFIG_VNC_SSL_CERT=CONFIG_VNC_SSL_KEY=CONFIG_NOVA_PCI_ALIAS=CONFIG_NOVA_PCI_PASSTHROUGH_WHITELIST=CONFIG_NOVA_LIBVIRT_VIRT_TYPE=%&#123;::default_hypervisor&#125;CONFIG_NOVA_COMPUTE_PRIVIF=CONFIG_NOVA_NETWORK_MANAGER=nova.network.manager.FlatDHCPManagerCONFIG_NOVA_NETWORK_PUBIF=eth0CONFIG_NOVA_NETWORK_PRIVIF=CONFIG_NOVA_NETWORK_FIXEDRANGE=192.168.32.0/22CONFIG_NOVA_NETWORK_FLOATRANGE=10.3.4.0/22CONFIG_NOVA_NETWORK_AUTOASSIGNFLOATINGIP=nCONFIG_NOVA_NETWORK_VLAN_START=100CONFIG_NOVA_NETWORK_NUMBER=1CONFIG_NOVA_NETWORK_SIZE=255CONFIG_NEUTRON_KS_PW=e6c0896ef2fa412cCONFIG_NEUTRON_DB_PW=1202d0c54fdb4f5dCONFIG_NEUTRON_L3_EXT_BRIDGE=br-exCONFIG_NEUTRON_METADATA_PW=fd7e182acbc842e3CONFIG_LBAAS_INSTALL=nCONFIG_NEUTRON_METERING_AGENT_INSTALL=yCONFIG_NEUTRON_FWAAS=nCONFIG_NEUTRON_VPNAAS=nCONFIG_NEUTRON_ML2_TYPE_DRIVERS=vxlanCONFIG_NEUTRON_ML2_TENANT_NETWORK_TYPES=vxlanCONFIG_NEUTRON_ML2_MECHANISM_DRIVERS=openvswitchCONFIG_NEUTRON_ML2_FLAT_NETWORKS=*CONFIG_NEUTRON_ML2_VLAN_RANGES=CONFIG_NEUTRON_ML2_TUNNEL_ID_RANGES=CONFIG_NEUTRON_ML2_VXLAN_GROUP=CONFIG_NEUTRON_ML2_VNI_RANGES=10:100CONFIG_NEUTRON_L2_AGENT=openvswitchCONFIG_NEUTRON_ML2_SUPPORTED_PCI_VENDOR_DEVS=[&apos;15b3:1004&apos;, &apos;8086:10ca&apos;]CONFIG_NEUTRON_ML2_SRIOV_AGENT_REQUIRED=nCONFIG_NEUTRON_ML2_SRIOV_INTERFACE_MAPPINGS=CONFIG_NEUTRON_LB_INTERFACE_MAPPINGS=CONFIG_NEUTRON_OVS_BRIDGE_MAPPINGS=CONFIG_NEUTRON_OVS_BRIDGE_IFACES=CONFIG_NEUTRON_OVS_BRIDGES_COMPUTE=CONFIG_NEUTRON_OVS_TUNNEL_IF=CONFIG_NEUTRON_OVS_TUNNEL_SUBNETS=CONFIG_NEUTRON_OVS_VXLAN_UDP_PORT=4789CONFIG_MANILA_DB_PW=PW_PLACEHOLDERCONFIG_MANILA_KS_PW=PW_PLACEHOLDERCONFIG_MANILA_BACKEND=genericCONFIG_MANILA_NETAPP_DRV_HANDLES_SHARE_SERVERS=falseCONFIG_MANILA_NETAPP_TRANSPORT_TYPE=httpsCONFIG_MANILA_NETAPP_LOGIN=adminCONFIG_MANILA_NETAPP_PASSWORD=CONFIG_MANILA_NETAPP_SERVER_HOSTNAME=CONFIG_MANILA_NETAPP_STORAGE_FAMILY=ontap_clusterCONFIG_MANILA_NETAPP_SERVER_PORT=443CONFIG_MANILA_NETAPP_AGGREGATE_NAME_SEARCH_PATTERN=(.*)CONFIG_MANILA_NETAPP_ROOT_VOLUME_AGGREGATE=CONFIG_MANILA_NETAPP_ROOT_VOLUME_NAME=rootCONFIG_MANILA_NETAPP_VSERVER=CONFIG_MANILA_GENERIC_DRV_HANDLES_SHARE_SERVERS=trueCONFIG_MANILA_GENERIC_VOLUME_NAME_TEMPLATE=manila-share-%sCONFIG_MANILA_GENERIC_SHARE_MOUNT_PATH=/sharesCONFIG_MANILA_SERVICE_IMAGE_LOCATION=https://www.dropbox.com/s/vi5oeh10q1qkckh/ubuntu_1204_nfs_cifs.qcow2CONFIG_MANILA_SERVICE_INSTANCE_USER=ubuntuCONFIG_MANILA_SERVICE_INSTANCE_PASSWORD=ubuntuCONFIG_MANILA_NETWORK_TYPE=neutronCONFIG_MANILA_NETWORK_STANDALONE_GATEWAY=CONFIG_MANILA_NETWORK_STANDALONE_NETMASK=CONFIG_MANILA_NETWORK_STANDALONE_SEG_ID=CONFIG_MANILA_NETWORK_STANDALONE_IP_RANGE=CONFIG_MANILA_NETWORK_STANDALONE_IP_VERSION=4CONFIG_MANILA_GLUSTERFS_SERVERS=CONFIG_MANILA_GLUSTERFS_NATIVE_PATH_TO_PRIVATE_KEY=CONFIG_MANILA_GLUSTERFS_VOLUME_PATTERN=CONFIG_MANILA_GLUSTERFS_TARGET=CONFIG_MANILA_GLUSTERFS_MOUNT_POINT_BASE=CONFIG_MANILA_GLUSTERFS_NFS_SERVER_TYPE=glusterCONFIG_MANILA_GLUSTERFS_PATH_TO_PRIVATE_KEY=CONFIG_MANILA_GLUSTERFS_GANESHA_SERVER_IP=CONFIG_HORIZON_SSL=nCONFIG_HORIZON_SECRET_KEY=82c2cbbf5e744a4785e2d0ddd3ec9b80CONFIG_HORIZON_SSL_CERT=CONFIG_HORIZON_SSL_KEY=CONFIG_HORIZON_SSL_CACERT=CONFIG_SWIFT_KS_PW=d7b6a1ec2c8f4334CONFIG_SWIFT_STORAGES=CONFIG_SWIFT_STORAGE_ZONES=1CONFIG_SWIFT_STORAGE_REPLICAS=1CONFIG_SWIFT_STORAGE_FSTYPE=ext4CONFIG_SWIFT_HASH=67485c805b524830CONFIG_SWIFT_STORAGE_SIZE=2GCONFIG_HEAT_DB_PW=PW_PLACEHOLDERCONFIG_HEAT_AUTH_ENC_KEY=ac8680aafaca4fa9CONFIG_HEAT_KS_PW=PW_PLACEHOLDERCONFIG_HEAT_CLOUDWATCH_INSTALL=nCONFIG_HEAT_CFN_INSTALL=nCONFIG_HEAT_DOMAIN=heatCONFIG_HEAT_DOMAIN_ADMIN=heat_adminCONFIG_HEAT_DOMAIN_PASSWORD=PW_PLACEHOLDERCONFIG_PROVISION_DEMO=nCONFIG_PROVISION_TEMPEST=nCONFIG_PROVISION_DEMO_FLOATRANGE=172.24.4.224/28CONFIG_PROVISION_IMAGE_NAME=cirrosCONFIG_PROVISION_IMAGE_URL=http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgCONFIG_PROVISION_IMAGE_FORMAT=qcow2CONFIG_PROVISION_IMAGE_SSH_USER=cirrosCONFIG_PROVISION_UEC_IMAGE_NAME=cirros-uecCONFIG_PROVISION_UEC_IMAGE_KERNEL_URL=http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-kernelCONFIG_PROVISION_UEC_IMAGE_RAMDISK_URL=http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-initramfsCONFIG_PROVISION_UEC_IMAGE_DISK_URL=http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgCONFIG_TEMPEST_HOST=CONFIG_PROVISION_TEMPEST_USER=CONFIG_PROVISION_TEMPEST_USER_PW=PW_PLACEHOLDERCONFIG_PROVISION_TEMPEST_FLOATRANGE=172.24.4.224/28CONFIG_PROVISION_TEMPEST_REPO_URI=https://github.com/openstack/tempest.gitCONFIG_PROVISION_TEMPEST_REPO_REVISION=masterCONFIG_RUN_TEMPEST=nCONFIG_RUN_TEMPEST_TESTS=smokeCONFIG_PROVISION_OVS_BRIDGE=yCONFIG_GNOCCHI_DB_PW=9e46b3492cd441aeCONFIG_GNOCCHI_KS_PW=553369ab22294506CONFIG_CEILOMETER_SECRET=7d41192be1264a47CONFIG_CEILOMETER_KS_PW=2b56ad68aab341a0CONFIG_CEILOMETER_SERVICE_NAME=httpdCONFIG_CEILOMETER_COORDINATION_BACKEND=redisCONFIG_CEILOMETER_METERING_BACKEND=databaseCONFIG_MONGODB_HOST=192.168.12.50CONFIG_REDIS_MASTER_HOST=192.168.12.50CONFIG_REDIS_PORT=6379CONFIG_REDIS_HA=nCONFIG_REDIS_SLAVE_HOSTS=CONFIG_REDIS_SENTINEL_HOSTS=CONFIG_REDIS_SENTINEL_CONTACT_HOST=CONFIG_REDIS_SENTINEL_PORT=26379CONFIG_REDIS_SENTINEL_QUORUM=2CONFIG_REDIS_MASTER_NAME=mymasterCONFIG_AODH_KS_PW=d6881f5b3b0e4ce2CONFIG_TROVE_DB_PW=PW_PLACEHOLDERCONFIG_TROVE_KS_PW=PW_PLACEHOLDERCONFIG_TROVE_NOVA_USER=troveCONFIG_TROVE_NOVA_TENANT=servicesCONFIG_TROVE_NOVA_PW=PW_PLACEHOLDERCONFIG_SAHARA_DB_PW=PW_PLACEHOLDERCONFIG_SAHARA_KS_PW=PW_PLACEHOLDERCONFIG_NAGIOS_PW=383e86a1cd4348b5 5、安装 1packstack --answer-file=openstack.txt 6、安装后的修改 1234vim /etc/neutron/plugins/ml2/ml2_conf.ini 修改控制节点网络配置:[ml2_type_flat]flat_networks = * 1234567891011121314151617修改控制节点网卡配置:[root@controller network-scripts(keystone_admin)]# cat ifcfg-ens160 DEVICE=ens160TYPE=OVSPortDEVICETYPE=ovsOVS_BRIDGE=br-exONBOOT=yes[root@controller network-scripts(keystone_admin)]# cat ifcfg-br-ex DEVICE=br-exONBOOT=yesDEVICETYPE=ovsTYPE=OVSBridgeBOOTPROTO=staticIPADDR=192.168.12.50NETMASK=255.255.255.0GATEWAY=192.168.12.1DNS1=218.2.2.2 #####################为2个计算节点挂载共享存储，本次是nfs 12345678910111213141516171819202122232425262728293031323334353637383940找一个硬盘分区后，此次是sdc1挂载到/opt/nova/instances上[root@cinder ~]# cat /etc/fstab /dev/mapper/centos-root / xfs defaults 0 0UUID=06e34a87-b0fc-40e5-aae2-fa4d136543fc /boot xfs defaults 0 0/dev/mapper/centos-swap swap swap defaults 0 0/dev/sdc1 /opt/nova/instances xfs defaults 0 0格式化/dev/sdc1:mkfs.xfs /dev/sdb1修改过/etc/fstab后mount -a 即可生效yum -y install nfs-utils rpcbind -yvim /etc/exports/opt/nova/instances 192.168.12.0/24(rw,no_root_squash,no_all_squash,sync)exportfs -rsystemctl enable rpcbind.servicesystemctl start rpcbind.servicesystemctl enable nfs-server.service systemctl start nfs-server.service 2、2个nova节点查看showmount -e 192.168.12.50mount -t nfs 192.168.12.50:/opt/nova/instances /var/lib/nova/instances/mount -a2个nova节点：chown -R nova.nova /var/lib/nova","tags":[{"name":"Openstack","slug":"Openstack","permalink":"http://yoursite.com/tags/Openstack/"}]},{"title":"python代码调试工具","date":"2017-03-29T15:37:00.000Z","path":"2017/03/29/python 调试工具/","text":"以前都是用print或者log来调试程序,在小规模的程序下很方便，但是遇到向openstack这种项目级别的程序就太尴尬了。于是找到了ipdb 1、安装 1pip install ipdb 2、使用方法有两种使用方法,一种是不用改变程序直接用ipdb单步执行Python程序,第二种是在程序里标记断点,进行调试. 1) 第一种方法 1python -m ipdb xxx.py 2) 第二种方法 在需要断点的地方插入 12from ipdb import set_traceset_trace() 3、常用命令 n(下一个) ENTER(重复上次命令) q(退出) p&lt;变量&gt;(打印变量) c(继续) l(查找当前位于哪里) s(进入子程序) r(运行直到子程序结束) ! h(帮助) 如果上面的工具觉得好用的话，下面这款工具真心觉得用的爽。 1、安装1pip install pudb 2、在代码的入口处插入 1from pudb import set_trace; set_trace() 3、调试代码 1sudo pudb bp.py 4、简单使用 1Ctrl-p可以配置pudb: 是否显示行号, 选择主题, 内置Python解释器的类别 n: next，也就是执行一步 s: step into，进入函数内部 c: continue b: break point，断点 !: python command line ?: help","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"devstack mitaka 安装配置","date":"2017-03-29T15:37:00.000Z","path":"2017/03/29/devstack/","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551、操作系统ubuntu 14.04 2、安装方式 devstack 3、安装版本 openstack mitaka版本4、安装devstack 步骤1) 目前国内已经提供了完整的OpenStack的github的mirrorhttp://git.trystack.cn2) 另外devstack还会下载image，下载的过程也是非常缓慢。trystack也提供大家常用的image下载。http://images.trystack.cn3) 设置源sudo vim /etc/apt/sources.listdeb http://cn.archive.ubuntu.com/ubuntu/ trusty main restricted universe multiversedeb http://cn.archive.ubuntu.com/ubuntu/ trusty-security main restricted universe multiversedeb http://cn.archive.ubuntu.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://cn.archive.ubuntu.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://cn.archive.ubuntu.com/ubuntu/ trusty-backports main restricted universe multiversesudo apt-get updatesudo ntpdate 192.168.2.161sudo apt-get install git -ysudo git clone http://git.trystack.cn/openstack-dev/devstack.git -b stable/mitaka# 目前Devstack脚本已经不支持直接使用root身份运行，你需要创建stack用户运行cd devstack/tools/sudo ./create-stack-user.shsudo passwd stack# 修改devstack目录权限,让stack用户可以运行sudo chown -R stack:stack devstacksudo chmod 777 /dev/pts/0su stackcd devstackvim local.confstack@ubuntu:/home/wanstack/devstack$ cat local.conf [[local|localrc]]# use TryStack git mirrorGIT_BASE=http://git.trystack.cnNOVNC_REPO=http://git.trystack.cn/kanaka/noVNC.gitSPICE_REPO=http://git.trystack.cn/git/spice/spice-html5.git#OFFLINE=TrueRECLONE=False# Define images to be automatically downloaded during the DevStack built process.DOWNLOAD_DEFAULT_IMAGES=FalseIMAGE_URLS=&quot;http://images.trystack.cn/cirros/cirros-0.3.4-x86_64-disk.img&quot;HOST_IP=192.168.12.10# CredentialsDATABASE_PASSWORD=openstackADMIN_PASSWORD=openstackSERVICE_PASSWORD=openstackSERVICE_TOKEN=openstackRABBIT_PASSWORD=openstackHORIZON_BRANCH=stable/mitakaKEYSTONE_BRANCH=stable/mitakaNOVA_BRANCH=stable/mitakaNEUTRON_BRANCH=stable/mitakaGLANCE_BRANCH=stable/mitakaCINDER_BRANCH=stable/mitakaCEILOMETER_BRANCH=stable/mitakaAODH_BRANCH=stable/mitakaenable_plugin ceilometer https://git.openstack.org/openstack/ceilometer stable/mitakaenable_plugin aodh https://git.openstack.org/openstack/aodh stable/mitaka#keystoneKEYSTONE_TOKEN_FORMAT=UUID##HeatHEAT_BRANCH=stable/mitakaenable_service h-eng h-api h-api-cfn h-api-cw## SwiftSWIFT_BRANCH=stable/mitakaENABLED_SERVICES+=,s-proxy,s-object,s-container,s-accountSWIFT_REPLICAS=1SWIFT_HASH=011688b44136573e209e# Enabling Neutron (network) Servicedisable_service n-netenable_service q-svcenable_service q-agtenable_service q-dhcpenable_service q-l3enable_service q-metaenable_service q-meteringenable_service neutron## Neutron optionsQ_USE_SECGROUP=TrueFLOATING_RANGE=&quot;192.168.12.0/24&quot;FIXED_RANGE=&quot;10.0.0.0/24&quot;Q_FLOATING_ALLOCATION_POOL=start=192.168.12.110,end=192.168.12.120PUBLIC_NETWORK_GATEWAY=&quot;192.168.12.1&quot;Q_L3_ENABLED=TruePUBLIC_INTERFACE=eth0Q_USE_PROVIDERNET_FOR_PUBLIC=TrueOVS_PHYSICAL_BRIDGE=br-exPUBLIC_BRIDGE=br-exOVS_BRIDGE_MAPPINGS=public:br-ex# #VLAN configuration.Q_PLUGIN=ml2ENABLE_TENANT_VLANS=True# LoggingLOGFILE=/opt/stack/logs/stack.sh.logVERBOSE=TrueLOG_COLOR=TrueSCREEN_LOGDIR=/opt/stack/logs报错:File &quot;/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/response.py&quot;, line 246, in _error_catcher2017-03-21 07:54:11.931 | raise ReadTimeoutError(self._pool, None, &apos;Read timed out.&apos;)2017-03-21 07:54:11.931 | ReadTimeoutError: HTTPSConnectionPool(host=&apos;pypi.python.org&apos;, port=443): Read timed out.报错解决:./stack.sh: line 488: generate-subunit: command not foundsudo apt-get install python-pip -ysudo pip install --upgrade pipsudo pip install -U os-testr报错:File &quot;/usr/local/lib/python2.7/dist-packages/openstack/session.py&quot;, line 29, in &lt;module&gt;2017-03-20 04:52:02.612 | DEFAULT_USER_AGENT = &quot;openstacksdk/%s&quot; % openstack.__version__2017-03-20 04:52:02.612 | AttributeError: &apos;module&apos; object has no attribute &apos;__version__&apos;2017-03-20 04:52:02.643 | +lib/keystone:create_keystone_accounts:373 admin_tenant=处理:root@ubuntu:~# python &gt;&gt;&gt; import openstack &gt;&gt;&gt; import pbr.version &gt;&gt;&gt; print(pbr.version.VersionInfo(&apos;openstacksdk&apos;).version_string()) 8 0.8.1 &gt;&gt;&gt; quit() vim /usr/local/lib/python2.7/dist-packages/openstack/session.py修改为： DEFAULT_USER_AGENT = &quot;openstacksdk/0.8.1&quot;./stack.sh 在ubuntu上搭建sambasudo apt-get install samba samba-common -y 我们把devstack安装在了/opt目录下，所以共享这个目录#sudo chmod -R 777 /optsudo useradd openstacksudo smbpasswd -a openstacksudo vim /etc/samba/smb.conf在文件末尾添加：[openstack]comment=openstackpublic=yesbrowseable = yespath=/optread only = no forceuser=rootforcegroup=root #################################################### 重启服务sudo /etc/init.d/smbd restart ```","tags":[{"name":"Openstack","slug":"Openstack","permalink":"http://yoursite.com/tags/Openstack/"}]},{"title":"ceilometer-mitaka api分析","date":"2017-03-29T14:36:22.000Z","path":"2017/03/29/ceilometer-mitaka api分析/","text":"本文的讲解是基于目前ceilometer的mitaka版本 ceilometer –debug meter-list的调用流程分析 大概过程 python-ceilometerclient ceilometer –debug meter-list curl GET http://192.168.12.10:8777/v2/meters Python Application的创建 HTTP请求的解析 RootController V2Controller MetersController 实际去获取数据的两个函数 pecan.request.storage_conn.get_meters的执行 Meter.from_db_model的执行 curl GET http://192.168.12.10:8777/v2/meters 这句很重要，下面我们就分析ceilometer收到这个HTTP请求后是如何解析，如何使用Python Application去查询MongoDB数据库的 1、Python Application的创建过程需要使用Python Application来接收HTTP请求，因此需要先创建Python Application。下面我们介绍使用Pecan+PasetDeploy创建Python Application的过程。 1在配置文件api_pasete.ini中我们可以看到，PasteDeploy会调用ceilometer.api.app:app_factory。 123456789101112131415[pipeline:main]pipeline = cors request_id authtoken api-server[app:api-server]paste.app_factory = ceilometer.api.app:app_factory[filter:authtoken]paste.filter_factory = keystonemiddleware.auth_token:filter_factory[filter:request_id]paste.filter_factory = oslo_middleware:RequestId.factory[filter:cors]paste.filter_factory = oslo_middleware.cors:filter_factoryoslo_config_project = ceilometer 根据[app:api-server] 我们得知app_factory函数的具体位置返回的是setup_app() 12def app_factory(global_config, **local_conf): return setup_app() setup_app函数就是真正创建Python Application的函数，在此函数中最重要的就是调用了pecan.make_app函数，在此函数中最重要的就是指定了解析HTTP Request的RootController，是通过pecan_config.app.root参数指定的；另外一个重要的地方就是hoooks.DBHook，在这里面初始化了数据库的链接，关于这一点后面再做介绍。 1234567891011121314151617181920212223242526272829303132def setup_app(pecan_config=None): # FIXME: Replace DBHook with a hooks.TransactionHook app_hooks = [hooks.ConfigHook(), hooks.DBHook(), hooks.NotifierHook(), hooks.TranslationHook()] pecan_config = pecan_config or &#123; &quot;app&quot;: &#123; &apos;root&apos;: &apos;ceilometer.api.controllers.root.RootController&apos;, &apos;modules&apos;: [&apos;ceilometer.api&apos;], &#125; &#125; pecan.configuration.set_config(dict(pecan_config), overwrite=True) # NOTE(sileht): pecan debug won&apos;t work in multi-process environment pecan_debug = CONF.api.pecan_debug if CONF.api.workers and CONF.api.workers != 1 and pecan_debug: pecan_debug = False LOG.warning(_LW(&apos;pecan_debug cannot be enabled, if workers is &gt; 1, &apos; &apos;the value is overrided with False&apos;)) app = pecan.make_app( pecan_config[&apos;app&apos;][&apos;root&apos;], debug=pecan_debug, hooks=app_hooks, wrap_app=middleware.ParsableErrorMiddleware, guess_content_type_from_ext=False ) return app 在上面我们提到解析HTTP请求的RootController是通过pecan_config.app.root参数指定的，阅读代码可知pecan_config.app.root就pecan_config中指定的，也就是ceilometer.api.controllers.root.RootController。这样Python Application就创建完成了，并且指定好了解析HTTP Request的RootController，也就是说/v2/meters/从RootController开始处理。 2、HTTP请求的解析过程在RootController我们可以看到它先创建了一个类属性:v2，于是所有的以/v2开头的HTTP Request都会由V2Controller来处理，/v2/meters/当然也不例外 12345678910111213class RootController(object): def __init__(self): self.v2 = v2.V2Controller() @pecan.expose(&apos;json&apos;) def index(self): base_url = pecan.request.application_url available = [&#123;&apos;tag&apos;: &apos;v2&apos;, &apos;date&apos;: &apos;2013-02-13T00:00:00Z&apos;, &#125;] collected = [version_descriptor(base_url, v[&apos;tag&apos;], v[&apos;date&apos;]) for v in available] versions = &#123;&apos;versions&apos;: &#123;&apos;values&apos;: collected&#125;&#125; return versions 下面我们再去看V2Controller，我们可以看到它有类属性:event_types,events,capabilities，也就是说/v2/event_types会由EventTypesController来处理，/v2/events/会由EventsController来处理，/v2/capabilities/会由CapabilitiesController来处理，那/v2/meters/呢？往下看，在_lookup函数中我们可以看到/v2/meters/会由MetersController来处理，关于__lookup可以参看官方文档。 123456class V2Controller(object): &quot;&quot;&quot;Version 2 API controller root.&quot;&quot;&quot; event_types = events.EventTypesController() events = events.EventsController() capabilities = capabilities.CapabilitiesController() 12345678910111213141516171819202122232425262728@pecan.expose() def _lookup(self, kind, *remainder): if (kind in [&apos;meters&apos;, &apos;resources&apos;, &apos;samples&apos;] and self.gnocchi_is_enabled): if kind == &apos;meters&apos; and pecan.request.method == &apos;POST&apos;: direct = pecan.request.params.get(&apos;direct&apos;, &apos;&apos;) if strutils.bool_from_string(direct): pecan.abort(400, _(&apos;direct option cannot be true when &apos; &apos;Gnocchi is enabled.&apos;)) return meters.MetersController(), remainder gnocchi_abort() elif kind == &apos;meters&apos;: return meters.MetersController(), remainder elif kind == &apos;resources&apos;: return resources.ResourcesController(), remainder elif kind == &apos;samples&apos;: return samples.SamplesController(), remainder elif kind == &apos;query&apos;: return QueryController( gnocchi_is_enabled=self.gnocchi_is_enabled, aodh_url=self.aodh_url, ), remainder elif kind == &apos;alarms&apos; and (not self.aodh_url): aodh_abort() elif kind == &apos;alarms&apos; and self.aodh_url: aodh_redirect(self.aodh_url) else: pecan.abort(404) 因为我们是meters所以最终会执行1return meters.MetersController(), remainder 终于到了MetersController，这里就是最终处理HTTP请求:/v2/meters/的地方 12345678910111213141516171819202122232425262728class MetersController(rest.RestController): &quot;&quot;&quot;Works on meters.&quot;&quot;&quot; @pecan.expose() def _lookup(self, meter_name, *remainder): return MeterController(meter_name), remainder @wsme_pecan.wsexpose([Meter], [base.Query], int, str) def get_all(self, q=None, limit=None, unique=&apos;&apos;): &quot;&quot;&quot;Return all known meters, based on the data recorded so far. :param q: Filter rules for the meters to be returned. :param unique: flag to indicate unique meters to be returned. &quot;&quot;&quot; rbac.enforce(&apos;get_meters&apos;, pecan.request) q = q or [] # Timestamp field is not supported for Meter queries limit = v2_utils.enforce_limit(limit) kwargs = v2_utils.query_to_kwargs( q, pecan.request.storage_conn.get_meters, [&apos;limit&apos;], allow_timestamps=False) return [Meter.from_db_model(m) for m in pecan.request.storage_conn.get_meters( limit=limit, unique=strutils.bool_from_string(unique), **kwargs)] 在这里最重要的就是最后那一行代码：return [Meter.from_db_model(m) for m in pecan.request.storage_conn.get_meters(limit=limit,**kwargs)]，在这行代码中有两个重要的函数调用：pecan.request.storage_conn.get_meters 以及 Meter.from_db_model。所以还没结束，下面还得分析这两个函数是如何执行的。 pecan.request.storage_conn.get_meters的执行过程要分析pecan.request.storage_conn.get_meters(limit=limit, **kwargs)的执行过程我们分两步：一是storage_conn是如何获得的，二是get_meters是如何执行的。 storage_conn指的是数据库的链接，在DBHook.init中可以看出它是通过调用函数get_connection来获得的，而函数get_connection又调用了函数storage.get_connection_from_config。 123456789101112131415161718192021222324class DBHook(hooks.PecanHook): def __init__(self): self.storage_connection = DBHook.get_connection(&apos;metering&apos;) self.event_storage_connection = DBHook.get_connection(&apos;event&apos;) if (not self.storage_connection and not self.event_storage_connection): raise Exception(&quot;Api failed to start. Failed to connect to &quot; &quot;databases, purpose: %s&quot; % &apos;, &apos;.join([&apos;metering&apos;, &apos;event&apos;])) def before(self, state): state.request.storage_conn = self.storage_connection state.request.event_storage_conn = self.event_storage_connection @staticmethod def get_connection(purpose): try: return storage.get_connection_from_config(cfg.CONF, purpose) except Exception as err: params = &#123;&quot;purpose&quot;: purpose, &quot;err&quot;: err&#125; LOG.exception(_LE(&quot;Failed to connect to db, purpose %(purpose)s &quot; &quot;retry later: %(err)s&quot;) % params) 下面再看storage.get_connection_from_config函数是如何执行的，函数storage.get_connection_from_config调用了函数storage.get_connection，而在函数storage.get_connection中重要的是: mgr = driver.DriverManger(namespace, engine_name)，其中的driver为：from stevedore import driver，namespace为：ceilometer.meterings.storage，engine_name为：mongodb。于是stevedore会到setup.cfg中查找相应的设置。 12345678910111213141516171819202122232425262728293031323334def get_connection_from_config(conf, purpose=&apos;metering&apos;): retries = conf.database.max_retries # Convert retry_interval secs to msecs for retry decorator @retrying.retry(wait_fixed=conf.database.retry_interval * 1000, stop_max_attempt_number=retries if retries &gt;= 0 else None) def _inner(): if conf.database_connection: conf.set_override(&apos;connection&apos;, conf.database_connection, group=&apos;database&apos;) namespace = &apos;ceilometer.%s.storage&apos; % purpose url = (getattr(conf.database, &apos;%s_connection&apos; % purpose) or conf.database.connection) return get_connection(url, namespace) return _inner()def get_connection(url, namespace): &quot;&quot;&quot;Return an open connection to the database.&quot;&quot;&quot; connection_scheme = urlparse.urlparse(url).scheme # SqlAlchemy connections specify may specify a &apos;dialect&apos; or # &apos;dialect+driver&apos;. Handle the case where driver is specified. engine_name = connection_scheme.split(&apos;+&apos;)[0] if engine_name == &apos;db2&apos;: import warnings warnings.simplefilter(&quot;always&quot;) import debtcollector debtcollector.deprecate(&quot;The DB2nosql driver is no longer supported&quot;, version=&quot;Liberty&quot;, removal_version=&quot;N*-cycle&quot;) # NOTE: translation not applied bug #1446983 LOG.debug(&apos;looking for %(name)r driver in %(namespace)r&apos;, &#123;&apos;name&apos;: engine_name, &apos;namespace&apos;: namespace&#125;) mgr = driver.DriverManager(namespace, engine_name) return mgr.driver(url) 根据 12from stevedore import driver namespace为：ceilometer.meterings.storage，engine_name为：mongodb 所以会去调用setup.cfg中的 12ceilometer.event.storage = mongodb = ceilometer.event.storage.impl_mongodb:Connection 在ceilometer.storage.imple_mongodb.Connection类的初始化函数中会初始化数据库的连接，没有相应collection的情况下新建相应collection，设置ttl等。 ../ceilometer/storage/impl_mongodb.py: Connection.init123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112class Connection(pymongo_base.Connection): CAPABILITIES = utils.update_nested(pymongo_base.Connection.CAPABILITIES, AVAILABLE_CAPABILITIES) CONNECTION_POOL = pymongo_utils.ConnectionPool() STANDARD_AGGREGATES = dict([(a.name, a) for a in [ pymongo_utils.SUM_AGGREGATION, pymongo_utils.AVG_AGGREGATION, pymongo_utils.MIN_AGGREGATION, pymongo_utils.MAX_AGGREGATION, pymongo_utils.COUNT_AGGREGATION, ]]) AGGREGATES = dict([(a.name, a) for a in [ pymongo_utils.SUM_AGGREGATION, pymongo_utils.AVG_AGGREGATION, pymongo_utils.MIN_AGGREGATION, pymongo_utils.MAX_AGGREGATION, pymongo_utils.COUNT_AGGREGATION, pymongo_utils.STDDEV_AGGREGATION, pymongo_utils.CARDINALITY_AGGREGATION, ]]) SORT_OPERATION_MAPPING = &#123;&apos;desc&apos;: (pymongo.DESCENDING, &apos;$lt&apos;), &apos;asc&apos;: (pymongo.ASCENDING, &apos;$gt&apos;)&#125; MAP_RESOURCES = bson.code.Code(&quot;&quot;&quot; function () &#123; emit(this.resource_id, &#123;user_id: this.user_id, project_id: this.project_id, source: this.source, first_timestamp: this.timestamp, last_timestamp: this.timestamp, metadata: this.resource_metadata&#125;) &#125;&quot;&quot;&quot;) REDUCE_RESOURCES = bson.code.Code(&quot;&quot;&quot; function (key, values) &#123; var merge = &#123;user_id: values[0].user_id, project_id: values[0].project_id, source: values[0].source, first_timestamp: values[0].first_timestamp, last_timestamp: values[0].last_timestamp, metadata: values[0].metadata&#125; values.forEach(function(value) &#123; if (merge.first_timestamp - value.first_timestamp &gt; 0) &#123; merge.first_timestamp = value.first_timestamp; merge.user_id = value.user_id; merge.project_id = value.project_id; merge.source = value.source; &#125; else if (merge.last_timestamp - value.last_timestamp &lt;= 0) &#123; merge.last_timestamp = value.last_timestamp; merge.metadata = value.metadata; &#125; &#125;); return merge; &#125;&quot;&quot;&quot;) _GENESIS = datetime.datetime(year=datetime.MINYEAR, month=1, day=1) _APOCALYPSE = datetime.datetime(year=datetime.MAXYEAR, month=12, day=31, hour=23, minute=59, second=59) def __init__(self, url): # NOTE(jd) Use our own connection pooling on top of the Pymongo one. # We need that otherwise we overflow the MongoDB instance with new # connection since we instantiate a Pymongo client each time someone # requires a new storage connection. self.conn = self.CONNECTION_POOL.connect(url) self.version = self.conn.server_info()[&apos;versionArray&apos;] # Require MongoDB 2.4 to use $setOnInsert if self.version &lt; pymongo_utils.MINIMUM_COMPATIBLE_MONGODB_VERSION: raise storage.StorageBadVersion( &quot;Need at least MongoDB %s&quot; % pymongo_utils.MINIMUM_COMPATIBLE_MONGODB_VERSION) connection_options = pymongo.uri_parser.parse_uri(url) self.db = getattr(self.conn, connection_options[&apos;database&apos;]) if connection_options.get(&apos;username&apos;): self.db.authenticate(connection_options[&apos;username&apos;], connection_options[&apos;password&apos;]) # NOTE(jd) Upgrading is just about creating index, so let&apos;s do this # on connection to be sure at least the TTL is correctly updated if # needed. self.upgrade() @staticmethod def update_ttl(ttl, ttl_index_name, index_field, coll): &quot;&quot;&quot;Update or create time_to_live indexes. :param ttl: time to live in seconds. :param ttl_index_name: name of the index we want to update or create. :param index_field: field with the index that we need to update. :param coll: collection which indexes need to be updated. &quot;&quot;&quot; indexes = coll.index_information() if ttl &lt;= 0: if ttl_index_name in indexes: coll.drop_index(ttl_index_name) return if ttl_index_name in indexes: return coll.database.command( &apos;collMod&apos;, coll.name, index=&#123;&apos;keyPattern&apos;: &#123;index_field: pymongo.ASCENDING&#125;, &apos;expireAfterSeconds&apos;: ttl&#125;) coll.create_index([(index_field, pymongo.ASCENDING)], expireAfterSeconds=ttl, name=ttl_index_name)... get_meters的执行过程这里有一个类的继承关系：object &lt;– storage.base.Connection &lt;– storage.pymongo_base.Connection &lt;– storage.impl_mongodb.Connection，关于storage.impl_mongodb.Connection需要讲的都在上面提到了。 在storage.base.Connection中给出了一些函数定义，但都没有具体的实现。 在storage.pymongo_base.Connection中给出了get_meters的定义，在这里我们就可以真正的看到查询数据库的语句了: self.db.resource.find(q)，下面我们还要解释一下语句models.Meter。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283class Connection(base.Connection): &quot;&quot;&quot;Base Connection class for MongoDB and DB2 drivers.&quot;&quot;&quot; CAPABILITIES = utils.update_nested(base.Connection.CAPABILITIES, COMMON_AVAILABLE_CAPABILITIES) STORAGE_CAPABILITIES = utils.update_nested( base.Connection.STORAGE_CAPABILITIES, AVAILABLE_STORAGE_CAPABILITIES, ) def get_meters(self, user=None, project=None, resource=None, source=None, metaquery=None, limit=None, unique=False): &quot;&quot;&quot;Return an iterable of models.Meter instances :param user: Optional ID for user that owns the resource. :param project: Optional ID for project that owns the resource. :param resource: Optional resource filter. :param source: Optional source filter. :param metaquery: Optional dict with metadata to match on. :param limit: Maximum number of results to return. :param unique: If set to true, return only unique meter information. &quot;&quot;&quot; if limit == 0: return metaquery = pymongo_utils.improve_keys(metaquery, metaquery=True) or &#123;&#125; q = &#123;&#125; if user == &apos;None&apos;: q[&apos;user_id&apos;] = None elif user is not None: q[&apos;user_id&apos;] = user if project == &apos;None&apos;: q[&apos;project_id&apos;] = None elif project is not None: q[&apos;project_id&apos;] = project if resource == &apos;None&apos;: q[&apos;_id&apos;] = None elif resource is not None: q[&apos;_id&apos;] = resource if source is not None: q[&apos;source&apos;] = source q.update(metaquery) count = 0 if unique: meter_names = set() for r in self.db.resource.find(q): for r_meter in r[&apos;meter&apos;]: if unique: if r_meter[&apos;counter_name&apos;] in meter_names: continue else: meter_names.add(r_meter[&apos;counter_name&apos;]) if limit and count &gt;= limit: return else: count += 1 if unique: yield models.Meter( name=r_meter[&apos;counter_name&apos;], type=r_meter[&apos;counter_type&apos;], # Return empty string if &apos;counter_unit&apos; is not valid # for backward compatibility. unit=r_meter.get(&apos;counter_unit&apos;, &apos;&apos;), resource_id=None, project_id=None, source=None, user_id=None) else: yield models.Meter( name=r_meter[&apos;counter_name&apos;], type=r_meter[&apos;counter_type&apos;], # Return empty string if &apos;counter_unit&apos; is not valid # for backward compatibility. unit=r_meter.get(&apos;counter_unit&apos;, &apos;&apos;), resource_id=r[&apos;_id&apos;], project_id=r[&apos;project_id&apos;], source=r[&apos;source&apos;], user_id=r[&apos;user_id&apos;]) 这里又有一个类的继承关系：object &lt;– storage.base.Model &lt;– storage.models.Meter。 这里其实就是把查询到的值和键做个对应的设置。 1234567891011121314151617181920212223242526class Model(object): &quot;&quot;&quot;Base class for storage API models.&quot;&quot;&quot; def __init__(self, **kwds): self.fields = list(kwds) for k, v in six.iteritems(kwds): setattr(self, k, v) def as_dict(self): d = &#123;&#125; for f in self.fields: v = getattr(self, f) if isinstance(v, Model): v = v.as_dict() elif isinstance(v, list) and v and isinstance(v[0], Model): v = [sub.as_dict() for sub in v] d[f] = v return d def __eq__(self, other): return self.as_dict() == other.as_dict() @classmethod def get_field_names(cls): fields = inspect.getargspec(cls.__init__)[0] return set(fields) - set([&quot;self&quot;]) Meter.from_db_model(m)的执行过程这里有个类的继承关系：wsme.types.Base &lt;– wsme.types.DynamicBase &lt;– api.controllers.v2.base.Base &lt;– api.v2.meters.Meter 后面api.v2.meters.Meter的初始化函数调用了wsme.types.Base.init函数。 123456789101112131415161718192021222324252627282930class Base(six.with_metaclass(BaseMeta)): &quot;&quot;&quot;Base type for complex types&quot;&quot;&quot; def __init__(self, **kw): for key, value in kw.items(): if hasattr(self, key): setattr(self, key, value) class DynamicBase(Base): &quot;&quot;&quot;Base type for complex types for which all attributes are not defined when the class is constructed. This class is meant to be used as a base for types that have properties added after the main class is created, such as by loading plugins. &quot;&quot;&quot; @classmethod def add_attributes(cls, **attrs): &quot;&quot;&quot;Add more attributes The arguments should be valid Python attribute names associated with a type for the new attribute. &quot;&quot;&quot; for n, t in attrs.items(): setattr(cls, n, t) cls.__registry__.reregister(cls) 函数Meter.from_db_model定义在ceilometer.api.controllers.v2.base:Base中，函数Meter.from_db_model是把返回的ceilometer.storage.models.Meter实例， 进一步做一些简单的处理。 123456789101112131415161718192021class Base(wtypes.DynamicBase): @classmethod def from_db_model(cls, m): return cls(**(m.as_dict())) @classmethod def from_db_and_links(cls, m, links): return cls(links=links, **(m.as_dict())) def as_dict(self, db_model): valid_keys = inspect.getargspec(db_model.__init__)[0] if &apos;self&apos; in valid_keys: valid_keys.remove(&apos;self&apos;) return self.as_dict_from_keys(valid_keys) def as_dict_from_keys(self, keys): return dict((k, getattr(self, k)) for k in keys if hasattr(self, k) and getattr(self, k) != wsme.Unset) Meter是在ceilometer.api.v2.meters中定义的，Meter中定义它的的类属性:name, type, unit, resource_id, project_id, user_id, source, meter_id；Meter中还定义了初始化函数init，该初始化函数主要是构造meter_id和调用wsme.types.Base的初始化函数init。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Meter(base.Base): &quot;&quot;&quot;One category of measurements.&quot;&quot;&quot; name = wtypes.text &quot;The unique name for the meter&quot; type = wtypes.Enum(str, *sample.TYPES) &quot;The meter type (see :ref:`measurements`)&quot; unit = wtypes.text &quot;The unit of measure&quot; resource_id = wtypes.text &quot;The ID of the :class:`Resource` for which the measurements are taken&quot; project_id = wtypes.text &quot;The ID of the project or tenant that owns the resource&quot; user_id = wtypes.text &quot;The ID of the user who last triggered an update to the resource&quot; source = wtypes.text &quot;The ID of the source that identifies where the meter comes from&quot; meter_id = wtypes.text &quot;The unique identifier for the meter&quot; def __init__(self, **kwargs): meter_id = &apos;%s+%s&apos; % (kwargs[&apos;resource_id&apos;], kwargs[&apos;name&apos;]) # meter_id is of type Unicode but base64.encodestring() only accepts # strings. See bug #1333177 meter_id = base64.b64encode(meter_id.encode(&apos;utf-8&apos;)) kwargs[&apos;meter_id&apos;] = meter_id super(Meter, self).__init__(**kwargs) @classmethod def sample(cls): return cls(name=&apos;instance&apos;, type=&apos;gauge&apos;, unit=&apos;instance&apos;, resource_id=&apos;bd9431c1-8d69-4ad3-803a-8d4a6b89fd36&apos;, project_id=&apos;35b17138-b364-4e6a-a131-8f3099c5be68&apos;, user_id=&apos;efd87807-12d2-4b38-9c70-5f5c2ac427ff&apos;, source=&apos;openstack&apos;, ) 完毕。本文只是简单梳理了一些的代码流程，没有对于细节问题深究。","tags":[{"name":"Openstack","slug":"Openstack","permalink":"http://yoursite.com/tags/Openstack/"}]}]